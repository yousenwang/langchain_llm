{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yousenwang/langchain_llm/blob/main/RetrievalQA_by_Ethan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VDflBJKBZTbb"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain==0.0.215 chromadb pypdf sentence_transformers InstructorEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nA83UriMaJzb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c1d0b79-58b3-4005-c7c9-2995e13da37d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.0.215\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://www.github.com/hwchase17/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, async-timeout, dataclasses-json, langchainplus-sdk, numexpr, numpy, openapi-schema-pydantic, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VUAQPbfCa9pM"
      },
      "outputs": [],
      "source": [
        "# from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "# from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zzrV7jO4bga4"
      },
      "outputs": [],
      "source": [
        "loader = DirectoryLoader('./', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
        "\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Q6Xl-Zn1bodt"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000//2, chunk_overlap=200//2)\n",
        "texts = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_4_W72cbs4-",
        "outputId": "d5a85acb-e81d-45b6-8661-56e5dbc039e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n"
          ]
        }
      ],
      "source": [
        "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\",\n",
        "                                                      model_kwargs={\"device\": \"cuda\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "WzUKaVMFb0P0"
      },
      "outputs": [],
      "source": [
        "persist_directory = 'db'\n",
        "\n",
        "## Here is the nmew embeddings being used\n",
        "embedding = instructor_embeddings\n",
        "\n",
        "vectordb = Chroma.from_documents(documents=texts,\n",
        "                                 embedding=embedding,\n",
        "                                 persist_directory=persist_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "u5woJaqrcJ7R"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.llms import GPT4All\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gpt4all==0.3.6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3MbB_6S8qQB",
        "outputId": "fdbc7fc8-1c77-4cdb-d4bf-4b0211881b7e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gpt4all==0.3.6 in /usr/local/lib/python3.10/dist-packages (0.3.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt4all==0.3.6) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt4all==0.3.6) (4.65.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all==0.3.6) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all==0.3.6) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all==0.3.6) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all==0.3.6) (2023.5.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_name = \"ggml-gpt4all-j-v1.3-groovy.bin\""
      ],
      "metadata": {
        "id": "1qvrlwFeAfAb"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gpt4all\n",
        "gptj = gpt4all.GPT4All(llm_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoeOB6Kd8sIE",
        "outputId": "fa64839f-f83b-4d90-edd7-1aea95c6a28a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found model file at  /root/.cache/gpt4all/ggml-gpt4all-j-v1.3-groovy.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# You can run raw generate as well on your input. But performance will degrade.\n",
        "res = gptj.generate('I am Ethan and I live in Taiwan. What is my name?')\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udjhRT0c8tal",
        "outputId": "39f7458d-c670-49de-97b5-80ad8e547274"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ethan\n",
            "\n",
            "Ethan\n",
            "CPU times: user 25.7 s, sys: 17.4 ms, total: 25.8 s\n",
            "Wall time: 16.1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = texts[0].page_content"
      ],
      "metadata": {
        "id": "gUVf1-V1-tNT"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(texts[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfT4mRGTAcMC",
        "outputId": "e117e447-dcaa-4219-dfe6-5821044ac4ef"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 / 6 \n",
            "This document  belongs to Wareconn  Technology Services (Tianjin) Co., Ltd. It is only intended to be \n",
            "used to introduce Wareconn functions and procedures. Please do not use it for other purposes.  1 wareconn standard operating procedure  \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "wareconn standard operating procedure  \n",
            "RMA request  SOP  \n",
            " \n",
            "Version : \n",
            "Version  Date  Editor  Description  \n",
            "v 1.0  2023/03/16 Eric Sun First draft (English version)  \n",
            " \n",
            " \n",
            "Content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(texts[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yu6VR7pxArJE",
        "outputId": "73cb9260-45d8-4f9d-81bb-316f706a9a44"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='1 / 6 \\nThis document  belongs to Wareconn  Technology Services (Tianjin) Co., Ltd. It is only intended to be \\nused to introduce Wareconn functions and procedures. Please do not use it for other purposes.  1 wareconn standard operating procedure  \\n \\n \\n \\n \\n \\n \\nwareconn standard operating procedure  \\nRMA request  SOP  \\n \\nVersion : \\nVersion  Date  Editor  Description  \\nv 1.0  2023/03/16 Eric Sun First draft (English version)  \\n \\n \\nContent' metadata={'source': 'MSFT RMA request SOP v1.0.pdf', 'page': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How can I navigate to the RMA request page in the Wareconn Customer Portal?\""
      ],
      "metadata": {
        "id": "huhIqYSF_V3t"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_prompt = f\"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "Answer:\"\"\""
      ],
      "metadata": {
        "id": "zM9l8Pe8_FLm"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# You can run raw generate as well on your input. But performance will degrade.\n",
        "res = gptj.generate(input_prompt)\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkqrC6Vn_bjw",
        "outputId": "5a6abde6-7a89-4353-8b91-3b90843b5c11"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " To navigate to the RMA request page in the Wareconn Customer Portal, follow these steps:\n",
            "1. Log in to your Wareconn account by entering the username and password.\n",
            "2. Once you are logged in, click on the \"My Account\" tab.\n",
            "3. In the \"My Account\" page, click on the \"Wareconn Customer Portal\" tab.\n",
            "4. In the \"Wareconn Customer Portal\" page, click on the \"RMAs\" tab.\n",
            "5. In the \"RMAs\" page, click on the specific RMA you want to navigate to\n",
            " To navigate to the RMA request page in the Wareconn Customer Portal, follow these steps:\n",
            "1. Log in to your Wareconn account by entering the username and password.\n",
            "2. Once you are logged in, click on the \"My Account\" tab.\n",
            "3. In the \"My Account\" page, click on the \"Wareconn Customer Portal\" tab.\n",
            "4. In the \"Wareconn Customer Portal\" page, click on the \"RMAs\" tab.\n",
            "5. In the \"RMAs\" page, click on the specific RMA you want to navigate to\n",
            "CPU times: user 7min 31s, sys: 1.05 s, total: 7min 32s\n",
            "Wall time: 4min 40s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://python.langchain.com/docs/modules/chains/popular/vector_db_qa\n",
        "from langchain.prompts import PromptTemplate\n",
        "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer in Italian:\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")"
      ],
      "metadata": {
        "id": "YK9M7VyR9OPm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docsearch = Chroma.from_documents(texts, embeddings)"
      ],
      "metadata": {
        "id": "6H_b33PH-b3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain_type_kwargs = {\"prompt\": PROMPT}\n",
        "qa = RetrievalQA.from_chain_type(llm=gptj, chain_type=\"stuff\", retriever=docsearch.as_retriever(), chain_type_kwargs=chain_type_kwargs)"
      ],
      "metadata": {
        "id": "UThpmms79VVv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOM18+ADHo5ZNgHqPXeFmR6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}