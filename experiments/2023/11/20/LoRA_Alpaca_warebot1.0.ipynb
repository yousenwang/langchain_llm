{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "753265c2-7934-4611-9ea1-6e0c8b78cdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11/17/2023\n",
    "## LoRA_FlagAlpha_Llama2_IPQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c055d055-6317-4cee-939a-5ac1663cc6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fba1eca-bd02-47ef-a878-94e7be343673",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet1 = pd.read_excel(\"./data/(warebot1.0)wareconn Q&A.xlsx\", sheet_name=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "034764cf-a4b4-41c0-b5df-a2120120be79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>编号</th>\n",
       "      <th>问题</th>\n",
       "      <th>答案</th>\n",
       "      <th>相似问题1</th>\n",
       "      <th>相似问题2</th>\n",
       "      <th>功能</th>\n",
       "      <th>负责人</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>wareconn是什么?</td>\n",
       "      <td>Wareconn是一个提供售后服务的云平台，包括保修管理、维修服务、数据分析、设备管理APP...</td>\n",
       "      <td>wareconn是做什么的?</td>\n",
       "      <td>保修云是什么?</td>\n",
       "      <td>平台</td>\n",
       "      <td>怡辰</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>wareconn有哪些功能?</td>\n",
       "      <td>1. 保修管理：建立一个售后服务相关的逆向运筹管理平台的基础，让厂家与客户在此平台上共同管理...</td>\n",
       "      <td>保修云有什么功能?</td>\n",
       "      <td>系统有什么功能?</td>\n",
       "      <td>平台</td>\n",
       "      <td>怡辰</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>保修管理是做什么的?</td>\n",
       "      <td>保修管理是Wareconn保修云的一个主要功能之一，主要包括保修合约管理、返修作业申请、实时...</td>\n",
       "      <td>保修管理是什么?</td>\n",
       "      <td>什么是保修管理功能?</td>\n",
       "      <td>平台</td>\n",
       "      <td>怡辰</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>保修管理功能有几个角色?</td>\n",
       "      <td>保修管理功能主要涉及厂家和客户两个角色, 客户是指有保修服务需求的企业或个人，可以提交保修申...</td>\n",
       "      <td>保修管理功能谁可以用?</td>\n",
       "      <td>哪些角色会用到保修管理?</td>\n",
       "      <td>平台</td>\n",
       "      <td>怡辰</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>保修管理功能的作业流程是什么?</td>\n",
       "      <td>保修管理功能的作业流程可分为两个主要方面：厂家（提供保修服务的业者）和客户（拥有保修服务需求...</td>\n",
       "      <td>保修管理可以管控什么流程?</td>\n",
       "      <td>应用保修管理的步骤是什么?</td>\n",
       "      <td>平台</td>\n",
       "      <td>怡辰</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>26</td>\n",
       "      <td>怎样跳工站?</td>\n",
       "      <td>工单服务状态已经是\"服务中\"状态, 且当前工站是\"服务中状态\"方可跳站,【进度管控】选择【进...</td>\n",
       "      <td>当前作业工站如何跳到指定工站?</td>\n",
       "      <td>工站怎么跳转?</td>\n",
       "      <td>维修\\n服务\\n(RS)</td>\n",
       "      <td>王晶晶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>27</td>\n",
       "      <td>服务中心如何更新升级?</td>\n",
       "      <td>更新升级分为系统设定升级规则和未设定,且服务状态需在\"服务中\",在服务中心页面,【进度管控】...</td>\n",
       "      <td>产品升级怎么操作?</td>\n",
       "      <td>怎么进行产品变更?</td>\n",
       "      <td>维修\\n服务\\n(RS)</td>\n",
       "      <td>王晶晶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>28</td>\n",
       "      <td>从哪里查看零件替换信息?</td>\n",
       "      <td>服务中心工站作业时若有填写零件替换信息,则可以查看,否则无,具体操作为:\\n1. 未结案: ...</td>\n",
       "      <td>如何确认产品是否有替换物料?</td>\n",
       "      <td>物料使用记录怎样查看?</td>\n",
       "      <td>维修\\n服务\\n(RS)</td>\n",
       "      <td>王晶晶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>29</td>\n",
       "      <td>服务中心工作数据怎么导出?</td>\n",
       "      <td>服务中心的工站作业数据可由[任务汇总】导出\\n【统计核算】中选择【任务汇总】进入,点\"保修项...</td>\n",
       "      <td>如何导出报表?</td>\n",
       "      <td>送修品工站作业数据哪里导出报表?</td>\n",
       "      <td>维修\\n服务\\n(RS)</td>\n",
       "      <td>王晶晶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>30</td>\n",
       "      <td>服务中心维修数据图表在哪里看?</td>\n",
       "      <td>服务中心维修数据可以用统计图表呈现,呈现前需先进行设定,具体操作为:\\n1. 设定:【统计核...</td>\n",
       "      <td>维修统计怎么用图表显示?</td>\n",
       "      <td>服务中心图表怎么查看?</td>\n",
       "      <td>维修\\n服务\\n(RS)</td>\n",
       "      <td>王晶晶</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     编号               问题                                                 答案  \\\n",
       "0     1     wareconn是什么?  Wareconn是一个提供售后服务的云平台，包括保修管理、维修服务、数据分析、设备管理APP...   \n",
       "1     2   wareconn有哪些功能?  1. 保修管理：建立一个售后服务相关的逆向运筹管理平台的基础，让厂家与客户在此平台上共同管理...   \n",
       "2     3       保修管理是做什么的?  保修管理是Wareconn保修云的一个主要功能之一，主要包括保修合约管理、返修作业申请、实时...   \n",
       "3     4     保修管理功能有几个角色?  保修管理功能主要涉及厂家和客户两个角色, 客户是指有保修服务需求的企业或个人，可以提交保修申...   \n",
       "4     5  保修管理功能的作业流程是什么?  保修管理功能的作业流程可分为两个主要方面：厂家（提供保修服务的业者）和客户（拥有保修服务需求...   \n",
       "..   ..              ...                                                ...   \n",
       "115  26           怎样跳工站?  工单服务状态已经是\"服务中\"状态, 且当前工站是\"服务中状态\"方可跳站,【进度管控】选择【进...   \n",
       "116  27      服务中心如何更新升级?  更新升级分为系统设定升级规则和未设定,且服务状态需在\"服务中\",在服务中心页面,【进度管控】...   \n",
       "117  28     从哪里查看零件替换信息?  服务中心工站作业时若有填写零件替换信息,则可以查看,否则无,具体操作为:\\n1. 未结案: ...   \n",
       "118  29    服务中心工作数据怎么导出?  服务中心的工站作业数据可由[任务汇总】导出\\n【统计核算】中选择【任务汇总】进入,点\"保修项...   \n",
       "119  30  服务中心维修数据图表在哪里看?  服务中心维修数据可以用统计图表呈现,呈现前需先进行设定,具体操作为:\\n1. 设定:【统计核...   \n",
       "\n",
       "               相似问题1             相似问题2            功能  负责人  \n",
       "0     wareconn是做什么的?           保修云是什么?            平台   怡辰  \n",
       "1          保修云有什么功能?          系统有什么功能?            平台   怡辰  \n",
       "2           保修管理是什么?        什么是保修管理功能?            平台   怡辰  \n",
       "3        保修管理功能谁可以用?      哪些角色会用到保修管理?            平台   怡辰  \n",
       "4      保修管理可以管控什么流程?     应用保修管理的步骤是什么?            平台   怡辰  \n",
       "..               ...               ...           ...  ...  \n",
       "115  当前作业工站如何跳到指定工站?           工站怎么跳转?  维修\\n服务\\n(RS)  王晶晶  \n",
       "116        产品升级怎么操作?         怎么进行产品变更?  维修\\n服务\\n(RS)  王晶晶  \n",
       "117   如何确认产品是否有替换物料?       物料使用记录怎样查看?  维修\\n服务\\n(RS)  王晶晶  \n",
       "118          如何导出报表?  送修品工站作业数据哪里导出报表?  维修\\n服务\\n(RS)  王晶晶  \n",
       "119     维修统计怎么用图表显示?       服务中心图表怎么查看?  维修\\n服务\\n(RS)  王晶晶  \n",
       "\n",
       "[120 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sheet1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed79f60a-f72e-4dd5-919b-0df470793f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([sheet1], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7491a97b-a366-4d53-8583-204c9bdb4f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data[[\"問題\",\"答案\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff319be5-8b55-4527-a90f-8c3e08d0d35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ab5491b-0bbd-4b7c-a6ad-b167a93a9cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'编号': 1,\n",
       " '问题': 'wareconn是什么?',\n",
       " '答案': 'Wareconn是一个提供售后服务的云平台，包括保修管理、维修服务、数据分析、设备管理APP和派工管理APP等功能，旨在提高售后服务的效率和质量，帮助企业实现数字化流程掌控、支持多系统串接及智慧决策，推助工业 4.0 发展，赋能産品生命周期、价值链全周期管理与服务，持续提升企业营运绩效。',\n",
       " '相似问题1': 'wareconn是做什么的?',\n",
       " '相似问题2': '保修云是什么?',\n",
       " '功能': '平台',\n",
       " '负责人': '怡辰'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccb6c501-c8e0-4095-aef6-f1c695ffcc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "# instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"/root/.cache/torch/sentence_transformers/GanymedeNil_text2vec-large-chinese\",\n",
    "#                                                       # local_files_only=True,\n",
    "#                                                       model_kwargs={\"device\": \"cuda\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "191b864a-b415-4f73-84b7-1971a2b326c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mchinese-alpaca-2-13b-16k-q6_k.gguf\u001b[0m*  \u001b[34;42mchinese-alpaca-2-7b\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls /data/python/laby/models/LLM/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "02f18725-96a9-4256-b6aa-6f4035a05930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mchinese-alpaca-2-13b-16k-q6_k.gguf\u001b[0m*  \u001b[34;42mchinese-alpaca-2-7b\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls /data/python/laby/models/LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "387dbd68-2378-4179-b9aa-664bda87d28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mlfs\u001b[0m/  \u001b[01;34mLLM\u001b[0m/  \u001b[01;34mtext2vec-bge-large-chinese\u001b[0m/  \u001b[01;34mtransformers.js\u001b[0m/  \u001b[01;34mXenova\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls /data/python/laby/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b182cc3-cd5c-4c78-afba-93d4e2feae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /data/python/laby/models/LLM/chinese-alpaca-2-7b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c55fe2fc-6b31-4836-a741-5e342ac8dbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8.0K\n",
      "drwxr-xr-x  6 root root   65 Nov 17 13:34 .\n",
      "drwxr-xr-x 14 root root 4.0K Nov 17 13:34 ..\n",
      "drwxr-xr-x  2 root root 4.0K Nov 17 13:34 blobs\n",
      "drwxr-xr-x  3 root root   54 Nov 17 10:34 .no_exist\n",
      "drwxr-xr-x  2 root root   18 Nov 17 10:34 refs\n",
      "drwxr-xr-x  3 root root   54 Nov 17 10:34 snapshots\n"
     ]
    }
   ],
   "source": [
    "!ls -lah /root/.cache/huggingface/hub/models--FlagAlpha--Llama2-Chinese-7b-Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe883730-cd81-4e7c-8837-20bd2ba2f416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blobs  refs  snapshots\n"
     ]
    }
   ],
   "source": [
    "!ls /root/.cache/huggingface/hub/models--FlagAlpha--Llama2-Chinese-7b-Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b58a01e-0a64-4169-bab6-2f2ddad42276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_dir = \"/root/.cache/huggingface/hub/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "894fb2f1-7d0c-426f-a9c0-58b7ff6dfbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 11G\n",
      "drwxr-xr-x 3 root root  87 Nov 20 10:43 .\n",
      "drwxr-xr-x 7 root root 123 Nov 20 08:18 ..\n",
      "-rwxr-xr-x 1 root root 11G Nov 20 08:21 chinese-alpaca-2-13b-16k-q6_k.gguf\n",
      "drwxrwxrwx 2 laby laby 301 Nov 20 08:18 chinese-alpaca-2-7b\n"
     ]
    }
   ],
   "source": [
    "!ls -lah /data/python/laby/models/LLM/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "48dc9af4-f2f8-401c-b9ab-5dc21420f575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 241M\n",
      "drwxr-xr-x 15 root root 4.0K Nov 20 12:41 .\n",
      "drwxr-xr-x  5 root root   61 Oct 24 15:26 ..\n",
      "drwxr-xr-x  4 root root   94 Nov 13 15:53 .locks\n",
      "drwxr-xr-x  6 root root   65 Aug 21 09:16 models--bigscience--bloom-1b7\n",
      "drwxr-xr-x  6 root root   65 Oct 24 18:26 models--bigscience--bloom-7b1\n",
      "drwxr-xr-x  6 root root   65 Nov 17 13:34 models--FlagAlpha--Llama2-Chinese-7b-Chat\n",
      "drwxr-xr-x  6 root root   65 Nov 17 12:14 models--FlagAlpha--Llama2-Chinese-7b-Chat_original_copy\n",
      "drwxr-xr-x  6 root root   65 Nov 20 12:41 models--hfl--chinese-alpaca-2-7b\n",
      "drwxr-xr-x  6 root root   65 Oct 24 11:46 models--meta-llama--Llama-2-7b-chat-hf\n",
      "drwxr-xr-x  6 root root   65 Sep  5 00:14 models--shibing624--text2vec-base-chinese\n",
      "drwxr-xr-x  2 root root    6 Aug 29 13:57 models--TheBloke--Llama-2-7B-GGML\n",
      "drwxr-xr-x  6 root root   65 Oct 26 13:21 models--tiiuae--falcon-7b\n",
      "drwxr-xr-x  5 root root   48 Nov 13 09:43 models--vivo-ai--BlueLM-7B-Chat-32K\n",
      "drwxr-xr-x  5 root root   48 Nov 13 15:54 models--vivo-ai--BlueLM-7B-Chat-4bits\n",
      "drwxr-xr-x  5 root root   48 Oct 26 09:36 models--ysw96--my_awesome_peft_model\n",
      "-rw-------  1 root root  80M Oct 26 08:22 tmpd_8zze2t\n",
      "-rw-------  1 root root 160M Nov 17 10:35 tmpewibny_1\n",
      "-rw-------  1 root root    0 Oct 24 11:18 tmpxe5e_q0g\n",
      "-rw-r--r--  1 root root    1 Jul 26 16:10 version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -lah /root/.cache/huggingface/hub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6342862-27d6-481e-916a-193bfd46733d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "628b5b5e-980c-4d23-b88f-f61b0604da4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "# shutil.copytree(\n",
    "#     os.path.join(llm_dir, \"chinese-alpaca-2-7b\"),\n",
    "#     os.path.join(llm_dir, \"chinese-alpaca-2-7b_original_copy\"),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67a2dd34-ae0a-45bf-b8b0-95c7508d9f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 14:03:32.941983: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-20 14:03:33.684083: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8187ddf5-250a-4a67-9922-c7c70bfffb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "932a7865-f955-4947-b8e5-160060de1275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 20 13:50:20 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.06              Driver Version: 545.23.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A40                     Off | 00000000:AF:00.0 Off |                    0 |\n",
      "|  0%   27C    P8              14W / 300W |      7MiB / 46068MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cb05c41-f9d2-4a24-98f6-e118e8a2cf61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eafa612d-d896-437d-a4e0-5ab7e3fa307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os,os.path\n",
    "os.environ['HTTP_PROXY']=\"http://127.0.0.1:8098\"\n",
    "os.environ['HTTPS_PROXY']=\"http://127.0.0.1:8098\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "85ce43d2-086e-4935-ac83-cfc3e0325c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU times: user 54.8 s, sys: 57.3 s, total: 1min 52s\n",
    "# Wall time: 1h 25min 34s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "379301f3-a086-49e3-808b-e22b9573860f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dad1a8b7ecf4db4bc7762bed1d83db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52 s, sys: 19.1 s, total: 1min 11s\n",
      "Wall time: 17.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# MODEL_NAME = \"/data/python/laby/models/LLM/chinese-alpaca-2-7b\"\n",
    "MODEL_NAME = \"hfl/chinese-alpaca-2-7b\"\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    # trust_remote_code=True,\n",
    "    # local_files_only=True,\n",
    "    # quantization_config=bnb_config,\n",
    "    # temperature=0.0\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d4989fd-6e0e-4dab-b43a-943394372c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 20 13:48:05 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.06              Driver Version: 545.23.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A40                     Off | 00000000:AF:00.0 Off |                    0 |\n",
      "|  0%   30C    P0              69W / 300W |  20738MiB / 46068MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A    207387      C   /usr/local/bin/python3.10                 20666MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab1cb3c1-cc54-4011-b536-1be357aa0bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc3d4f61-a9e1-432a-903c-f095f1169961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6929256448 || all params: 6929256448 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b11ebf5d-2f7c-497f-96cf-9e9f7408a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e80da789-88f8-497d-b5da-18c98b321ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 159907840 || all params: 7089164288 || trainable%: 2.2556655975751596\n"
     ]
    }
   ],
   "source": [
    "lora_r = 64\n",
    "lora_alpha = 128\n",
    "lora_dropout = 0.05\n",
    "lora_target_modules = [\n",
    "    \"q_proj\",\n",
    "    \"v_proj\",\n",
    "    \"k_proj\",\n",
    "    \"o_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"down_proj\",\n",
    "    \"up_proj\",\n",
    "]\n",
    "\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    target_modules=lora_target_modules,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce8d661-2aa5-46c9-aa16-e6202549ed54",
   "metadata": {},
   "source": [
    "## Inference Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c3961c9-05a4-4c42-9abd-013a0f465033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": wareconn是什么?\n",
      ":\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    ": wareconn是什么?\n",
    ":\n",
    "\"\"\".strip()\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5cce3cf4-3a0e-499e-85fd-89e678de1ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.0\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64f0b7a6-df8a-410f-b7cb-235ca4bf3c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"max_new_tokens\": 200,\n",
       "  \"pad_token_id\": 2,\n",
       "  \"temperature\": 0.0,\n",
       "  \"top_p\": 0.7\n",
       "}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00174bf5-1f0e-4905-8ade-3117afb481bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      "wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      "CPU times: user 10.7 s, sys: 822 ms, total: 11.6 s\n",
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "device = \"cuda:0\"\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6964182-bf08-4390-90b9-efc6df3c3e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"\n",
    ": {data_point[\"问题\"]}\n",
    ": {data_point[\"答案\"]}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d357979-e9bf-4577-b4ce-a31d6631b5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c3cd6fa8ad40c48fcd291aae09e27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "train_data = dataset.shuffle().map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19a43542-4e4f-4a50-8fb1-88cd5ebebfe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['编号', '问题', '答案', '相似问题1', '相似问题2', '功能', '负责人', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 120\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f347701-7707-4791-87c0-0ce1acb7d6d8",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0354dc57-81c2-4633-8f68-5aa20b7872e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"experiments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd0a2198-603f-4d16-bebc-6193c9cf315d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f84f95b7563cee4f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f84f95b7563cee4f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir experiments/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5128496c-2579-4c00-8279-fdf55b964d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=1,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_steps=100,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    report_to=\"tensorboard\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62c05b51-e4a2-4086-8a9b-bb01b96cecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    args=training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4626204-5bcf-4a48-a8a2-536c18053587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:22, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.591400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.588100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.622500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.539000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.394000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.397600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.582800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.467400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.828000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.786900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.828200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.619200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.491200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.799400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.601900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.769500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.278000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.656800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.206200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.946500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.611400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.199100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.831200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.491100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.865100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.274000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.902600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.490100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.758800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.848600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.812700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.519200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.177300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.743800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.899400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.852800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.016100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.438500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.756000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.432400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.333200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.593700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.648900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.299000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.599700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.998500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.671700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.326400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.736700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.280100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.133700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.092200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.021100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.355700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.548500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.536600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.625700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.533700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.698600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.957800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.283400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.729300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.601100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.667700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.547900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.951200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.716900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.523300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.966100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.892600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.262500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.009200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.874900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.297300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.646700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.441800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.567800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.659300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.666300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.704900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.122600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.952100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.512200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.605600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.146600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.198700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.168900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.536400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.645400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.457600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.750100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.545200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.604800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.901700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.557500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.231900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.232900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.621900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 15s, sys: 9.52 s, total: 2min 24s\n",
      "Wall time: 2min 25s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=1.6625355882942676, metrics={'train_runtime': 144.7035, 'train_samples_per_second': 2.764, 'train_steps_per_second': 0.691, 'total_flos': 1896403294420992.0, 'train_loss': 1.6625355882942676, 'epoch': 3.33})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "db1b5d26-53e5-4e44-87c8-4427823d1e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"alpaca_cn_trained-model_warebot1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d5e23c-751f-4fbf-adf4-3e4ccf18952c",
   "metadata": {},
   "source": [
    "## Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b52b845-c641-4444-92f8-ee0313c115f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bee0dab48d7469aad8a523edd00c081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PEFT_MODEL = f\"ysw96/{peft_model_name}\"\n",
    "PEFT_MODEL = f\"./alpaca_cn_trained-model_warebot1.0\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    # trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = PeftModel.from_pretrained(model, PEFT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "509c128e-b571-48dc-98f5-fad0e56de67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 20 14:06:12 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.06              Driver Version: 545.23.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A40                     Off | 00000000:AF:00.0 Off |                    0 |\n",
      "|  0%   30C    P0              69W / 300W |  28190MiB / 46068MiB |      9%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A    208528      C   /usr/local/bin/python3.10                 28178MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ada604-8149-4d2e-a7a7-0e74e17e7697",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64ab659a-9227-4129-b867-a280033d65e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.0\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83803a2c-b51b-4978-925d-0c24c916ae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b86e4642-7405-431a-8427-7c50dfb6c390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": wareconn是什么?\n",
      ": Wareconn是一个提供售后服务的云平台，包括保修管理、维修服务、数据分析、设备管理APP和派工管理APP等功能，旨在提高售后服务的效率和质量，帮助企业实现数字化流程掌控、多系统串接、智慧决策，持续提升企业营运绩效。\n",
      "Wareconn平台提供安全可靠的售后服务云管理，支持多系统串接，实时掌控售后服务数据，推助工业 4.0 发展，赋能産品生命周期、价值链全周期管理与服务，持续提升企业营运绩效。\n",
      "Wareconn平台服务全球客户，提供多语言支持，可在不同时区作业，确保与全球客户无时差沟通与服务。同时，Wareconn平台提供灵活的账号设定，可设定角色权限，确保数据安全性。\n",
      "Wareconn平台致力于与客户共同成长，持续推助産品与服务的创新发展，为産品生命周期、价值链\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c0a4fea-f99e-425c-86c6-e4b60b3c6e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    ": {question}\n",
    ":\n",
    "\"\"\".strip()\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            input_ids=encoding.input_ids,\n",
    "            attention_mask=encoding.attention_mask,\n",
    "            generation_config=generation_config,\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    assistant_start = \":\"\n",
    "    response_start = response.find(assistant_start)\n",
    "    return response[response_start + len(assistant_start) :].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "86933ec7-480c-46bb-a122-31cf0b329fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wareconn是什么?\n",
      ": Wareconn是一个提供售后服务的云平台，包括保修管理、维修服务、数据分析、设备管理APP和派工管理APP等功能，旨在提高售后服务的效率和质量，帮助企业实现数字化流程掌控、多系统串接、智慧决策，持续提升企业营运绩效。\n",
      "Wareconn平台提供安全可靠的售后服务云管理，支持多系统串接，实时掌控售后服务数据，推助工业 4.0 发展，赋能産品生命周期、价值链全周期管理与服务，持续提升企业营运绩效。\n",
      "Wareconn平台服务全球客户，提供多语言支持，可在不同时区作业，确保与全球客户无时差沟通与服务。同时，Wareconn平台提供灵活的账号设定，可设定角色权限，确保数据安全性。\n",
      "Wareconn平台致力于与客户共同成长，持续推助産品与服务的创新发展，为産品生命周期、价值链\n"
     ]
    }
   ],
   "source": [
    "prompt = \"wareconn是什么?\"\n",
    "print(generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ece06092-e802-433d-a20c-dd02e40f1fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wareconn是什么?\n",
      ": Wareconn是一个提供售后服务的云平台，包括保修管理、维修服务、数据分析、设备管理APP和派工管理APP等功能，旨在提高售后服务的效率和质量，帮助企业实现数字化流程掌控、设备预测维保、高效服务决策，以及与供应链的有效沟通，推助工业 4.0 发展，赋能産品生命周期的数字化管理与服务。\n",
      "Wareconn平台通过整合内部系统与外部供应商，实现售后服务的数字化流程掌控，同时提供多系统串接与数据整合功能，有效解决数据孤岛问题，提升企业运营绩效。平台还提供设备管理APP和派工管理APP，协助企业实现设备维保的数字化流程掌控，以及与外部维修中心的有效沟通与费用结算管理。\n",
      "总之，Wareconn是一个提供售后服务的云平台，旨在帮助企业实现数字化流程掌控、设备预测维保、高效服务决策，以及与供应链\n"
     ]
    }
   ],
   "source": [
    "prompt = \"wareconn是什么?\"\n",
    "print(generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac373b5a-b7bb-4896-a1c8-bc95547520d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wareconn有哪些功能?\n",
      ": Wareconn有以下主要功能：零件供应商管理、物料供应商管理、售后服务管理、工程支援服务管理、进度维护服务管理、货物收发管理、设备管理、人员指派服务管理、厂内维修服务管理、数据统计核算、紧急维保管理、资产履历管理、资产转移申请、资产保修管理、资产维保服务管理、资产替换管理、资产购置申请、资产信息设定、资产位置设定、资产进度设定、资产紧急维保、资产保修预测、资产保修费用核算、资产保修合约管理、资产保修费用核算、资产保修进度维护、资产保修费用预测、资产保修合约设定、资产保修费用核算、资产保修进度维护、资产保修费用预测、资产保修合约设定、资产保修费用核算、资产保修进度维护、资产保修费用预测、资产保修合约设定、资产保修费用核算、资产保修进度维护、资产保修费用预测、资产保修合约设定、资产保修费用核算\n"
     ]
    }
   ],
   "source": [
    "prompt = \"wareconn有哪些功能?\"\n",
    "print(generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1fef590-5a89-4701-beee-97459b4f67ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 20 14:23:31 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.06              Driver Version: 545.23.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A40                     Off | 00000000:AF:00.0 Off |                    0 |\n",
      "|  0%   37C    P0             197W / 300W |  28382MiB / 46068MiB |     99%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A    208528      C   /usr/local/bin/python3.10                 28370MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72246354-7f75-4149-ae1c-6692358f57ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
