{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "753265c2-7934-4611-9ea1-6e0c8b78cdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 22 15:11:54 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.06              Driver Version: 545.23.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A40                     Off | 00000000:AF:00.0 Off |                    0 |\n",
      "|  0%   32C    P0              70W / 300W |      4MiB / 46068MiB |      3%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe1a488c-ca32-4ab4-8a45-3d9e53817849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys,os,os.path\n",
    "# os.environ['HTTP_PROXY']=\"http://127.0.0.1:8098\"\n",
    "# os.environ['HTTPS_PROXY']=\"http://127.0.0.1:8098\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c055d055-6317-4cee-939a-5ac1663cc6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fba1eca-bd02-47ef-a878-94e7be343673",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_1 = pd.read_excel(\"./data/(warebot1.0)工廠端匯總Q&A.xlsx\", sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "034764cf-a4b4-41c0-b5df-a2120120be79",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_2 = pd.read_excel(\"./data/(warebot1.0)平台開發Q&A.xlsx\", sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a5eb31e-07a6-447a-a74f-ce86e0c67c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_3 = pd.read_excel(\"./data/(warebot1.0)交管Q&A.xlsx\", sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed79f60a-f72e-4dd5-919b-0df470793f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_excels = [\n",
    "    excel_1,\n",
    "    excel_2,\n",
    "    excel_3\n",
    "]\n",
    "map_sheets = map(lambda excel_i: pd.concat(excel_i.values(), axis=0, ignore_index=True), all_excels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6816768-ccab-4783-b5c0-d2a747ec6e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "data = reduce(lambda x, y: pd.concat([x,y], axis=0, ignore_index=True), list(map_sheets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7491a97b-a366-4d53-8583-204c9bdb4f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\"问题\",\"答案\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b459568a-c18e-4dad-8623-dcf5514b74ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(374, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff319be5-8b55-4527-a90f-8c3e08d0d35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ab5491b-0bbd-4b7c-a6ad-b167a93a9cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'问题': '无法登入Wareconn员工账号？如何查寻',\n",
       " '答案': '進入管理員權限的服務中心點擊基本資料->帳號管理\\n1.點擊添加帳號\\n2.填寫工號，郵箱等信息，設置密碼，狀態選擇啟用\\n3.保存并退出\\n4.在帳號管理頁面的搜索欄輸入工號即可查詢'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "894fb2f1-7d0c-426f-a9c0-58b7ff6dfbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16G\n",
      "drwxr-xr-x 4 root root  174 Nov 21 11:28 .\n",
      "drwxr-xr-x 7 root root  123 Nov 20 08:18 ..\n",
      "drwxr-xr-x 2 root root  125 Nov 21 11:31 alpaca_cn_trained-model_warebot1.0\n",
      "-rwxr-xr-x 1 root root  11G Nov 20 08:21 chinese-alpaca-2-13b-16k-q6_k.gguf\n",
      "drwxrwxrwx 2 laby laby  301 Nov 20 08:18 chinese-alpaca-2-7b\n",
      "-rw-r--r-- 1 root root 5.3G Nov 20 16:32 chinese-alpaca-2-7b-q6_k.gguf\n"
     ]
    }
   ],
   "source": [
    "!ls -lah /data/python/laby/models/LLM/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48dc9af4-f2f8-401c-b9ab-5dc21420f575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 241M\n",
      "drwxr-xr-x 15 root root 4.0K Nov 20 12:41 .\n",
      "drwxr-xr-x  5 root root   61 Oct 24 15:26 ..\n",
      "drwxr-xr-x  4 root root   94 Nov 13 15:53 .locks\n",
      "drwxr-xr-x  6 root root   65 Aug 21 09:16 models--bigscience--bloom-1b7\n",
      "drwxr-xr-x  6 root root   65 Oct 24 18:26 models--bigscience--bloom-7b1\n",
      "drwxr-xr-x  6 root root   65 Nov 17 13:34 models--FlagAlpha--Llama2-Chinese-7b-Chat\n",
      "drwxr-xr-x  6 root root   65 Nov 17 12:14 models--FlagAlpha--Llama2-Chinese-7b-Chat_original_copy\n",
      "drwxr-xr-x  6 root root   65 Nov 20 12:41 models--hfl--chinese-alpaca-2-7b\n",
      "drwxr-xr-x  6 root root   65 Oct 24 11:46 models--meta-llama--Llama-2-7b-chat-hf\n",
      "drwxr-xr-x  6 root root   65 Sep  5 00:14 models--shibing624--text2vec-base-chinese\n",
      "drwxr-xr-x  2 root root    6 Aug 29 13:57 models--TheBloke--Llama-2-7B-GGML\n",
      "drwxr-xr-x  6 root root   65 Oct 26 13:21 models--tiiuae--falcon-7b\n",
      "drwxr-xr-x  5 root root   48 Nov 13 09:43 models--vivo-ai--BlueLM-7B-Chat-32K\n",
      "drwxr-xr-x  5 root root   48 Nov 13 15:54 models--vivo-ai--BlueLM-7B-Chat-4bits\n",
      "drwxr-xr-x  5 root root   48 Oct 26 09:36 models--ysw96--my_awesome_peft_model\n",
      "-rw-------  1 root root  80M Oct 26 08:22 tmpd_8zze2t\n",
      "-rw-------  1 root root 160M Nov 17 10:35 tmpewibny_1\n",
      "-rw-------  1 root root    0 Oct 24 11:18 tmpxe5e_q0g\n",
      "-rw-r--r--  1 root root    1 Jul 26 16:10 version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -lah /root/.cache/huggingface/hub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6342862-27d6-481e-916a-193bfd46733d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "628b5b5e-980c-4d23-b88f-f61b0604da4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "# shutil.copytree(\n",
    "#     os.path.join(llm_dir, \"chinese-alpaca-2-7b\"),\n",
    "#     os.path.join(llm_dir, \"chinese-alpaca-2-7b_original_copy\"),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67a2dd34-ae0a-45bf-b8b0-95c7508d9f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 15:12:06.222280: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-22 15:12:07.107384: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8187ddf5-250a-4a67-9922-c7c70bfffb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cb05c41-f9d2-4a24-98f6-e118e8a2cf61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85ce43d2-086e-4935-ac83-cfc3e0325c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU times: user 54.8 s, sys: 57.3 s, total: 1min 52s\n",
    "# Wall time: 1h 25min 34s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "379301f3-a086-49e3-808b-e22b9573860f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18cee2223b454a2d947e22d66e0bf993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56.7 s, sys: 30.7 s, total: 1min 27s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# MODEL_NAME = \"/data/python/laby/models/LLM/chinese-alpaca-2-7b\"\n",
    "MODEL_NAME = \"hfl/chinese-alpaca-2-7b\"\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    # trust_remote_code=True,\n",
    "    local_files_only=True,\n",
    "    # quantization_config=bnb_config,\n",
    "    # temperature=0.0\n",
    ")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_NAME)\n",
    "# tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33c5093b-562a-4a0c-9c68-e231898a3788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['down_proj', 'o_proj', 'q_proj', 'gate_proj', 'v_proj', 'up_proj', 'k_proj', 'lm_head']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = r'\\((\\w+)\\): Linear'\n",
    "linear_layers = re.findall(pattern, str(model.modules))\n",
    "target_modules = list(set(linear_layers))\n",
    "print(target_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d4989fd-6e0e-4dab-b43a-943394372c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 22 15:13:08 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.06              Driver Version: 545.23.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A40                     Off | 00000000:AF:00.0 Off |                    0 |\n",
      "|  0%   31C    P0              69W / 300W |  26946MiB / 46068MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     97706      C   /usr/local/bin/python3.10                 26934MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab1cb3c1-cc54-4011-b536-1be357aa0bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc3d4f61-a9e1-432a-903c-f095f1169961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6929256448 || all params: 6929256448 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b11ebf5d-2f7c-497f-96cf-9e9f7408a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.gradient_checkpointing_enable()\n",
    "# model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e80da789-88f8-497d-b5da-18c98b321ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 159907840 || all params: 7089164288 || trainable%: 2.2556655975751596\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/blob/main/scripts/training/run_sft.sh\n",
    "lora_r = 64\n",
    "lora_alpha = 128\n",
    "lora_dropout = 0.05\n",
    "lora_target_modules = [\n",
    "    \"q_proj\",\n",
    "    \"v_proj\",\n",
    "    \"k_proj\",\n",
    "    \"o_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"down_proj\",\n",
    "    \"up_proj\",\n",
    "]\n",
    "# https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/sft_scripts_zh#%E5%85%B3%E4%BA%8E%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8\n",
    "\n",
    "\"\"\"\n",
    "关于显存占用\n",
    "默认配置训练llama，单卡24G会OOM，可以删去脚本中的--modules_to_save ${modules_to_save} \\, \n",
    "即不训练embed_tokens和lm_head（这两部分参数量较大），只训练LoRA参数，单卡使用显存约21G。\n",
    "\"\"\"\n",
    "modules_to_save = [\n",
    "    \"embed_tokens\",\n",
    "    \"lm_head\"\n",
    "]\n",
    "# https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/config.py\n",
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    target_modules=lora_target_modules,\n",
    "    bias=\"none\", # default is \"none\", this will change the results of original base model even when adapter is not active.\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    inference_mode=False, ## default is false too # line 446\n",
    "    # modules_to_save=modules_to_save ## default is \"None\". List of modules apart from LoRA layers to be set as trainable and saved in the final checkpoint.\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89e3abd3-3ce5-4dce-9de5-7a46978fd22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 22 15:13:10 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.06              Driver Version: 545.23.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A40                     Off | 00000000:AF:00.0 Off |                    0 |\n",
      "|  0%   31C    P0              69W / 300W |  27578MiB / 46068MiB |      3%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     97706      C   /usr/local/bin/python3.10                 27566MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce8d661-2aa5-46c9-aa16-e6202549ed54",
   "metadata": {},
   "source": [
    "## Inference Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c3961c9-05a4-4c42-9abd-013a0f465033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": wareconn是什么?\n",
      ":\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    ": wareconn是什么?\n",
    ":\n",
    "\"\"\".strip()\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5cce3cf4-3a0e-499e-85fd-89e678de1ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "# generation_config.temperature = 0.0\n",
    "generation_config.do_sample = False\n",
    "# generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64f0b7a6-df8a-410f-b7cb-235ca4bf3c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"max_new_tokens\": 200,\n",
       "  \"pad_token_id\": 2\n",
       "}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00174bf5-1f0e-4905-8ade-3117afb481bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      "wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      ": wareconn是什么?\n",
      "CPU times: user 10.7 s, sys: 881 ms, total: 11.6 s\n",
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "device = \"cuda:0\"\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6964182-bf08-4390-90b9-efc6df3c3e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"\n",
    ": 在Wareconn系統上，{data_point[\"问题\"]}\n",
    ": {data_point[\"答案\"]}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True, max_length=512)\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d357979-e9bf-4577-b4ce-a31d6631b5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9aaa81b8e8409994b7499ef2df988e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/374 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = dataset.shuffle().map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19a43542-4e4f-4a50-8fb1-88cd5ebebfe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['问题', '答案', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 374\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f347701-7707-4791-87c0-0ce1acb7d6d8",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0354dc57-81c2-4633-8f68-5aa20b7872e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"experiments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd0a2198-603f-4d16-bebc-6193c9cf315d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-2f303cea05de3ffb\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-2f303cea05de3ffb\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir experiments/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5128496c-2579-4c00-8279-fdf55b964d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py\n",
    "# https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/sft_scripts_zh\n",
    "training_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=1, # script is 8 but wiki is 1 (hf default is 1)\n",
    "    num_train_epochs=2, #2 or 1\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    save_total_limit=3,\n",
    "    save_strategy=\"steps\",\n",
    "    # eval_steps=250,\n",
    "    # save_steps=500,\n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_steps=100, # alpaca cn didn't set this # OOM error if it's too large\n",
    "    optim=\"adamw_bnb_8bit\", # 6BG of VRAM #\"paged_adamw_8bit\", # default is \"adamw_torch\" (requires 24 GB)\n",
    "    # https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#optimizer-choice \n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    weight_decay=0, # default is 0\n",
    "    report_to=\"tensorboard\",\n",
    "    save_safetensors=False, # transformers' default is True\n",
    "    # gradient_checkpointing= ## not specify but default is False\n",
    "    ddp_find_unused_parameters=False, # default is False\n",
    "    ddp_timeout=30000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62c05b51-e4a2-4086-8a9b-bb01b96cecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    # data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer),\n",
    ")\n",
    "model.config.use_cache = False #line 421"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4626204-5bcf-4a48-a8a2-536c18053587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:27, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.830100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.177300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.899400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.866200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.053500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.741500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.259300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.308400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.542900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.567300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.640300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.5 s, sys: 2.09 s, total: 29.6 s\n",
      "Wall time: 29.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=2.7121367835998536, metrics={'train_runtime': 29.1727, 'train_samples_per_second': 3.428, 'train_steps_per_second': 3.428, 'total_flos': 399119270731776.0, 'train_loss': 2.7121367835998536, 'epoch': 0.27})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "458611bc-0070-48fe-b6c6-6ba57c3cd183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 22 15:13:52 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.06              Driver Version: 545.23.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A40                     Off | 00000000:AF:00.0 Off |                    0 |\n",
      "|  0%   41C    P0             168W / 300W |  44290MiB / 46068MiB |      7%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     97706      C   /usr/local/bin/python3.10                 44278MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db1b5d26-53e5-4e44-87c8-4427823d1e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"alpaca_cn_trained-model_warebot1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d5e23c-751f-4fbf-adf4-3e4ccf18952c",
   "metadata": {},
   "source": [
    "## Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b52b845-c641-4444-92f8-ee0313c115f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac7986bac204845bd36e6d7240c98d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PEFT_MODEL = f\"ysw96/{peft_model_name}\"\n",
    "PEFT_MODEL = f\"./alpaca_cn_trained-model_warebot1.0\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    # trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = PeftModel.from_pretrained(model, PEFT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "509c128e-b571-48dc-98f5-fad0e56de67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 21 10:10:39 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.06              Driver Version: 545.23.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A40                     Off | 00000000:AF:00.0 Off |                    0 |\n",
      "|  0%   31C    P0              69W / 300W |  45412MiB / 46068MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     22404      C   /usr/local/bin/python3.10                 45166MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ada604-8149-4d2e-a7a7-0e74e17e7697",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64ab659a-9227-4129-b867-a280033d65e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "# generation_config.temperature = 0.0\n",
    "generation_config.do_sample = False\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83803a2c-b51b-4978-925d-0c24c916ae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b86e4642-7405-431a-8427-7c50dfb6c390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": wareconn是什么?\n",
      ": Wareconn是一个提供售后服务的云平台，包括保修管理、维修服务、数据分析、设备管理APP和派工管理APP等功能，旨在提高售后服务的效率和质量，帮助企业实现数字化流程掌控、多系统串接、智慧决策，持续提升企业营运绩效。\n",
      "Wareconn平台提供安全可靠的售后服务云管理，支持多系统串接，实时掌控售后服务数据，推助工业 4.0 发展，赋能産品生命周期、价值链全周期管理与服务，持续提升企业营运绩效。\n",
      "Wareconn平台服务全球客户，提供多语言支持，可在不同时区作业，确保与全球客户无时差沟通与服务。同时，Wareconn平台提供灵活的账号设定，可设定角色权限，确保数据安全性。\n",
      "Wareconn平台致力于与客户共同成长，持续推助産品与服务的创新发展，为産品生命周期、价值链\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c0a4fea-f99e-425c-86c6-e4b60b3c6e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    ": {question}\n",
    ":\n",
    "\"\"\".strip()\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            input_ids=encoding.input_ids,\n",
    "            attention_mask=encoding.attention_mask,\n",
    "            generation_config=generation_config,\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    assistant_start = \":\"\n",
    "    response_start = response.find(assistant_start)\n",
    "    return response[response_start + len(assistant_start) :].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "86933ec7-480c-46bb-a122-31cf0b329fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wareconn是什么?\n",
      ": Wareconn是一个提供售后服务的云平台，包括保修管理、维修服务、数据分析、设备管理APP和派工管理APP等功能，旨在提高售后服务的效率和质量，帮助企业实现数字化流程掌控、多系统串接、智慧决策，持续提升企业营运绩效。\n",
      "Wareconn平台提供安全可靠的售后服务云管理，支持多系统串接，实时掌控售后服务数据，推助工业 4.0 发展，赋能産品生命周期、价值链全周期管理与服务，持续提升企业营运绩效。\n",
      "Wareconn平台服务全球客户，提供多语言支持，可在不同时区作业，确保与全球客户无时差沟通与服务。同时，Wareconn平台提供灵活的账号设定，可设定角色权限，确保数据安全性。\n",
      "Wareconn平台致力于与客户共同成长，持续推助産品与服务的创新发展，为産品生命周期、价值链\n"
     ]
    }
   ],
   "source": [
    "prompt = \"wareconn是什么?\"\n",
    "print(generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ece06092-e802-433d-a20c-dd02e40f1fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "派工APP能不能查看工程师工作数量统计？\n",
      ": 主管可以在首页点击【行事历】查看所有工程师的接单数量,也可以点击【统计核算】查看所有工程师的月/年度工作数量统计。\n",
      "工程师可以在首页点击【行事历】查看自己全部的接单数量,也可以点击【统计核算】查看自己月/年度的工作数量统计。\n",
      "厂家可以在首页点击【行事历】查看所有服务中心的接单数量,也可以点击【统计核算】查看所有服务中心的月/年度工作数量统计。\n",
      "客户可以在首页点击【行事历】查看所有服务中心的接单数量,也可以点击【统计核算】查看所有服务中心的月/年度工作数量统计。\n",
      "厂家可以在首页点击【统计核算】查看所有服务中心的月/年度工作数量统计。\n",
      "客户可以在首页点击【统计核算】查看所有服务中心的月/年度工作数量统计。\n",
      "主管可以在首页点击【统计核算】查看所有服务中心的月/年度工作数量统计。\n",
      "工程师\n"
     ]
    }
   ],
   "source": [
    "prompt = \"派工APP能不能查看工程师工作数量统计？\"\n",
    "print(generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac373b5a-b7bb-4896-a1c8-bc95547520d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wareconn有哪些功能?\n",
      ": Wareconn有以下主要功能：零件供应商管理、物料供应商管理、售后服务管理、工程支援服务管理、进度维护服务管理、货物收发管理、设备管理、人员指派服务管理、厂内维修服务管理、数据统计核算、紧急维保管理、资产履历管理、资产转移申请、资产保修管理、资产维保服务管理、资产替换管理、资产购置申请、资产信息设定、资产位置设定、资产进度设定、资产紧急维保、资产保修预测、资产保修费用核算、资产保修合约管理、资产保修费用核算、资产保修进度维护、资产保修费用预测、资产保修合约设定、资产保修费用核算、资产保修进度维护、资产保修费用预测、资产保修合约设定、资产保修费用核算、资产保修进度维护、资产保修费用预测、资产保修合约设定、资产保修费用核算、资产保修进度维护、资产保修费用预测、资产保修合约设定、资产保修费用核算\n"
     ]
    }
   ],
   "source": [
    "prompt = \"wareconn有哪些功能?\"\n",
    "print(generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1fef590-5a89-4701-beee-97459b4f67ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 20 14:23:31 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.06              Driver Version: 545.23.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A40                     Off | 00000000:AF:00.0 Off |                    0 |\n",
      "|  0%   37C    P0             197W / 300W |  28382MiB / 46068MiB |     99%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A    208528      C   /usr/local/bin/python3.10                 28370MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72246354-7f75-4149-ae1c-6692358f57ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
