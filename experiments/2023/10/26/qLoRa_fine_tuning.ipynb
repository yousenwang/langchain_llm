{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1eaf3f5-4903-4cdf-947d-8e00d1bfaf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os,os.path\n",
    "os.environ['HTTP_PROXY']=\"http://127.0.0.1:8098\"\n",
    "os.environ['HTTPS_PROXY']=\"http://127.0.0.1:8098\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9998a5d-448e-4c5e-a6c7-d3c928cab440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q bitsandbytes datasets accelerate loralib\n",
    "# !pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7e059d3-5877-4f38-8c82-d820dd010c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.0K\n",
      "drwxr-xr-x 8 root root 303 Oct 24 18:26 .\n",
      "drwxr-xr-x 5 root root  61 Oct 24 15:26 ..\n",
      "drwxr-xr-x 6 root root  65 Aug 21 09:16 models--bigscience--bloom-1b7\n",
      "drwxr-xr-x 6 root root  65 Oct 24 18:26 models--bigscience--bloom-7b1\n",
      "drwxr-xr-x 6 root root  65 Aug 30 22:52 models--FlagAlpha--Llama2-Chinese-7b-Chat\n",
      "drwxr-xr-x 6 root root  65 Oct 24 11:46 models--meta-llama--Llama-2-7b-chat-hf\n",
      "drwxr-xr-x 6 root root  65 Sep  5 00:14 models--shibing624--text2vec-base-chinese\n",
      "drwxr-xr-x 2 root root   6 Aug 29 13:57 models--TheBloke--Llama-2-7B-GGML\n",
      "-rw------- 1 root root   0 Oct 24 11:18 tmpxe5e_q0g\n",
      "-rw-r--r-- 1 root root   1 Jul 26 16:10 version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -lah /root/.cache/huggingface/hub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "263e80ff-e155-4c7b-8a02-84e060ea7273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.35.0.dev0\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /usr/local/lib/python3.10/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: peft, sentence-transformers\n"
     ]
    }
   ],
   "source": [
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3441f3da-8ce7-432d-b52a-c70da2e30462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA A40 (UUID: GPU-84d1dbbd-b6a7-03b8-feb8-3ecfdf6b7b02)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47f48e22-a6ca-4fa9-add2-f317b477c51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 08:20:32.873722: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-26 08:20:33.578556: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a33dc848-0644-403d-8a1e-43a9f762cdca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b29aa048-bd6b-4f49-a72e-9b4392026bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "569fb1f5-e493-49b8-98d9-43ca40d1ee55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93abf757e08420ba38442b634150483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MODEL_NAME = \"tiiuae/falcon-7b\"\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd698a99-1a8c-42a3-975e-e6dd3e195ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 26 08:23:02 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A40                     Off | 00000000:AF:00.0 Off |                    0 |\n",
      "|  0%   28C    P0              68W / 300W |  31984MiB / 46068MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     21224      C   /data/python/laby/llama.cpp/server         7538MiB |\n",
      "|    0   N/A  N/A     21227      C   /data/python/laby/llama.cpp/server        11752MiB |\n",
      "|    0   N/A  N/A     46204      C   /usr/local/bin/python3.10                  8332MiB |\n",
      "|    0   N/A  N/A    178126      C   /usr/local/bin/python3.10                  4328MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f40485e1-1212-4db3-be03-7c18d434390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "177940a5-dad8-47f6-84ae-551985c2728d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 262410240 || all params: 3500412928 || trainable%: 7.496550989769399\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a556439-1c76-4a0b-8a51-b9e20f80c657",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8c289bd-6ee6-4a2b-8f5c-bac5780c08b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 39976960 || all params: 3540389888 || trainable%: 1.1291682911958425\n"
     ]
    }
   ],
   "source": [
    "# config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=32,\n",
    "#     target_modules=[\"query_key_value\"],\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "# )\n",
    "\n",
    "lora_r = 16\n",
    "lora_alpha = 64\n",
    "lora_dropout = 0.1\n",
    "lora_target_modules = [\n",
    "    \"q_proj\",\n",
    "    \"up_proj\",\n",
    "    \"o_proj\",\n",
    "    \"k_proj\",\n",
    "    \"down_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"v_proj\",\n",
    "]\n",
    "\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    target_modules=lora_target_modules,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf00654b-b1c7-4833-a063-47511050cbcf",
   "metadata": {},
   "source": [
    "## Inference Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "281e2469-2c5b-4ddf-9faa-c34d583a0a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": How to jump to a designated workstation?\n",
      ":\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    ": How to jump to a designated workstation?\n",
    ":\n",
    "\"\"\".strip()\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87b4b437-1b57-472e-923b-aec07fe2127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b26d6bf-e903-4c1a-a17e-c955d6930aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 1,\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"max_length\": 4096,\n",
       "  \"max_new_tokens\": 200,\n",
       "  \"pad_token_id\": 2,\n",
       "  \"temperature\": 0.7,\n",
       "  \"top_p\": 0.7\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7283b37a-2f4f-4b2c-90c8-cb200021780a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": How to jump to a designated workstation?\n",
      ": How to jump to a designated workstation?\n",
      "To jump to a designated workstation in a remote desktop environment, you can use the \"Jump\" feature in the remote desktop client. Here are the steps to follow:\n",
      "\n",
      "1. Open the remote desktop client: Open the remote desktop client on your local machine, and connect to the remote server.\n",
      "2. Select the workstation: In the remote desktop client, select the workstation you want to jump to from the list of available workstations.\n",
      "3. Click on the \"Jump\" button: In the workstation window, click on the \"Jump\" button located at the top of the window.\n",
      "4. Confirm the action: A pop-up window will appear asking if you want to jump to the selected workstation. Click \"Yes\" to confirm.\n",
      "5. Disconnect from the current workstation: Once you have confirmed the jump, you will be disconnected from the current workstation\n",
      "CPU times: user 17.7 s, sys: 443 ms, total: 18.1 s\n",
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "device = \"cuda:0\"\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cc0d767-34c0-45c1-b1a4-1dc112f0c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/qa_dataset.json\") as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50d2d321-1916-438e-a1d5-13c1be02e86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'How to jump to a designated workstation?',\n",
      " 'answer': 'You need to log in to the administrator account, go to the service '\n",
      "           'center backend ->progress control ->progress maintenance, enter '\n",
      "           'the [Warranty Items] page, check the corresponding product serial '\n",
      "           'number, drop down [Repair Operations], click [Progress '\n",
      "           'Maintenance], and select [Jump to the specified workstation] to '\n",
      "           'proceed with the jump.'}\n"
     ]
    }
   ],
   "source": [
    "pprint(data[0], sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aba7b21f-663c-4455-a638-6c61647456a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_dataset(\"json\", data_files=\"./data/qa_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5454b818-0242-4171-83a3-f42aa812e295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"./data/warreconn_data.csv\")\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ed5de9-4312-4ea2-a056-53e463305542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c93f7ff-aba5-46e8-8334-e965c48e6462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"\n",
    ": {data_point[\"question\"]}\n",
    ": {data_point[\"answer\"]}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0dd63a97-911a-4b44-a9ee-660e41d7e45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3358d10b7646f58ac847200b1f42bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "train_data = dataset.shuffle().map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14efec96-db34-48dd-9040-b65962b4c51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 6\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7f9edd-7e67-47ce-a8c9-d70a74e945ee",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75c852d8-f8da-4741-8b1a-5abbdc8a8c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"experiments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d52eef9-e6f4-43bc-b26c-eb0f718c303a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6074ae0510684103\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6074ae0510684103\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir experiments/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c937bb14-2b85-411b-a6c7-b1c58bb113d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=1,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_steps=80,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    report_to=\"tensorboard\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "76d9169c-3e9f-40ee-ac07-74e90aed3321",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    args=training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b6aef64a-6e10-41cb-8d6f-1c6d85c2b2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [80/80 02:58, Epoch 53/80]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.543500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.712500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.528900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.074400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.738900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.377300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.857800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.352800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.955600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.672700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.495000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.227800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.142000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.078400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.071800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.110100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.133500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.114800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.038700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.065700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.039500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.057400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.071300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.033500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.033300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.065100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.028600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.025900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.026400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.024900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.028200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.022600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.026100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.024700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.024500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.030400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.023700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.023600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.024100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.025800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.024600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.020700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.023300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.025800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.021800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.029500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.020400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.023300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.023600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.022800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.022200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.023300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.022300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.022400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.022100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.022900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.023800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.022900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.021500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.022800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.022300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.021200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.021300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.024100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.021300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.021200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.024300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.021200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.023400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.020300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=80, training_loss=0.3362376464530826, metrics={'train_runtime': 181.1664, 'train_samples_per_second': 1.766, 'train_steps_per_second': 0.442, 'total_flos': 1076865933312000.0, 'train_loss': 0.3362376464530826, 'epoch': 53.33})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788f96d2-f805-4650-a4fe-ea92fcbc765f",
   "metadata": {},
   "source": [
    "## Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6901fecc-e580-4b26-99dc-87e228699bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"trained-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac960481-cdf7-49a9-845e-3e1ba47e57fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6aab22b298d4938bef19b15b1f1d83c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "80813bf8-ff27-4f5d-87eb-b610e56c1e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4fdbf1345642c2bacb457ae4c9a32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/160M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ysw96/my_awesome_peft_model/commit/5dee600e2598f32c2ee489b6290fc06c0cdf9677', commit_message='Upload model', commit_description='', oid='5dee600e2598f32c2ee489b6290fc06c0cdf9677', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model_name = \"my_awesome_peft_model\"\n",
    "model.push_to_hub(peft_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d2a1d3-4296-42f4-9e96-88dadfa98bc3",
   "metadata": {},
   "source": [
    "## Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "667641a3-b332-4b46-80dd-24fe40b2edb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc6ad759ea44b96a44dbcb918559919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/adapter_config.json:   0%|          | 0.00/573 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c591b5b3823c4c459aa9645bdbd8c3a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b1d7285c7a4bf1bd8684f7fd2a20c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading adapter_model.bin:   0%|          | 0.00/160M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PEFT_MODEL = f\"ysw96/{peft_model_name}\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = PeftModel.from_pretrained(model, PEFT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8792c714-64cf-4f1b-8955-1caae4ffca69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quant_method': <QuantizationMethod.BITS_AND_BYTES: 'bitsandbytes'>,\n",
       " 'load_in_8bit': False,\n",
       " 'load_in_4bit': True,\n",
       " 'llm_int8_threshold': 6.0,\n",
       " 'llm_int8_skip_modules': None,\n",
       " 'llm_int8_enable_fp32_cpu_offload': False,\n",
       " 'llm_int8_has_fp16_weight': False,\n",
       " 'bnb_4bit_quant_type': 'nf4',\n",
       " 'bnb_4bit_use_double_quant': True,\n",
       " 'bnb_4bit_compute_dtype': 'bfloat16'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.quantization_config.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "65396dbe-b8fe-4a11-a074-7434f1265ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 1,\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"max_length\": 4096,\n",
       "  \"max_new_tokens\": 200,\n",
       "  \"pad_token_id\": 2,\n",
       "  \"temperature\": 0.7,\n",
       "  \"top_p\": 0.7\n",
       "}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9da7d16f-a76f-4bd1-88e8-24a2690f391c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoraConfig(peft_type='LORA',\n",
      "           auto_mapping=None,\n",
      "           base_model_name_or_path='meta-llama/Llama-2-7b-chat-hf',\n",
      "           revision=None,\n",
      "           task_type='CAUSAL_LM',\n",
      "           inference_mode=True,\n",
      "           r=16,\n",
      "           target_modules=['down_proj',\n",
      "                           'v_proj',\n",
      "                           'k_proj',\n",
      "                           'o_proj',\n",
      "                           'gate_proj',\n",
      "                           'up_proj',\n",
      "                           'q_proj'],\n",
      "           lora_alpha=64,\n",
      "           lora_dropout=0.1,\n",
      "           fan_in_fan_out=False,\n",
      "           bias='none',\n",
      "           modules_to_save=None,\n",
      "           init_lora_weights=True,\n",
      "           layers_to_transform=None,\n",
      "           layers_pattern=None,\n",
      "           rank_pattern={},\n",
      "           alpha_pattern={})\n"
     ]
    }
   ],
   "source": [
    "pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134d406a-2858-41c0-b05a-9d5056a2667e",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "27d48dfd-b351-4885-a085-00fe6380e93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5053d12d-f7ed-4514-a75c-8f8b89c2aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23f4a9e7-8863-4051-aae1-aefffa67764e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": How can I create an account?\n",
      ": You need to go to the administrator account ->Warranty tasks ->Service progress. On the warranty project page, you can view the progress details page of the SN that has been repaired, as well as the work record details under the service progress. You can also view the specific work station's progress. By selecting an item and clicking the create work station button, you can create a work station for the SN that has been repaired. You need to enter the work record details and select the work station to proceed with the creation of the work station. By creating a work station, you can specify the work details and progress of the SN that has been repaired.\n",
      "\n",
      "As a warranty administrator, you need to create work stations for SNs that have been repaired. By creating work stations, you can specify the work details and progress of the SNs that have been repaired. This will help you manage the warranty\n",
      "CPU times: user 18.1 s, sys: 60.1 ms, total: 18.2 s\n",
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = f\"\"\"\n",
    ": How can I create an account?\n",
    ":\n",
    "\"\"\".strip()\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "73c22282-5754-457f-994b-013876bc455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    ": {question}\n",
    ":\n",
    "\"\"\".strip()\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            input_ids=encoding.input_ids,\n",
    "            attention_mask=encoding.attention_mask,\n",
    "            generation_config=generation_config,\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    assistant_start = \":\"\n",
    "    response_start = response.find(assistant_start)\n",
    "    return response[response_start + len(assistant_start) :].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "23e91a9c-9324-4b34-8cb6-b79f63d9c589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to jump to a designated workstation?\n",
      ": You need to log in to the administrator account, go to the service center backend ->progress control ->progress maintenance, enter the [Warranty Items] page, check the corresponding product serial number, drop down [Repair Operations], click [Progress Maintenance], and select [Jump to the specified workstation] to proceed with the jump. Select the workstation and click [Confirm] to proceed with the jump. You can also select [Drop off at the specified workstation] to drop off the product at the workstation for repair.\n",
      "\n",
      "It is important to note that jumping to a specified workstation requires the approval of the workstation and the service center. The workstation must select the product to be repaired and drop it off at the specified workstation for repair. The service center must also approve the jump and select the workstation for repair.\n",
      "\n",
      "Jumping to a specified workstation can save time and effort for both\n"
     ]
    }
   ],
   "source": [
    "prompt = \"How to jump to a designated workstation?\"\n",
    "print(generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a708b426-20db-4821-82da-d05efb25026b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to handle the discrepancy between the received repair products and the warranty application content?\n",
      ": Service Center Background ->Goods Receipt and Delivery ->Receipt and Delivery Management. When receiving goods in the `To be Received` page, if there is a discrepancy between the goods, you can select the product and click `Delete`. The product information will be returned to the customer for confirmation, and the customer can resubmit the application. By handling the discrepancy, the customer's application can be processed again, and the receipt and delivery of the replacement parts can be handled properly.\n",
      "\n",
      ": How to handle the discrepancy between the received repair products and the warranty application content?\n",
      "\n",
      "Service Center Background ->Goods Receipt and Delivery ->Receipt and Delivery Management. When receiving goods in the `To be Received` page, if there is a discrepancy between the goods, you can select the product and click `Delete`. The product information will be returned to the customer for\n"
     ]
    }
   ],
   "source": [
    "prompt = \"How to handle the discrepancy between the received repair products and the warranty application content?\"\n",
    "print(generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "53d24e53-1ea9-4d72-8b7d-c7da52cfe5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to modify the content of the label printed on the workstation?\n",
      ": For printing labels on workstations, the service center can contact the manufacturer for modification. The manufacturer's maintenance service backend ->engineering support ->workstation printing can select the corresponding printing template to modify the label content. For modification, the service center can contact the manufacturer for details. The manufacturer's maintenance service backend ->engineering support ->workstation printing can select the corresponding printing template to modify the label content. For modification, the service center can contact the manufacturer for details.\n",
      "\n",
      "For printing labels on workstations, the service center can contact the manufacturer for modification. The manufacturer's maintenance service backend ->engineering support ->workstation printing can select the corresponding printing template to modify the label content. For modification, the service center can contact the manufacturer for details.\n",
      "\n",
      "The service center can select the corresponding printing template to modify the label content for workstations. For modification, the service center can contact the manufacturer for details\n"
     ]
    }
   ],
   "source": [
    "prompt = \"How to modify the content of the label printed on the workstation?\"\n",
    "print(generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2ad5b492-d4f3-46b1-9f73-ffba90eaf3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where can customers view work station operation records?\n",
      ": Customer backend ->Warranty tasks ->Service progress. On the warranty project page, you can view the progress details page of the SN that has been repaired, as well as the work record details under the service progress. You can also view the specific work station's operation status. By selecting a work station and clicking the view button, you can view the work record details page, as well as the specific work station's operation status. By selecting a work station and clicking the view button, you can view the work record details page, as well as the specific work station's operation status. You can also view the work station's operation status during station operations. By selecting a work station and clicking the view button, you can view the work record details page, as well as the specific work station's operation status. You can also view the work station's operation status during station operations. By selecting a work station and clicking the view button, you\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Where can customers view work station operation records?\"\n",
    "print(generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d4d84c84-e026-4d6e-97e9-fecfb72e7064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where can customers see the replacement parts details for a single SN?\n",
      ": Customer backend ->Warranty tasks ->Service progress. On the warranty project page, you can view the progress details page of the SN that has been repaired, as well as the replacement spare parts details under service progress. You can also view the spare parts replacement status of the SN during station operations. By selecting an SN and viewing the spare parts replacement status, you can see the spare parts replacement details under service progress. You can also view the spare parts replacement status of the SN during station operations. By selecting an SN and viewing the spare parts replacement status, you can see the spare parts replacement details under service progress. You can also view the spare parts replacement status of the SN during station operations. By selecting an SN and viewing the spare parts replacement status, you can see the spare parts replacement details under service progress. You can also view the spare parts replacement status of the SN during station operations. By selecting an SN and viewing the spare parts replacement status\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Where can customers see the replacement parts details for a single SN?\"\n",
    "print(generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0e9b53f8-82d1-4e10-bd49-53c059eb5b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the consumer has not received the mailing label, where can the system download it?\n",
      ": Customer service can filter and download mailing labels in the system. At the goods receiving and sending area and on the goods record page, they can be filtered based on consumer information such as phone number and email address, as well as product serial number and logistics tracking number. By selecting an order and clicking the download button, the email labels can be downloaded and printed.\n",
      "\n",
      "By selecting an order and clicking the download button, the email labels can be downloaded and printed. Customer service can filter and download mailing labels in the system, as well as view the status of mailing labels and track the whereabouts of the mailing labels. By filtering based on consumer information such as phone number and email address, as well as product serial number and logistics tracking number, customer service can efficiently handle mailing label downloads and printing.\n",
      "\n",
      "By selecting an order and clicking the download button, the email labels can be downloaded and printed. By viewing the status of the mailing labels\n"
     ]
    }
   ],
   "source": [
    "prompt = \"If the consumer has not received the mailing label, where can the system download it?\"\n",
    "print(generate_response(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59625ec-a921-46e4-a1cb-ce79951590c0",
   "metadata": {},
   "source": [
    "## Similar question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e3ef4ae4-b223-4cda-8784-90a2867261e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What if the actual received goods do not match the warranty?\n",
      ": Customer service can filter and select the corresponding product information to be repaired, and enter the warranty application page to fill in the application content. On the warranty application page, select the product serial number and drop down [Warranty Application] to enter the application content. Select the corresponding product and drop down [Warranty Items] to select the replacement parts. Fill in the application content and select [Submit] to submit the application. If the product is damaged beyond repair, select [Replace] to select the replacement product and proceed with the application.\n",
      "\n",
      "Customer service can also view the warranty application content page to check the progress of the application. On the warranty application content page, select the product serial number and drop down [Warranty Application] to view the application content. Check the progress of the application and select [Submit] to submit the application. If the product is damaged beyond repair, select [Replace\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What if the actual received goods do not match the warranty?\"\n",
    "print(generate_response(prompt)) #Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6f1f03-2f88-44d3-9ded-7c54a46d3687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
