{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82e7b80c-1cfe-46cc-8324-c3d7e7bcc5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.document_transformers import DoctranTextTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a63ea281-382b-4472-9029-b18bc87d0742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 21 11:05:10 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   43C    P0              29W /  70W |      2MiB / 15360MiB |      7%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7e2f45f-286e-45d6-aa03-71b3b010cf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.0.268\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://www.github.com/hwchase17/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /usr/local/lib/python3.10/site-packages\n",
      "Requires: aiohttp, async-timeout, dataclasses-json, langsmith, numexpr, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2da1071-7fdb-4bf2-87d1-db5b49c1d4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.0K\n",
      "drwxr-xr-x 4 root root 108 2023-08-21__09:16:52 .\n",
      "drwxr-xr-x 3 root root  30 2023-08-16__14:49:17 ..\n",
      "drwxr-xr-x 6 root root  65 2023-08-21__09:16:52 models--bigscience--bloom-1b7\n",
      "drwxr-xr-x 6 root root  65 2023-08-16__17:33:10 models--meta-llama--Llama-2-7b-chat-hf\n",
      "-rw-r--r-- 1 root root   1 2023-07-26__16:10:47 version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -lah /root/.cache/huggingface/hub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05e2fd7b-2f9d-498c-a533-876650b4c7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys,os,os.path\n",
    "# os.environ['HTTP_PROXY']=\"http://127.0.0.1:8098\"\n",
    "# os.environ['HTTPS_PROXY']=\"http://127.0.0.1:8098\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16e3a37f-2b9a-4b3e-a021-fd5aecb8ca45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03baf158-e322-4aed-bad0-77cb6b901591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6b409b4-8678-4222-9312-1a753c4a06a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "247f003d-a643-454f-9b84-6d98025ca9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/bigscience/bloom-1b7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8f708942-cb93-4534-ba9b-889ea5685b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "91acc2a1-551f-407b-8c3a-79bb724920fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BloomTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "16ac1af5-f5dd-48fe-8bd0-155c443d2c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BloomTokenizerFast.from_pretrained(\n",
    "#     \"bigscience/bloom-1b7\",\n",
    "#     local_files_only=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1e0522d3-b385-44a5-9a1a-aea93dede9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"bigscience/bloom-1b7\",\n",
    "                                          local_files_only=True\n",
    "                                          # use_auth_token=True,\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "46010d89-3940-45dc-8af3-0653d29b91bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b8dd1ec4-5f36-4159-a9de-45c4a5a9107a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b73a95b5-43cc-4c41-9b71-9d781781ac33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_tokens',\n",
       " '_additional_special_tokens',\n",
       " '_auto_class',\n",
       " '_batch_encode_plus',\n",
       " '_bos_token',\n",
       " '_build_conversation_input_ids',\n",
       " '_call_one',\n",
       " '_cls_token',\n",
       " '_convert_encoding',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_create_repo',\n",
       " '_decode',\n",
       " '_decode_use_source_tokenizer',\n",
       " '_encode_plus',\n",
       " '_eos_token',\n",
       " '_eventual_warn_about_too_long_sequence',\n",
       " '_eventually_correct_t5_max_length',\n",
       " '_from_pretrained',\n",
       " '_get_files_timestamps',\n",
       " '_get_padding_truncation_strategies',\n",
       " '_in_target_context_manager',\n",
       " '_mask_token',\n",
       " '_pad',\n",
       " '_pad_token',\n",
       " '_pad_token_type_id',\n",
       " '_processor_class',\n",
       " '_save_pretrained',\n",
       " '_sep_token',\n",
       " '_set_processor_class',\n",
       " '_switch_to_input_mode',\n",
       " '_switch_to_target_mode',\n",
       " '_tokenizer',\n",
       " '_unk_token',\n",
       " '_upload_modified_files',\n",
       " 'add_prefix_space',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'additional_special_tokens',\n",
       " 'additional_special_tokens_ids',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'all_special_tokens_extended',\n",
       " 'as_target_tokenizer',\n",
       " 'backend_tokenizer',\n",
       " 'batch_decode',\n",
       " 'batch_encode_plus',\n",
       " 'bos_token',\n",
       " 'bos_token_id',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'can_save_slow_tokenizer',\n",
       " 'clean_up_tokenization',\n",
       " 'clean_up_tokenization_spaces',\n",
       " 'cls_token',\n",
       " 'cls_token_id',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'create_token_type_ids_from_sequences',\n",
       " 'decode',\n",
       " 'decoder',\n",
       " 'deprecation_warnings',\n",
       " 'encode',\n",
       " 'encode_plus',\n",
       " 'eos_token',\n",
       " 'eos_token_id',\n",
       " 'from_pretrained',\n",
       " 'get_added_vocab',\n",
       " 'get_special_tokens_mask',\n",
       " 'get_vocab',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'is_fast',\n",
       " 'mask_token',\n",
       " 'mask_token_id',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'max_model_input_sizes',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'name_or_path',\n",
       " 'num_special_tokens_to_add',\n",
       " 'pad',\n",
       " 'pad_token',\n",
       " 'pad_token_id',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'prepare_for_model',\n",
       " 'prepare_seq2seq_batch',\n",
       " 'pretrained_init_configuration',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'push_to_hub',\n",
       " 'register_for_auto_class',\n",
       " 'sanitize_special_tokens',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'sep_token',\n",
       " 'sep_token_id',\n",
       " 'set_truncation_and_padding',\n",
       " 'slow_tokenizer_class',\n",
       " 'special_tokens_map',\n",
       " 'special_tokens_map_extended',\n",
       " 'tokenize',\n",
       " 'train_new_from_iterator',\n",
       " 'truncate_sequences',\n",
       " 'truncation_side',\n",
       " 'unk_token',\n",
       " 'unk_token_id',\n",
       " 'verbose',\n",
       " 'vocab',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b7f5d2-0fe2-48cd-9f82-23c080eec181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "05ed3a17-9585-4091-ab66-378f7d3c8ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-1b7\",\n",
    "                                             local_files_only=True,\n",
    "                                             device_map='auto',\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             temperature=0\n",
    "                                             # use_auth_token=True,\n",
    "                                            #  load_in_8bit=True,\n",
    "                                            #  load_in_4bit=True\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f2451c65-8ceb-4014-bb47-e452e6221d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Mon Aug 21 11:38:07 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   51C    P0              29W /  70W |   6845MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A    165821      C   /usr/local/bin/python3.10                  6840MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "450732c9-883a-4480-bdff-2029694cc58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms import HuggingFacePipeline\n",
    "# import torch\n",
    "# llm = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=\"bigscience/bloom-1b7\",\n",
    "#     task=\"text-generation\",\n",
    "#     model_kwargs={\"temperature\": 0, \"max_length\": 1024},\n",
    "#     # device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb29db35-5da5-409e-83c7-2c7a33b2e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae221f91-788e-4f8b-921a-1ec3ed6ede7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/modules/model_io/models/chat/prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1f80a44e-39e3-4afb-b545-19f840cd9a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 512,\n",
    "                do_sample=True,\n",
    "                top_k=30,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e8385af0-7757-4202-b6ae-6d914043fcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "  If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "  {context}\n",
    "\n",
    "  Question: {question}\n",
    "  Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e3ffad67-d84c-43c3-b9b7-fa5a49a5f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b9fd0089-0c75-4a98-8be8-05648c0c7b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cc985c-0a36-40b3-a0b1-fd9109ca9d96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5b5f6ff7-1016-4391-ac90-2090b62102e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(\n",
    "        # output_language=\"Tranditional Chinese\",\n",
    "        # output_language=\"English\",\n",
    "        # output_language=\"Chinese\",\n",
    "        context='My name is Ethan. I live in Taiwan.',\n",
    "        question=\"What is my name?\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4eb59b4b-ca04-4460-9914-9e211d438b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import textwrap\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<>\\n\", \"\\n<>\\n\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
    "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "    return prompt_template\n",
    "\n",
    "def cut_off_text(text, prompt):\n",
    "    cutoff_phrase = prompt\n",
    "    index = text.find(cutoff_phrase)\n",
    "    if index != -1:\n",
    "        return text[:index]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def remove_substring(string, substring):\n",
    "    return string.replace(substring, \"\")\n",
    "\n",
    "\n",
    "\n",
    "def generate(text):\n",
    "    prompt = get_prompt(text)\n",
    "    with torch.autocast('cuda', dtype=torch.bfloat16):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "        outputs = model.generate(**inputs,\n",
    "                                 max_new_tokens=512,\n",
    "                                 eos_token_id=tokenizer.eos_token_id,\n",
    "                                 pad_token_id=tokenizer.pad_token_id,\n",
    "                                 )\n",
    "        final_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        final_outputs = cut_off_text(final_outputs, '')\n",
    "        final_outputs = remove_substring(final_outputs, prompt)\n",
    "\n",
    "    return final_outputs#, outputs\n",
    "\n",
    "def parse_text(text):\n",
    "        wrapped_text = textwrap.fill(text, width=100)\n",
    "        print(wrapped_text +'\\n\\n')\n",
    "        # return assistant_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b5b95fa7-e2b9-4e51-adad-bb57c7e52a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_generate(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "    outputs = model.generate(**inputs,\n",
    "                            max_new_tokens=512,\n",
    "                            eos_token_id=tokenizer.eos_token_id,\n",
    "                            pad_token_id=tokenizer.pad_token_id,\n",
    "                            )\n",
    "    final_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # final_outputs = cut_off_text(final_outputs, '')\n",
    "    final_outputs = remove_substring(final_outputs, prompt)\n",
    "    return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "34e01e83-d976-42ce-aa85-d065d521da3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' My name is Ethan. I live in Taiwan.\\n\\nA:\\n\\nThe answer is\\n\\n My name is Ethan. I live in Taiwan.\\n\\nThe question is\\n\\n What is my name?\\n\\nThe answer is\\n\\n My name is Ethan. I live in Taiwan.\\n\\nThe question is\\n\\n What is my name?\\n\\nThe answer is\\n\\n My name is Ethan. I live in Taiwan.\\n\\nThe question is\\n\\n What is my name?\\n\\nThe answer is\\n\\n My name is Ethan. I live in Taiwan.\\n\\nThe question is\\n\\n What is my name?\\n\\nThe answer is\\n\\n My name is Ethan. I live in Taiwan.\\n\\nThe question is\\n\\n What is my name?\\n\\nThe answer is\\n\\n My name is Ethan. I live in Taiwan.\\n\\nThe question is\\n\\n What is my name?\\n\\nThe answer is\\n\\n My name is Ethan. I live in Taiwan.\\n\\nThe question is\\n\\n What is my name?\\n\\nThe answer is\\n\\n My name is Ethan. I live in Taiwan.\\n\\nThe question is\\n\\n What is my name?\\n\\nThe answer is\\n\\n My name is Ethan. I live in Taiwan.\\n\\nThe question is\\n\\n What is my name?\\n\\nThe answer is\\n\\n My name is Ethan. I live in Taiwan.\\n\\nThe question is\\n\\n What is my name?\\n\\nThe answer is\\n\\n My name is Ethan. I live in Taiwan.\\n\\nThe question is\\n\\n What is my name?\\n\\nThe answer is\\n\\n My name is Ethan. I live in Taiwan.\\n\\nThe question is\\n\\n What is my name?\\n\\nThe answer is\\n\\n My name is Ethan. I live in Taiwan.\\n\\nThe question is\\n\\n What is my name?\\n\\nThe answer is\\n\\n My name is Ethan. I live in Taiwan.\\n\\nThe question is\\n\\n What is my name?\\n\\nThe answer is\\n\\n My name is Ethan. I live in Taiwan.\\n\\nThe question is\\n\\n What is my name?\\n\\nThe answer is\\n\\n My name is Ethan. I live in Taiwan.\\n\\nThe question is\\n\\n What is my name?\\n\\nThe answer is\\n\\n My name is Ethan. I live in Taiwan.\\n\\nThe question is\\n\\n What is my name?\\n\\nThe answer is\\n\\n My name is Ethan. I live in Taiwan.\\n\\nThe question is\\n\\n What is my name?\\n\\nThe answer is\\n\\n My name is Ethan. I live in Taiwan.\\n\\nThe question is\\n\\n What is my name?\\n\\nThe answer is\\n\\n My name is Ethan. I live in Taiwan.\\n\\nThe question is\\n\\n What is my name?\\n\\nThe answer is\\n\\n My name is Ethan. I live in Taiwan.\\n\\nThe question is\\n\\n What is my name?\\n\\nThe answer is\\n\\n My name is Ethan. I live in Taiwan.\\n\\nThe question is'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_generate(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fdcc8bc0-e723-49a4-88f6-d3d82d021f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device has 1 GPUs available. Provide device={deviceId} to `from_model_id` to use availableGPUs for execution. deviceId is -1 (default) for CPU and can be a positive integer associated with CUDA device id.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"bigscience/bloom-1b7\",\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\"temperature\": 0, \"max_length\": 512},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "90c37ab6-ab23-434a-be82-3e0379dc0260",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f90cecf-cfc9-4593-b8b2-a3cd31cf584b",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c863ea4a-29eb-4f2f-b4b3-7f4e69dbf5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context1 = \"\"\"\n",
    "  2 / 6 \n",
    "This document  belongs to Wareconn  Technology Services (Tianjin) Co., Ltd. It is only intended to be \n",
    "used to introduce Wareconn functions and procedures. Please do not use it for other purposes.  2 wareconn standard operating procedure  \n",
    "SOP  \n",
    "I. This guide will walk you through the process of requesting  RMA  through the \n",
    "Wareconn Customer Portal.  \n",
    "II. Details on how to fill in the Part Number (PN) and Serial Number (SN)  \n",
    "1.1 RMA request  procedure (Parts return)  \n",
    "1. System login  \n",
    "⚫ Module: https://www.wareconn.com/  \n",
    "⚫ Description:  Login  with account and password  \n",
    " \n",
    "To log in, follow these steps:  \n",
    "Go to https://www.wareconn.com/ and click Log in  \n",
    " \n",
    "Fill in your email and password, then click Login\n",
    " \n",
    "Click Enter  in the Customer portal3 / 6 \n",
    "This document  belongs to Wareconn  Technology Services (Tianjin) Co., Ltd. It is only intended to be \n",
    "used to introduce Wareconn functions and procedures. Please do not use it for other purposes.  3 wareconn standard operating procedure  \n",
    "2. Go to  RMA Request  Page \n",
    "⚫ Module: Customer portal -Warranty Claims -Warranty Claims  \n",
    "⚫ Description: Go to RMA request page  \n",
    " \n",
    "To go to the RMA request page, follow these steps:  \n",
    "Click Warranty Claims  in the left menu, then click +Add  \n",
    " \n",
    "Click the icon shown in the image below  \n",
    "(Select the corresponding SI )4 / 6 \n",
    "This document  belongs to Wareconn  Technology Services (Tianjin) Co., Ltd. It is only intended to be \n",
    "used to introduce Wareconn functions and procedures. Please do not use it for other purposes.  4 wareconn standard operating procedure  \n",
    "3. Create RMA Request  \n",
    "⚫ Module: Customer portal -Warranty Claims -Warranty Claims -Create request  \n",
    "⚫ Description: Create RMA request order  \n",
    " \n",
    "To create a RMA request, follow these steps:  \n",
    "Click Add Warranty Claims   \n",
    " \n",
    "Then, click Parts Return\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6e926e68-d4d7-457e-8517-ab911bdb72e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \"How can I navigate to the RMA request page in the Wareconn Customer Portal?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b0646911-545a-45eb-b7e5-d3d9db2bf9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = prompt_template.format(\n",
    "        # output_language=\"Tranditional Chinese\",\n",
    "        # output_language=\"English\",\n",
    "        # output_language=\"English\",\n",
    "        context=context1,\n",
    "        question=question1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8240ffff-1b36-4a96-bbf4-61eb9b59f440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Use the following pieces of context to answer the question at the end. \\n  If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n  \\n  2 / 6 \\nThis document  belongs to Wareconn  Technology Services (Tianjin) Co., Ltd. It is only intended to be \\nused to introduce Wareconn functions and procedures. Please do not use it for other purposes.  2 wareconn standard operating procedure  \\nSOP  \\nI. This guide will walk you through the process of requesting  RMA  through the \\nWareconn Customer Portal.  \\nII. Details on how to fill in the Part Number (PN) and Serial Number (SN)  \\n1.1 RMA request  procedure (Parts return)  \\n1. System login  \\n⚫ Module: https://www.wareconn.com/  \\n⚫ Description:  Login  with account and password  \\n \\nTo log in, follow these steps:  \\nGo to https://www.wareconn.com/ and click Log in  \\n \\nFill in your email and password, then click Login\\n \\nClick Enter  in the Customer portal3 / 6 \\nThis document  belongs to Wareconn  Technology Services (Tianjin) Co., Ltd. It is only intended to be \\nused to introduce Wareconn functions and procedures. Please do not use it for other purposes.  3 wareconn standard operating procedure  \\n2. Go to  RMA Request  Page \\n⚫ Module: Customer portal -Warranty Claims -Warranty Claims  \\n⚫ Description: Go to RMA request page  \\n \\nTo go to the RMA request page, follow these steps:  \\nClick Warranty Claims  in the left menu, then click +Add  \\n \\nClick the icon shown in the image below  \\n(Select the corresponding SI )4 / 6 \\nThis document  belongs to Wareconn  Technology Services (Tianjin) Co., Ltd. It is only intended to be \\nused to introduce Wareconn functions and procedures. Please do not use it for other purposes.  4 wareconn standard operating procedure  \\n3. Create RMA Request  \\n⚫ Module: Customer portal -Warranty Claims -Warranty Claims -Create request  \\n⚫ Description: Create RMA request order  \\n \\nTo create a RMA request, follow these steps:  \\nClick Add Warranty Claims   \\n \\nThen, click Parts Return\\n\\n\\n  Question: How can I navigate to the RMA request page in the Wareconn Customer Portal?\\n  Answer:\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d3c0c996-6c5f-47cf-a8a3-73ab1a080a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 51s, sys: 4.46 s, total: 10min 55s\n",
      "Wall time: 54.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Click the RMA Request  button in the left menu, then click +Add  \\n \\nClick the icon shown in the image below  \\n(Select the corresponding SI )5 / 6 \\nThis document  belongs to Wareconn  Technology Services (Tianjin) Co., Ltd. It is only intended to be \\nused to introduce Wareconn functions and procedures. Please do not use it for other purposes.  5 wareconn standard operating procedure  \\n4. Click the RMA Request  button in the left menu, then click +Add  \\n \\nClick the icon shown in the image below  \\n(Select the corresponding SI )6 / 6 \\nThis document  belongs to Wareconn  Technology Services (Tianjin) Co., Ltd. It is only intended to be \\nused to introduce Wareconn functions and procedures. Please do not use it for other purposes.  6 wareconn standard operating procedure  \\n3. Click the RMA Request  button in the left menu, then click +Add  \\n \\nClick the icon shown in the image below  \\n(Select the corresponding SI )7 / 6 \\nThis document  belongs to Wareconn  Technology Services (Tianjin) Co., Ltd. It is only intended to be \\nused to introduce Wareconn functions and procedures. Please do not use it for other purposes.  7 wareconn standard operating procedure  \\n2. Click the RMA Request  button in the left menu, then click +Add  \\n \\nClick the icon shown in the image below  \\n(Select the corresponding SI )8 / 6 \\nThis document  belongs to Wareconn  Technology Services (Tianjin) Co., Ltd. It is only intended to be \\nused to introduce Wareconn functions and procedures. Please do not use it for other purposes.  8 wareconn standard operating procedure  \\n1. Click the RMA Request  button in the left menu, then click +Add  \\n \\nClick the icon shown in the image below  \\n(Select the corresponding SI )9 / 6 \\nThis document  belongs to Wareconn  Technology Services (Tianjin) Co., Ltd. It is only intended to be \\nused to introduce Wareconn functions and procedures. Please do not use it for other purposes.  9 wareconn standard operating procedure  \\n1. Click the RMA Request  button in the left menu, then click +Add  \\n \\nClick the icon shown in the image below  \\n(Select the corresponding SI )10 / 6 \\nThis document  belongs to Warec'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm(\n",
    "    prompt=prompt1,\n",
    "    max_new_tokens=512,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5f7229ed-19b1-4b66-becd-72802b3d8e30",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "':' expected after dictionary key (<unknown>, line 8)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3508\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[86], line 1\u001b[0m\n    get_ipython().run_cell_magic('time', '', 'chain.invoke(\\n    {\\n        # \"output_language\" : \"Tranditional Chinese\",\\n        # \"output_language\" : \"Simplified Chinese\",\\n        # \"output_language\" : \"English\",\\n        \"context\" : context1,\\n        \"question\": question1,\\n        \"eos_token_id\"=tokenizer.eos_token_id,\\n        \"pad_token_id\"=tokenizer.pad_token_id\\n    }\\n)\\n')\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2478\u001b[0m in \u001b[1;35mrun_cell_magic\u001b[0m\n    result = fn(*args, **kwargs)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/usr/local/lib/python3.10/site-packages/IPython/core/magics/execution.py:1281\u001b[0m in \u001b[1;35mtime\u001b[0m\n    expr_ast = self.shell.compile.ast_parse(expr)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m/usr/local/lib/python3.10/site-packages/IPython/core/compilerop.py:86\u001b[0;36m in \u001b[0;35mast_parse\u001b[0;36m\n\u001b[0;31m    return compile(source, filename, symbol, self.flags | PyCF_ONLY_AST, 1)\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<unknown>:8\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"eos_token_id\"=tokenizer.eos_token_id,\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m ':' expected after dictionary key\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "chain.invoke(\n",
    "    {\n",
    "        # \"output_language\" : \"Tranditional Chinese\",\n",
    "        # \"output_language\" : \"Simplified Chinese\",\n",
    "        # \"output_language\" : \"English\",\n",
    "        \"context\" : context1,\n",
    "        \"question\": question1,\n",
    "        \"eos_token_id\"=tokenizer.eos_token_id,\n",
    "        \"pad_token_id\"=tokenizer.pad_token_id\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b019c898-3fa5-4dd1-9656-e726bd1e70b9",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4f911021-85eb-4bda-9eb3-ed865dd35b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "context2 = \"\"\"\n",
    "  6 / 6 \n",
    "This document  belongs to Wareconn  Technology Services (Tianjin) Co., Ltd. It is only intended to be \n",
    "used to introduce Wareconn functions and procedures. Please do not use it for other purposes.  6 wareconn standard operating procedure  \n",
    "5. Submit the RMA  request  \n",
    " \n",
    "⚫ Module: Customer portal -Warranty Claims -Warranty Claims -Submit  \n",
    "⚫ Description: Submit the RMA request  \n",
    " \n",
    "The Reminder field will show you whether this RMA request is approvable  \n",
    "(acceptable)  for warranty provider  or not.  \n",
    "Please remember to fill in WAL# in Customer No. field  \n",
    "Lastly, click Submit  in the top right corner to finish the  RMA requesting process2 / 6 \n",
    "This document  belongs to Wareconn  Technology Services (Tianjin) Co., Ltd. It is only intended to be \n",
    "used to introduce Wareconn functions and procedures. Please do not use it for other purposes.  2 wareconn standard operating procedure  \n",
    "SOP  \n",
    "I. This guide will walk you through the process of requesting  RMA  through the \n",
    "Wareconn Customer Portal.  \n",
    "II. Details on how to fill in the Part Number (PN) and Serial Number (SN)  \n",
    "1.1 RMA request  procedure (Parts return)  \n",
    "1. System login  \n",
    "⚫ Module: https://www.wareconn.com/  \n",
    "⚫ Description:  Login  with account and password  \n",
    " \n",
    "To log in, follow these steps:  \n",
    "Go to https://www.wareconn.com/ and click Log in  \n",
    " \n",
    "Fill in your email and password, then click Login\n",
    " \n",
    "Click Enter  in the Customer portal4 / 6 \n",
    "This document  belongs to Wareconn  Technology Services (Tianjin) Co., Ltd. It is only intended to be \n",
    "used to introduce Wareconn functions and procedures. Please do not use it for other purposes.  4 wareconn standard operating procedure  \n",
    "3. Create RMA Request  \n",
    "⚫ Module: Customer portal -Warranty Claims -Warranty Claims -Create request  \n",
    "⚫ Description: Create RMA request order  \n",
    " \n",
    "To create a RMA request, follow these steps:  \n",
    "Click Add Warranty Claims   \n",
    " \n",
    "Then, click Parts Return\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2c53381b-d74b-4d52-b237-c1570da519f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = \"What does the Reminder field indicate when submitting an RMA request?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9f442d52-fe72-4cd8-b87c-33feb22a49da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. \n",
      "  If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "  \n",
      "  6 / 6 \n",
      "This document  belongs to Wareconn  Technology Services (Tianjin) Co., Ltd. It is only intended to be \n",
      "used to introduce Wareconn functions and procedures. Please do not use it for other purposes.  6 wareconn standard operating procedure  \n",
      "5. Submit the RMA  request  \n",
      " \n",
      "⚫ Module: Customer portal -Warranty Claims -Warranty Claims -Submit  \n",
      "⚫ Description: Submit the RMA request  \n",
      " \n",
      "The Reminder field will show you whether this RMA request is approvable  \n",
      "(acceptable)  for warranty provider  or not.  \n",
      "Please remember to fill in WAL# in Customer No. field  \n",
      "Lastly, click Submit  in the top right corner to finish the  RMA requesting process2 / 6 \n",
      "This document  belongs to Wareconn  Technology Services (Tianjin) Co., Ltd. It is only intended to be \n",
      "used to introduce Wareconn functions and procedures. Please do not use it for other purposes.  2 wareconn standard operating procedure  \n",
      "SOP  \n",
      "I. This guide will walk you through the process of requesting  RMA  through the \n",
      "Wareconn Customer Portal.  \n",
      "II. Details on how to fill in the Part Number (PN) and Serial Number (SN)  \n",
      "1.1 RMA request  procedure (Parts return)  \n",
      "1. System login  \n",
      "⚫ Module: https://www.wareconn.com/  \n",
      "⚫ Description:  Login  with account and password  \n",
      " \n",
      "To log in, follow these steps:  \n",
      "Go to https://www.wareconn.com/ and click Log in  \n",
      " \n",
      "Fill in your email and password, then click Login\n",
      " \n",
      "Click Enter  in the Customer portal4 / 6 \n",
      "This document  belongs to Wareconn  Technology Services (Tianjin) Co., Ltd. It is only intended to be \n",
      "used to introduce Wareconn functions and procedures. Please do not use it for other purposes.  4 wareconn standard operating procedure  \n",
      "3. Create RMA Request  \n",
      "⚫ Module: Customer portal -Warranty Claims -Warranty Claims -Create request  \n",
      "⚫ Description: Create RMA request order  \n",
      " \n",
      "To create a RMA request, follow these steps:  \n",
      "Click Add Warranty Claims   \n",
      " \n",
      "Then, click Parts Return\n",
      "\n",
      "\n",
      "  Question: What does the Reminder field indicate when submitting an RMA request?\n",
      "  Answer:\n"
     ]
    }
   ],
   "source": [
    "prompt2 = prompt_template.format(\n",
    "        # output_language=\"Tranditional Chinese\",\n",
    "        # output_language=\"English\",\n",
    "        # output_language=\"English\",\n",
    "        context=context2,\n",
    "        question=question2\n",
    "    )\n",
    "print(prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "563cb37c-13b1-49c4-a450-f373b5e74efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 38s, sys: 4.06 s, total: 10min 42s\n",
      "Wall time: 53.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The Reminder field will show you whether this RMA request is approvable  \\n(acceptable)  for warranty provider  or not.  \\nPlease remember to fill in WAL# in Customer No. field  \\n \\n  Question: What does the Warranty Claims field indicate when submitting an RMA request?\\n  Answer: The Warranty Claims field will show you the warranty provider name and the warranty type.   \\n \\n  Question: What does the Serial Number field indicate when submitting an RMA request?\\n  Answer: The Serial Number field will show you the serial number of the part that you are requesting.   \\n \\n  Question: What does the Part Number (PN) field indicate when submitting an RMA request?\\n  Answer: The Part Number (PN) field will show you the part number of the part that you are requesting.   \\n \\n  Question: What does the Warranty Claims (WCC) field indicate when submitting an RMA request?\\n  Answer: The Warranty Claims (WCC) field will show you the warranty provider name and the warranty type.   \\n \\n  Question: What does the Warranty Claims (WCC) field indicate when submitting an RMA request?\\n  Answer: The Warranty Claims (WCC) field will show you the warranty provider name and the warranty type.   \\n \\n  Question: What does the Warranty Claims (WCC) field indicate when submitting an RMA request?\\n  Answer: The Warranty Claims (WCC) field will show you the warranty provider name and the warranty type.   \\n \\n  Question: What does the Warranty Claims (WCC) field indicate when submitting an RMA request?\\n  Answer: The Warranty Claims (WCC) field will show you the warranty provider name and the warranty type.   \\n \\n  Question: What does the Warranty Claims (WCC) field indicate when submitting an RMA request?\\n  Answer: The Warranty Claims (WCC) field will show you the warranty provider name and the warranty type.   \\n \\n  Question: What does the Warranty Claims (WCC) field indicate when submitting an RMA request?\\n  Answer: The Warranty Claims (WCC) field will show you the warranty provider name and the warranty type.   \\n \\n  Question: What does the Warranty Claims (WCC) field indicate when submitting an RMA request'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm(\n",
    "    prompt=prompt2, \n",
    "    # max_new_tokens=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5bf52e32-ac86-45cb-95c3-9e1b87bfa1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 36s, sys: 4.22 s, total: 10min 40s\n",
      "Wall time: 53.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The Reminder field will show you whether this RMA request is approvable  \\n(acceptable)  for warranty provider  or not.  \\nPlease remember to fill in WAL# in Customer No. field  \\n \\n  Question: What does the Warranty Claims field indicate when submitting an RMA request?\\n  Answer: The Warranty Claims field will show you the warranty provider name and the warranty type.   \\n \\n  Question: What does the Serial Number field indicate when submitting an RMA request?\\n  Answer: The Serial Number field will show you the serial number of the part that you are requesting.   \\n \\n  Question: What does the Part Number (PN) field indicate when submitting an RMA request?\\n  Answer: The Part Number (PN) field will show you the part number of the part that you are requesting.   \\n \\n  Question: What does the Warranty Claims (WCC) field indicate when submitting an RMA request?\\n  Answer: The Warranty Claims (WCC) field will show you the warranty provider name and the warranty type.   \\n \\n  Question: What does the Warranty Claims (WCC) field indicate when submitting an RMA request?\\n  Answer: The Warranty Claims (WCC) field will show you the warranty provider name and the warranty type.   \\n \\n  Question: What does the Warranty Claims (WCC) field indicate when submitting an RMA request?\\n  Answer: The Warranty Claims (WCC) field will show you the warranty provider name and the warranty type.   \\n \\n  Question: What does the Warranty Claims (WCC) field indicate when submitting an RMA request?\\n  Answer: The Warranty Claims (WCC) field will show you the warranty provider name and the warranty type.   \\n \\n  Question: What does the Warranty Claims (WCC) field indicate when submitting an RMA request?\\n  Answer: The Warranty Claims (WCC) field will show you the warranty provider name and the warranty type.   \\n \\n  Question: What does the Warranty Claims (WCC) field indicate when submitting an RMA request?\\n  Answer: The Warranty Claims (WCC) field will show you the warranty provider name and the warranty type.   \\n \\n  Question: What does the Warranty Claims (WCC) field indicate when submitting an RMA request'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chain.invoke(\n",
    "    {\n",
    "        # \"output_language\" : \"Tranditional Chinese\",\n",
    "        # \"output_language\" : \"Simplified Chinese\",\n",
    "        # \"output_language\" : \"English\",\n",
    "        \"context\" : context2,\n",
    "        \"question\": question2\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6db6e4a-b300-4496-a0a3-72f5b3eaa815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eb45b9-4ab0-42bc-a292-3d02a926a084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f2328a-ab0c-4675-84f3-b41b409681c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f208506d-07a8-4ff5-a715-48c77e86205a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67abd35-8bf2-4182-a16e-543970ff8de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f6d2e1a-f188-4b51-90a9-c4ed7f908fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1=\"\"\"\n",
    "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "  2 / 6 \n",
    "This document  belongs to Wareconn  Technology Services (Tianjin) Co., Ltd. It is only intended to be \n",
    "used to introduce Wareconn functions and procedures. Please do not use it for other purposes.  2 wareconn standard operating procedure  \n",
    "SOP  \n",
    "I. This guide will walk you through the process of requesting  RMA  through the \n",
    "Wareconn Customer Portal.  \n",
    "II. Details on how to fill in the Part Number (PN) and Serial Number (SN)  \n",
    "1.1 RMA request  procedure (Parts return)  \n",
    "1. System login  \n",
    "⚫ Module: https://www.wareconn.com/  \n",
    "⚫ Description:  Login  with account and password  \n",
    " \n",
    "To log in, follow these steps:  \n",
    "Go to https://www.wareconn.com/ and click Log in  \n",
    " \n",
    "Fill in your email and password, then click Login\n",
    " \n",
    "Click Enter  in the Customer portal3 / 6 \n",
    "This document  belongs to Wareconn  Technology Services (Tianjin) Co., Ltd. It is only intended to be \n",
    "used to introduce Wareconn functions and procedures. Please do not use it for other purposes.  3 wareconn standard operating procedure  \n",
    "2. Go to  RMA Request  Page \n",
    "⚫ Module: Customer portal -Warranty Claims -Warranty Claims  \n",
    "⚫ Description: Go to RMA request page  \n",
    " \n",
    "To go to the RMA request page, follow these steps:  \n",
    "Click Warranty Claims  in the left menu, then click +Add  \n",
    " \n",
    "Click the icon shown in the image below  \n",
    "(Select the corresponding SI )4 / 6 \n",
    "This document  belongs to Wareconn  Technology Services (Tianjin) Co., Ltd. It is only intended to be \n",
    "used to introduce Wareconn functions and procedures. Please do not use it for other purposes.  4 wareconn standard operating procedure  \n",
    "3. Create RMA Request  \n",
    "⚫ Module: Customer portal -Warranty Claims -Warranty Claims -Create request  \n",
    "⚫ Description: Create RMA request order  \n",
    " \n",
    "To create a RMA request, follow these steps:  \n",
    "Click Add Warranty Claims   \n",
    " \n",
    "Then, click Parts Return\n",
    "\n",
    "  Question: How can I navigate to the RMA request page in the Wareconn Customer Portal?\n",
    "  Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be52aef4-fc48-4754-944c-2e1b7cc10232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 515, but `max_length` is set to 512. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='1', generation_info=None)]], llm_output=None, run=[RunInfo(run_id=UUID('943d2026-09bc-4946-9c74-a8e7811f86bc'))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.generate([prompt1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c32ee7e-f30b-4dac-b44e-f5b0c46f5d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"Use the following pieces of context to answer the question at the end.If you don't know the answer, just say that you don't know, don't try to make up an answer.Answer the question in {output_language} language.--------------{context}\", additional_kwargs={}),\n",
       " HumanMessage(content='Question: What is my name?', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.chat import SystemMessage, HumanMessagePromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"Use the following pieces of context to answer the question at the end.\"\n",
    "                \"If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\n",
    "                \"Answer the question in {output_language} language.\"\n",
    "                \"--------------\"\n",
    "                \"{context}\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"Question: {question}\"\n",
    "            # \"Answer:\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "template.format_messages(\n",
    "    output_language=\"Tranditional Chinese\",\n",
    "    context='My name is Ethan. I live in Taiwan.',\n",
    "    question=\"What is my name?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5014bb7-5610-4cca-9796-e2de1988dffb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
