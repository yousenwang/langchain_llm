{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d459b057-571b-4042-b1df-9dc5890c90c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c72dafcd-98b7-4b29-9e75-bda59f144d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Sep  6 08:39:18 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A40                     Off | 00000000:AF:00.0 Off |                    0 |\n",
      "|  0%   34C    P0              70W / 300W |      4MiB / 46068MiB |      2%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2112339f-b690-4299-ab36-01216f5bd967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/InstructorEmbedding/instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n",
      "No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/GanymedeNil_text2vec-large-chinese. Creating a new one with MEAN pooling.\n",
      "2023-09-06 09:45:53.150624: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-06 09:45:53.994388: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"/root/.cache/torch/sentence_transformers/GanymedeNil_text2vec-large-chinese\",\n",
    "                                                      # local_files_only=True,\n",
    "                                                      model_kwargs={\"device\": \"cuda\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80519082-c050-4fc3-bd88-9734d5053689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Config',\n",
       " '__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_vars__',\n",
       " '__config__',\n",
       " '__custom_root_type__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__exclude_fields__',\n",
       " '__fields__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_validators__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__include_fields__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__json_encoder__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__post_root_validators__',\n",
       " '__pre_root_validators__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__schema_cache__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__try_update_forward_refs__',\n",
       " '__validators__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_calculate_keys',\n",
       " '_copy_and_set_values',\n",
       " '_decompose_class',\n",
       " '_enforce_dict_if_root',\n",
       " '_get_value',\n",
       " '_init_private_attributes',\n",
       " '_iter',\n",
       " 'aembed_documents',\n",
       " 'aembed_query',\n",
       " 'cache_folder',\n",
       " 'client',\n",
       " 'construct',\n",
       " 'copy',\n",
       " 'dict',\n",
       " 'embed_documents',\n",
       " 'embed_instruction',\n",
       " 'embed_query',\n",
       " 'encode_kwargs',\n",
       " 'from_orm',\n",
       " 'json',\n",
       " 'model_kwargs',\n",
       " 'model_name',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'query_instruction',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'update_forward_refs',\n",
       " 'validate']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(instructor_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfe36dbc-5e15-4635-a6bd-8be4f5516d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_query = instructor_embeddings.embed_query(\"What was the name mentioned in the conversation?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43efcbdb-7f5b-43e0-a258-51f5285038a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedded_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8795e528-9e91-49c7-89d9-dac010c41b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = instructor_embeddings.embed_documents(\n",
    "    [\n",
    "        \"Hi there!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"What's your name?\",\n",
    "        \"My friends call me World\",\n",
    "        \"Hello World!\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac819453-7035-4fc1-a9b5-704ec9b2ee47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c1fdd86-603c-4972-b70e-ec75543d210e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-06 14:53:51.179926: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-06 14:53:51.873394: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10152dc947441fc9e82e38ce3016b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# import sklearn\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "model_name = \"FlagAlpha/Llama2-Chinese-7b-Chat\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name,\n",
    "                                          local_files_only=True\n",
    "                                          # use_auth_token=True,\n",
    "                                         )\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                         local_files_only=True,\n",
    "                                         device_map='auto',\n",
    "                                         torch_dtype=torch.float16,\n",
    "                                         temperature=0.2, # must be strictly positive float\n",
    "                                         do_sample=True,\n",
    "                                         # use_auth_token=True,\n",
    "                                        #  load_in_8bit=True,\n",
    "                                        #  load_in_4bit=True\n",
    "                                         )\n",
    "pipe = pipeline(\"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer= tokenizer,\n",
    "            # return_full_text=True,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            max_new_tokens = 512,\n",
    "            do_sample=True,\n",
    "            top_k=30,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "template = \"\"\"使用以下上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答案。\n",
    "\n",
    "  {context}\n",
    "\n",
    "  问题: {question}\n",
    "  答案:\"\"\"\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\n",
    "        \"message\": \"this is get\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0154ee3d-5c67-4fe2-981d-70c168cf29b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompt(BaseModel):\n",
    "    context: str\n",
    "    question: str\n",
    "    model: str = Field(default=\"FlagAlpha/Llama2-Chinese-7b-Chat\")\n",
    "    temperature: float = Field(default=0.2)\n",
    "\n",
    "@app.post(\"/completion\")\n",
    "async def get_completion(input: Prompt):\n",
    "    input_dict = input.dict()\n",
    "\n",
    "    # return input_dict\n",
    "\n",
    "    ans = chain.invoke(\n",
    "        {\n",
    "            \"context\" : input_dict[\"context\"],\n",
    "            \"question\": input_dict[\"question\"],\n",
    "            \"temperature\": input_dict[\"temperature\"], # not sure\n",
    "            # \"eos_token_id\": tokenizer.eos_token_id,\n",
    "            # \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"completion\": ans[\"text\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "974cca8f-e323-48ec-bfcc-28d91e869c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "# Allow for asyncio to work within the Jupyter notebook cell\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4136a0f-7ed0-4364-8cf9-fe81187c495b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [26527]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:24260 - \"POST /completion HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [26527]\n"
     ]
    }
   ],
   "source": [
    "import uvicorn\n",
    "uvicorn.run(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06b47264-3f00-4896-a1ef-c9840753cedc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f5d918-c25b-4116-a3a0-9c525065ccde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
