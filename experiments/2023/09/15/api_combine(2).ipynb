{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfe36dbc-5e15-4635-a6bd-8be4f5516d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Sep 15 09:56:16 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A40                     Off | 00000000:AF:00.0 Off |                    0 |\n",
      "|  0%   31C    P0              69W / 300W |   7492MiB / 46068MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A    144654      C   /data/python/laby/llama.cpp/server         6700MiB |\n",
      "|    0   N/A  N/A    247563      C   ...ython/laby/text2vec/env/bin/python3      770MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5796da1f-29c6-4950-892b-1ffb97b83013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-15 11:22:56.751298: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-15 11:22:57.540354: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, StoppingCriteria, StoppingCriteriaList\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "715ddc32-f893-4025-b674-9e13c153c68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/GanymedeNil_text2vec-large-chinese. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"/root/.cache/torch/sentence_transformers/GanymedeNil_text2vec-large-chinese\",\n",
    "                                                      # local_files_only=True,\n",
    "                                                      model_kwargs={\"device\": \"cuda\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bf0fc1c-71fb-4158-86fa-b8275404fc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a27c0076-b97d-4b11-9562-e06f1ceb0a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = instructor_embeddings.client.encode([\n",
    "    \"你叫甚麼名字?\",\n",
    "    \"你的教授幾歲?\"\n",
    "], convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf693d1a-c4d5-466e-8569-4ab35d83acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj1 = instructor_embeddings.client\n",
    "passage_embedding = obj1.encode([\n",
    "    \"你叫甚麼名字?\",\n",
    "    \"你的教授幾歲?\"\n",
    "], convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe461bb9-d2d0-427d-b8de-4ca3a5c12387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: tensor([[1.0000, 0.3970],\n",
      "        [0.3970, 1.0000]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarity:\", util.cos_sim(query_embedding, passage_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "568b5cbf-b3c6-4c18-9f5a-c316c25c2702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4680, -0.0854, -0.7444,  ...,  0.6296, -0.8117,  0.1516],\n",
       "        [-0.6097,  0.9862,  0.3081,  ...,  0.0890,  0.4821, -0.9317]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "326d743b-ab8d-43da-8069-411293f99304",
   "metadata": {},
   "outputs": [],
   "source": [
    "passage_embedding = instructor_embeddings.client.encode([\n",
    "    \"我該怎麼稱呼您?\",\n",
    "    \"你的教授幾歲?\"\n",
    "], convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cf5c0e5-3c95-4e56-a106-443c4315e578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: [[1.0, 0.39702120423316956], [0.39702120423316956, 1.0000001192092896]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarity:\", util.cos_sim(query_embedding, passage_embedding).cpu().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d800881f-ac05-4e7c-96d5-63715ee94939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/GanymedeNil_text2vec-large-chinese. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "persist_directory = 'db_cn'\n",
    "\n",
    "vectordb = Chroma(persist_directory=persist_directory,\n",
    "                  embedding_function=instructor_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "901fbbe6-255d-45f8-a597-9b2179b53722",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9d417e6-85b0-41c7-b637-d68e4c93b2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "instructor_embeddings_en = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\",\n",
    "                                                      model_kwargs={\"device\": \"cuda\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "740c20f6-0943-4951-850a-4be73b4603d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding_en = instructor_embeddings_en.client.encode([\n",
    "    \"What is your name?\",\n",
    "    \"How old is your professor?\"\n",
    "], convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f0229eb-27b9-4617-b737-fa275cc46cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "passage_embedding_en = instructor_embeddings_en.client.encode([\n",
    "    \"How may I call you?\",\n",
    "    \"How old is your professor?\"\n",
    "], convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f9cb94f-8fd1-4522-82fa-b273897c5944",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = util.cos_sim(query_embedding_en, passage_embedding_en).cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8dfeb3e-8f14-4cc5-a353-52f712819348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "811a570b-1a91-4be6-9264-644691df15bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: [[0.6406745910644531, 0.5355279445648193], [0.5164580345153809, 0.9999999403953552]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarity:\", util.cos_sim(query_embedding_en, passage_embedding_en).cpu().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b79efda-b6fa-4f36-97c0-ec954921e577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "\n",
    "persist_directory_en = 'db'\n",
    "\n",
    "vectordb_en = Chroma(persist_directory=persist_directory_en,\n",
    "                  embedding_function=instructor_embeddings_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a09c37f-a3db-4430-9697-e7614fa353bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_en = vectordb_en.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75365f9b-d390-4215-969c-e7153f8738af",
   "metadata": {},
   "source": [
    "## ENG LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3623a017-604a-4550-bb8e-0d6f1f6d580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENG\n",
    "model_name_en = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer_en = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name_en,\n",
    "                                             local_files_only=True,\n",
    "                                             # use_fast=False\n",
    "                                          # use_auth_token=True,\n",
    "                                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f693cfb9-f78a-49f9-8c81-97059c436265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad083f1dfc45405d9d1c50a06b78e15d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_en = AutoModelForCausalLM.from_pretrained(model_name_en,\n",
    "                                                local_files_only=True,\n",
    "                                                device_map='auto',\n",
    "                                                torch_dtype=torch.float16,\n",
    "                                                temperature=0.2, # must be strictly positive float\n",
    "                                                do_sample=True,\n",
    "                                                # use_auth_token=True,\n",
    "                                                #  load_in_8bit=True,\n",
    "                                                #  load_in_4bit=True\n",
    "                                               )\n",
    "pipe_en = pipeline(\"text-generation\",\n",
    "            model=model_en,\n",
    "            tokenizer= tokenizer_en,\n",
    "            # return_full_text=True,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            max_new_tokens = 512,\n",
    "            do_sample=True,\n",
    "            top_k=30,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer_en.eos_token_id\n",
    "            )\n",
    "llm_en = HuggingFacePipeline(pipeline=pipe_en)\n",
    "\n",
    "template_en = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "\n",
    "prompt_template_en = PromptTemplate.from_template(\n",
    "    template_en\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e36c92a-db1a-48e2-9de5-67e54ae24564",
   "metadata": {},
   "source": [
    "## 中文 LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c1fdd86-603c-4972-b70e-ec75543d210e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d15910d1c0d46b7abd1b89abc79dc9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_name = \"FlagAlpha/Llama2-Chinese-7b-Chat\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name,\n",
    "                                          local_files_only=True,\n",
    "                                          use_fast=False\n",
    "                                          # use_auth_token=True,\n",
    "                                         )\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                         local_files_only=True,\n",
    "                                         device_map='auto',\n",
    "                                         torch_dtype=torch.float16,\n",
    "                                         temperature=0.2, # must be strictly positive float\n",
    "                                         do_sample=True,\n",
    "                                         # use_auth_token=True,\n",
    "                                        #  load_in_8bit=True,\n",
    "                                        #  load_in_4bit=True\n",
    "                                         )\n",
    "pipe = pipeline(\"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer= tokenizer,\n",
    "            # return_full_text=True,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            max_new_tokens = 512,\n",
    "            do_sample=True,\n",
    "            top_k=30,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3165162c-ef0c-4791-89af-f263b079b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"使用以下上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答案。\n",
    "\n",
    "  {context}\n",
    "\n",
    "  问题: {question}\n",
    "  答案:\"\"\"\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "chain_type_kwargs = {\n",
    "    \"prompt\": prompt_template\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7adeea33-245a-489a-adfd-80e92202964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain\n",
    "qa = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 retriever=retriever,\n",
    "                                 return_source_documents=True,\n",
    "                                 chain_type_kwargs=chain_type_kwargs\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "033b08b3-a762-4362-8502-240831a7061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \"如何设定判定逻辑?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26ea1b5e-1dad-4edb-8c59-0bbb2031c351",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/generation/utils.py:1411: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.59 s, sys: 745 ms, total: 9.34 s\n",
      "Wall time: 9.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res1 = qa({\"query\": question1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7993a829-8dc7-4e76-8559-530a83e8c49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': '如何设定判定逻辑?',\n",
       " 'result': ' 在厂家保修管理 后台->基础设定 ->判定逻辑 页面中，点击【添加】按钮，添加判定逻辑，选定判定逻辑，填写解码规则名称、说明、 SN样例，点击【编辑】按钮进行应用，填写 PPID/SN子串区段，依照子串区段的代表意义，可自定义规则。如下图中点击【月份 规则自定义】，进入自定义模块后，点击【新增码值】，可自定义月份解析规则，例如，可依 PPID/SN规则设定 A代表 10月，B代表 11月份，C代表 12月份。\\n',\n",
       " 'source_documents': [Document(page_content='选择是否设定附加规则 , 限制 SN长度或厂家代码范围等 , 设定完毕后 , 点击【保存】 , \\n完成该产品序列号判定逻辑设定 .', metadata={'source': 'chinese_pdf/保修管理-廠家sop-repair(去圖標).pdf', 'page': 7}),\n",
       "  Document(page_content='2.2 检验逻辑设定   \\n逻辑判定设置完成后 , 厂家保修管理 后台->基础设定 ->判定逻辑 , 点击进入逻辑判定\\n页面, 最下方输入 PPID, 点击【解析】检验逻辑设定 ;如下图, 可依规则自动解析出厂\\n时间等信息 . \\n \\n3. 保修类别  \\n厂家保修管理 后台->基础设定 ->保修类别，点击【新增设定】 , 设定需要为产品配置\\n的保修类别 ,填写服务类型、时效值及单位 , 点击【保存】完成 ; \\n \\n \\n4. 保修约定  \\n当产品保修及保修卡无法满足保修需求时 , 可添加保修约定 .', metadata={'source': 'chinese_pdf/保修管理-廠家sop-repair(去圖標).pdf', 'page': 8}),\n",
       "  Document(page_content='基础设定  \\n1. 判定命名  \\n厂家保修管理 后台->基础设定 ->判定命名 ,依用户习惯 , 自主设定相关栏位名称 , 设定\\n后, 系统相关栏位均以设定名称显示 . \\n \\n2. 判定逻辑  \\n设定PPID/SN的逻辑规则 , 用于解析保修起点日期等信息 . \\n2.1 添加/编辑判定逻辑  \\n厂家保修管理 后台->基础设定 ->判定逻辑 , 点击【添加】按钮 , 添加判定逻辑 ;选定判\\n定逻辑, 点击对该判定逻辑进行编辑 ; \\n \\n填写解码规则名称、说明、 SN样例；点击【编辑】按钮 进行应用 ; \\n \\n填写 PPID/SN子串区段 , 依照子串区段 的代表意义 , 可自定义规则  \\n如下图中点击【月份 规则自定义】 ，  进入自定义模块后 , 点击【新增码值】 , 可自定义\\n月份解析规则 , 例如 , 可依 PPID/SN规则设定 A代表 10月, B代表 11月份 , C代表 12月\\n份; \\n【添加一项自定义规则】可添加 PPID/SN子串逻辑自定义规则 .', metadata={'source': 'chinese_pdf/保修管理-廠家sop-repair(去圖標).pdf', 'page': 6})]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "468becf0-4c0e-45cf-b13a-fb7abb5d8d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_en = RetrievalQA.from_chain_type(llm=llm_en,\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 retriever=retriever_en,\n",
    "                                 return_source_documents=True,\n",
    "                                 # chain_type_kwargs=chain_type_kwargs\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1794b9af-3682-439d-a7dc-d83eea0b7ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \"How can I navigate to the RMA request page in the Wareconn Customer Portal?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13cd0b4b-23e3-417a-8538-aa2887cfafc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.12 s, sys: 178 ms, total: 3.29 s\n",
      "Wall time: 3.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'How can I navigate to the RMA request page in the Wareconn Customer Portal?',\n",
       " 'result': ' You can navigate to the RMA request page in the Wareconn Customer Portal by following the steps outlined in the document. Specifically, you will need to log in to the portal, click on the Warranty Claims menu, and then click on the +Add button to access the RMA request page.',\n",
       " 'source_documents': [Document(page_content='2 / 6 \\nThis document  belongs to Wareconn  Technology Services (Tianjin) Co., Ltd. It is only intended to be \\nused to introduce Wareconn functions and procedures. Please do not use it for other purposes.  2 wareconn standard operating procedure  \\nSOP  \\nI. This guide will walk you through the process of requesting  RMA  through the \\nWareconn Customer Portal.  \\nII. Details on how to fill in the Part Number (PN) and Serial Number (SN)  \\n1.1 RMA request  procedure (Parts return)  \\n1. System login  \\n⚫ Module: https://www.wareconn.com/  \\n⚫ Description:  Login  with account and password  \\n \\nTo log in, follow these steps:  \\nGo to https://www.wareconn.com/ and click Log in  \\n \\nFill in your email and password, then click Login\\n \\nClick Enter  in the Customer portal', metadata={'source': 'MSFT RMA request SOP v1.0.pdf', 'page': 1}),\n",
       "  Document(page_content='3 / 6 \\nThis document  belongs to Wareconn  Technology Services (Tianjin) Co., Ltd. It is only intended to be \\nused to introduce Wareconn functions and procedures. Please do not use it for other purposes.  3 wareconn standard operating procedure  \\n2. Go to  RMA Request  Page \\n⚫ Module: Customer portal -Warranty Claims -Warranty Claims  \\n⚫ Description: Go to RMA request page  \\n \\nTo go to the RMA request page, follow these steps:  \\nClick Warranty Claims  in the left menu, then click +Add  \\n \\nClick the icon shown in the image below  \\n(Select the corresponding SI )', metadata={'source': 'MSFT RMA request SOP v1.0.pdf', 'page': 2}),\n",
       "  Document(page_content='4 / 6 \\nThis document  belongs to Wareconn  Technology Services (Tianjin) Co., Ltd. It is only intended to be \\nused to introduce Wareconn functions and procedures. Please do not use it for other purposes.  4 wareconn standard operating procedure  \\n3. Create RMA Request  \\n⚫ Module: Customer portal -Warranty Claims -Warranty Claims -Create request  \\n⚫ Description: Create RMA request order  \\n \\nTo create a RMA request, follow these steps:  \\nClick Add Warranty Claims   \\n \\nThen, click Parts Return', metadata={'source': 'MSFT RMA request SOP v1.0.pdf', 'page': 3})]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "qa_en({\"query\": question1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a56067c-20ed-4855-91a3-a485ac28beeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Wed Sep 13 09:04:31 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A40                     Off | 00000000:AF:00.0 Off |                    0 |\n",
      "|  0%   38C    P0             208W / 300W |  40599MiB / 46068MiB |     95%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A    105825      C   /usr/local/bin/python3.10                 33102MiB |\n",
      "|    0   N/A  N/A    247554      C   /data/python/laby/llama.cpp/server         6700MiB |\n",
      "|    0   N/A  N/A    247563      C   ...ython/laby/text2vec/env/bin/python3      770MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570aec27-2c47-4727-a951-bbac34b93337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0154ee3d-5c67-4fe2-981d-70c168cf29b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List\n",
    "\n",
    "description = \"\"\"\n",
    "WareconnChat API gives you information about Wareconn platform\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "tags_metadata = [\n",
    "    {\n",
    "        \"name\": \"question_answering\",\n",
    "        \"description\": \"Enter an question and language. The LLM will answer the question based on some relevant contexts.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"embeddings\",\n",
    "        \"description\": \"Enter a list of strings and it will return a list of text embedding vector.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_relevant_docs\",\n",
    "        \"description\": \"Enter an question and it will return k most relevant docs.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"WareconnChat\",\n",
    "    description=description,\n",
    "    summary=\"Wareconn\",\n",
    "    version=\"0.0.1\",\n",
    "    terms_of_services=\"https://www.wareconn.com/Info/agreement\",\n",
    "    contact={\n",
    "        \"name\": \"Wareconn Technology Service(Tianjin)Co.,Ltd\",\n",
    "        \"url\": \"https://www.wareconn.com/\",\n",
    "        \"email\": \"service@wareconn.com\",\n",
    "    },\n",
    "    # license_info={\n",
    "    #     \"name\": \"Apache 2.0\",\n",
    "    #     \"url\": \"https://www.apache.org/licenses/LICENSE-2.0.html\",\n",
    "    # },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a0742a69-ae3b-40c0-babc-dbbe3d34b016",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompt(BaseModel):\n",
    "    # context: str\n",
    "    question: str\n",
    "    lang: str\n",
    "    model: str = Field(default=\"FlagAlpha/Llama2-Chinese-7b-Chat\")\n",
    "    temperature: float = Field(default=0.2)\n",
    "\n",
    "@app.post(\"/question_answering\", tags=[\"question_answering\"])\n",
    "async def get_completion(input: Prompt):\n",
    "    input_dict = input.dict()\n",
    "\n",
    "    if input_dict[\"lang\"] == \"en_us\":\n",
    "        \n",
    "        ans = qa_en({\"query\": input_dict[\"question\"]})\n",
    "    else:\n",
    "      \n",
    "        ans = qa({\"query\": input_dict[\"question\"]})\n",
    "    return ans\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\n",
    "        \"message\": \"this is get\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1205536a-8e19-46ee-aff5-12c23114838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Documents(BaseModel):\n",
    "    input: List[str]\n",
    "    lang: str\n",
    "@app.post(\"/embeddings\", tags=[\"embeddings\"])\n",
    "async def embed_docs(input: Documents):\n",
    "    input_dict = input.dict()\n",
    "    docs = input_dict[\"input\"]\n",
    "    \n",
    "    if input_dict[\"lang\"] == \"en_us\":\n",
    "        embed_docs = instructor_embeddings_en.embed_documents(docs)\n",
    "    else:\n",
    "        embed_docs = instructor_embeddings.embed_documents(docs)\n",
    "\n",
    "    data = [{\"embedding\": emb, \"index\": idx} for idx, emb in enumerate(embed_docs)]\n",
    "    return {\n",
    "        \"data\": data,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3a1dc10-b8a6-4c92-82e1-258a0b69c142",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question(BaseModel):\n",
    "    user_input:str\n",
    "    lang: str\n",
    "    k: Optional[int] = Field(default=3)\n",
    "\n",
    "\n",
    "@app.post(\"/get_relevant_docs\", tags=[\"get_relevant_docs\"])\n",
    "async def retrieve_docs(input_q: Question):\n",
    "    input_dict = input_q.dict()\n",
    "\n",
    "    if input_dict[\"lang\"] == \"en_us\":\n",
    "        retriever_en = vectordb_en.as_retriever(search_kwargs={\"k\": input_dict['k']})\n",
    "        ans = retriever_en.get_relevant_documents(input_dict[\"user_input\"])\n",
    "    else:\n",
    "        retriever = vectordb.as_retriever(search_kwargs={\"k\": input_dict['k']})\n",
    "        ans = retriever.get_relevant_documents(input_dict[\"user_input\"])\n",
    "\n",
    "    docs = list(map(lambda a: {\n",
    "            'page_content': a.page_content,\n",
    "            'source': a.metadata[\"source\"],\n",
    "            'page': a.metadata[\"page\"]}, ans))\n",
    "    return {\n",
    "        # \"user_input\": question[\"user_input\"],\n",
    "        \"relavant_docs\": docs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e76710e8-453e-4d9e-8a35-50b2928e6e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentences(BaseModel):\n",
    "    query_sentences: List[str]\n",
    "    passage_sentences: List[str]\n",
    "    lang: str\n",
    "@app.post(\"/sentences_similarity\", tags=[\"sentences_similarity\"])\n",
    "async def sentences_similarity(input: Sentences):\n",
    "    input_dict = input.dict()\n",
    "   \n",
    "    query_sentences = input_dict[\"query_sentences\"]\n",
    "    passage_sentences = input_dict[\"passage_sentences\"]\n",
    "    \n",
    "    if input_dict[\"lang\"] == \"en_us\":\n",
    "        sentence_embed = instructor_embeddings_en\n",
    "    else:\n",
    "        sentence_embed = instructor_embeddings\n",
    "        \n",
    "    query_embedding = sentence_embed.client.encode(\n",
    "            query_sentences,\n",
    "            convert_to_tensor=True\n",
    "        )\n",
    "    passage_embedding = sentence_embed.client.encode(\n",
    "            passage_sentences,\n",
    "            convert_to_tensor=True\n",
    "        )\n",
    "    return {\n",
    "        \"similarity\": util.cos_sim(query_embedding, passage_embedding).cpu().tolist(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e45e43d-a59f-429e-b32f-304d48c69ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc18f9f-0969-464c-b63d-338e3142a41f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "974cca8f-e323-48ec-bfcc-28d91e869c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "# Allow for asyncio to work within the Jupyter notebook cell\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4136a0f-7ed0-4364-8cf9-fe81187c495b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [86104]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8006 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:9375 - \"POST /sentences_similarity HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "import uvicorn\n",
    "uvicorn.run(app=app, host=\"127.0.0.1\", port=8006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b47264-3f00-4896-a1ef-c9840753cedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f5d918-c25b-4116-a3a0-9c525065ccde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
