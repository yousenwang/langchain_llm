{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d459b057-571b-4042-b1df-9dc5890c90c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c72dafcd-98b7-4b29-9e75-bda59f144d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep 11 09:12:12 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A40                     Off | 00000000:AF:00.0 Off |                    0 |\n",
      "|  0%   29C    P0              68W / 300W |      4MiB / 46068MiB |      4%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfe36dbc-5e15-4635-a6bd-8be4f5516d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_query = instructor_embeddings.embed_query(\"What was the name mentioned in the conversation?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43efcbdb-7f5b-43e0-a258-51f5285038a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedded_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8795e528-9e91-49c7-89d9-dac010c41b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = instructor_embeddings.embed_documents(\n",
    "    [\n",
    "        \"Hi there!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"What's your name?\",\n",
    "        \"My friends call me World\",\n",
    "        \"Hello World!\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac819453-7035-4fc1-a9b5-704ec9b2ee47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5796da1f-29c6-4950-892b-1ffb97b83013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 10:38:29.815694: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-11 10:38:30.702483: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, StoppingCriteria, StoppingCriteriaList\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e1ca325d-e847-4e03-99c0-24542a2825da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "\n",
    "    def __init__(self, stops = [], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = [stop.to(\"cuda\") for stop in stops]\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        for stop in self.stops:\n",
    "            if torch.all((stop == input_ids[0][-len(stop):])).item():\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "63ae2cc3-6c43-4e68-a3a2-73c38958b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"\\n\\n\"]\n",
    "stop_words_ids = [tokenizer(stop_word, return_tensors='pt')['input_ids'].squeeze() for stop_word in stop_words]\n",
    "stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a611a9c-7dae-468c-aaf5-d1b825136729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([    1, 29871,    13,    13])]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0be57114-c78e-4a1c-9031-0cff7d8a97c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 20611,    13,    13]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"*.\\n\\n\", return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2bff5e32-e2af-497c-96eb-dfff8acb5596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 29871,  1094,   263,  7047, 22055, 29892,   366,  2874, 29892,\n",
       "          2693, 29892,   322,  1243,  7047, 11104,   363,  5164,  8324, 29889,\n",
       "            13,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\" As a software engineer, you design, develop, and test software programs for various applications.\\n\\n\", return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4a6b4c6d-d95c-4398-a683-293606c4bb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_ids=[torch.tensor([13, 13])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ff7199d9-9da0-443c-810e-51188d3e2948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([13, 13])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2aabf622-2bd6-430b-b7cd-c937cc9db207",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops = stop_words_ids)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3623a017-604a-4550-bb8e-0d6f1f6d580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENG\n",
    "model_name_en = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer_en = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name_en,\n",
    "                                             local_files_only=True,\n",
    "                                             # use_fast=False\n",
    "                                          # use_auth_token=True,\n",
    "                                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f693cfb9-f78a-49f9-8c81-97059c436265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc04e3a48f13429a96b34bb792115eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_en = AutoModelForCausalLM.from_pretrained(model_name_en,\n",
    "                                                local_files_only=True,\n",
    "                                                device_map='auto',\n",
    "                                                torch_dtype=torch.float16,\n",
    "                                                temperature=0.2, # must be strictly positive float\n",
    "                                                do_sample=True,\n",
    "                                                # use_auth_token=True,\n",
    "                                                #  load_in_8bit=True,\n",
    "                                                #  load_in_4bit=True\n",
    "                                               )\n",
    "pipe_en = pipeline(\"text-generation\",\n",
    "            model=model_en,\n",
    "            tokenizer= tokenizer_en,\n",
    "            # return_full_text=True,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            max_new_tokens = 512,\n",
    "            do_sample=True,\n",
    "            top_k=30,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer_en.eos_token_id\n",
    "            )\n",
    "llm_en = HuggingFacePipeline(pipeline=pipe_en)\n",
    "\n",
    "template_en = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "\n",
    "prompt_template_en = PromptTemplate.from_template(\n",
    "    template_en\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "73158b85-e2cc-425c-a82b-b6a9e3b6bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_kwargs = {\n",
    "                    \"temperature\": 0.2,\n",
    "                    \"stopping_criteria\": stopping_criteria\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6cc09b39-0ee9-4156-8958-a82d2dbbc97e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"How are you? It's been a while since we last spoke.\\nI'm doing well, thanks for asking! It's great to hear from you again. How have you been?\\nI've been good, thanks for asking! It's great to hear from you again too. How's life been treating you lately?\\nI've been keeping busy with work and other things, but I'm always happy to catch up with you. How about you?\\nI'm glad to hear that you're keeping busy! It's always important to stay active and engaged in life. Is there anything new or exciting happening in your world?\\nI'm glad to hear that you're doing well! It's always great to hear from you and catch up on what's going on in your life. How about you? Anything new or exciting happening?\\nI'm glad to hear that you're doing well! It's always great to hear from you and catch up on what's going on in your life. How about you? Anything new or exciting happening?\\nI'm glad to hear that you're doing well! It's always great to hear from you and catch up on what's going on in your life. How about you? Anything new or exciting happening?\\nI'm glad to hear that you're doing well! It's always great to hear from you and catch up on what's going on in your life. How about you? Anything new or exciting happening?\\nI'm glad to hear that you're doing well! It's always great to hear from you and catch up on what's going on in your life. How about you? Anything new or exciting happening?\\nI'm glad to hear that you're doing well! It's always great to hear from you and catch up on what's going on in your life. How about you? Anything new or exciting happening?\\nI'm glad to hear that you're doing well! It's always great to hear from you and catch up on what's going on in your life. How about you? Anything new or exciting happening?\\nI'm glad to hear that you're doing well! It's always great to hear from you and catch up on what's going\"}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_en(\"How are you?\",\n",
    "        clean_up_tokenization_spaces=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5d3c44b8-c8c7-4496-b0b7-5b03c5d17bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_en = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "I am Ethan.\n",
    "\n",
    "Question: What is my name?\n",
    "Helpful Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "04ff9c5f-5f6d-4212-9404-99bb95353232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nI am Ethan.\\n\\nQuestion: What is my name?\\nHelpful Answer: Ethan's name is Ethan.\\n\\n\"}]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_en(prompt_en,\n",
    "        # clean_up_tokenization_spaces=True,\n",
    "        **generate_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b7052853-ffa9-439d-96d0-948b0d027910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nI am Ethan.\\n\\nQuestion: What is my name?\\nHelpful Answer: Ethan's name is Ethan.\\n\\n\"}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_en(prompt_en,\n",
    "        clean_up_tokenization_spaces=True,\n",
    "        **generate_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e1a4b1e0-7907-4f65-bddf-ef1b905c8219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nI am Ethan.\\n\\nQuestion: What is my name?\\nHelpful Answer: Ethan's name is Ethan.\\n\\n\"}]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_en(prompt_en,\n",
    "        clean_up_tokenization_spaces=True,\n",
    "        **generate_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c1fdd86-603c-4972-b70e-ec75543d210e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e07d070c72314a12aa1f1ae36998eed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_name = \"FlagAlpha/Llama2-Chinese-7b-Chat\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name,\n",
    "                                          local_files_only=True,\n",
    "                                          use_fast=False\n",
    "                                          # use_auth_token=True,\n",
    "                                         )\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                         local_files_only=True,\n",
    "                                         device_map='auto',\n",
    "                                         torch_dtype=torch.float16,\n",
    "                                         temperature=0.2, # must be strictly positive float\n",
    "                                         do_sample=True,\n",
    "                                         # use_auth_token=True,\n",
    "                                        #  load_in_8bit=True,\n",
    "                                        #  load_in_4bit=True\n",
    "                                         )\n",
    "pipe = pipeline(\"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer= tokenizer,\n",
    "            # return_full_text=True,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            max_new_tokens = 512,\n",
    "            do_sample=True,\n",
    "            top_k=30,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "template = \"\"\"使用以下上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答案。\n",
    "\n",
    "  {context}\n",
    "\n",
    "  问题: {question}\n",
    "  答案:\"\"\"\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85111412-069b-481b-b604-9f9eeaa8eee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 8492, 29889, 13, 13]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en.encode(\"tools.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "850b3c37-08e6-4e59-a354-108ad68adcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.32.0.dev0\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /usr/local/lib/python3.10/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: sentence-transformers\n"
     ]
    }
   ],
   "source": [
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7bcee21-bed3-406e-871e-ee8fbd38f94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0154ee3d-5c67-4fe2-981d-70c168cf29b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List\n",
    "app = FastAPI()\n",
    "class Prompt(BaseModel):\n",
    "    context: str\n",
    "    question: str\n",
    "    lang: str\n",
    "    model: str = Field(default=\"FlagAlpha/Llama2-Chinese-7b-Chat\")\n",
    "    temperature: float = Field(default=0.2)\n",
    "\n",
    "@app.post(\"/question_answering\")\n",
    "async def get_completion(input: Prompt):\n",
    "    input_dict = input.dict()\n",
    "\n",
    "    generate_kwargs = {\n",
    "                    \"temperature\": input_dict[\"temperature\"],\n",
    "                    \"stopping_criteria\": stopping_criteria\n",
    "                }\n",
    "\n",
    "    if input_dict[\"lang\"] == \"en_us\":\n",
    "\n",
    "        chain_en = LLMChain(\n",
    "            llm=llm_en,\n",
    "            prompt=prompt_template_en,\n",
    "            llm_kwargs=generate_kwargs\n",
    "        )\n",
    "        \n",
    "        ans = chain_en(\n",
    "            {\n",
    "                \"context\" : input_dict[\"context\"],\n",
    "                \"question\": input_dict[\"question\"],\n",
    "            },\n",
    "            return_only_outputs=True\n",
    "        )\n",
    "    else:\n",
    "        chain = LLMChain(\n",
    "            llm=llm,\n",
    "            prompt=prompt_template,\n",
    "            llm_kwargs=generate_kwargs\n",
    "        )\n",
    "        ans = chain(\n",
    "            {\n",
    "                \"context\" : input_dict[\"context\"],\n",
    "                \"question\": input_dict[\"question\"],\n",
    "            },\n",
    "            return_only_outputs=True,\n",
    "            \n",
    "        )\n",
    "    return {\n",
    "        \"completion\": ans[\"text\"]\n",
    "    }\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\n",
    "        \"message\": \"this is get\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "974cca8f-e323-48ec-bfcc-28d91e869c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "# Allow for asyncio to work within the Jupyter notebook cell\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f4136a0f-7ed0-4364-8cf9-fe81187c495b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [100062]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8001 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:17620 - \"POST /question_answering HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:17622 - \"POST /question_answering HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:17624 - \"POST /question_answering HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:17626 - \"POST /question_answering HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:17648 - \"POST /question_answering HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:17650 - \"POST /question_answering HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [100062]\n"
     ]
    }
   ],
   "source": [
    "import uvicorn\n",
    "uvicorn.run(app=app, host=\"127.0.0.1\", port=8001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06b47264-3f00-4896-a1ef-c9840753cedc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f5d918-c25b-4116-a3a0-9c525065ccde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
