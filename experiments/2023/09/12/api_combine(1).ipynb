{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfe36dbc-5e15-4635-a6bd-8be4f5516d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep 12 14:57:18 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A40                     Off | 00000000:AF:00.0 Off |                    0 |\n",
      "|  0%   31C    P0              69W / 300W |   7490MiB / 46068MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A    247554      C   /data/python/laby/llama.cpp/server         6700MiB |\n",
      "|    0   N/A  N/A    247563      C   ...ython/laby/text2vec/env/bin/python3      768MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5796da1f-29c6-4950-892b-1ffb97b83013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-12 14:57:22.068530: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-12 14:57:22.933477: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, StoppingCriteria, StoppingCriteriaList\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d800881f-ac05-4e7c-96d5-63715ee94939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/GanymedeNil_text2vec-large-chinese. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"/root/.cache/torch/sentence_transformers/GanymedeNil_text2vec-large-chinese\",\n",
    "                                                      # local_files_only=True,\n",
    "                                                      model_kwargs={\"device\": \"cuda\"})\n",
    "persist_directory = 'db_cn'\n",
    "\n",
    "vectordb = Chroma(persist_directory=persist_directory,\n",
    "                  embedding_function=instructor_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "901fbbe6-255d-45f8-a597-9b2179b53722",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b79efda-b6fa-4f36-97c0-ec954921e577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "instructor_embeddings_en = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\",\n",
    "                                                      model_kwargs={\"device\": \"cuda\"})\n",
    "\n",
    "persist_directory_en = 'db'\n",
    "\n",
    "vectordb_en = Chroma(persist_directory=persist_directory_en,\n",
    "                  embedding_function=instructor_embeddings_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a09c37f-a3db-4430-9697-e7614fa353bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_en = vectordb_en.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75365f9b-d390-4215-969c-e7153f8738af",
   "metadata": {},
   "source": [
    "## ENG LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3623a017-604a-4550-bb8e-0d6f1f6d580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENG\n",
    "model_name_en = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer_en = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name_en,\n",
    "                                             local_files_only=True,\n",
    "                                             # use_fast=False\n",
    "                                          # use_auth_token=True,\n",
    "                                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f693cfb9-f78a-49f9-8c81-97059c436265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8f7069bcbd4da2a7fefd5f394f010a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_en = AutoModelForCausalLM.from_pretrained(model_name_en,\n",
    "                                                local_files_only=True,\n",
    "                                                device_map='auto',\n",
    "                                                torch_dtype=torch.float16,\n",
    "                                                temperature=0.2, # must be strictly positive float\n",
    "                                                do_sample=True,\n",
    "                                                # use_auth_token=True,\n",
    "                                                #  load_in_8bit=True,\n",
    "                                                #  load_in_4bit=True\n",
    "                                               )\n",
    "pipe_en = pipeline(\"text-generation\",\n",
    "            model=model_en,\n",
    "            tokenizer= tokenizer_en,\n",
    "            # return_full_text=True,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            max_new_tokens = 512,\n",
    "            do_sample=True,\n",
    "            top_k=30,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer_en.eos_token_id\n",
    "            )\n",
    "llm_en = HuggingFacePipeline(pipeline=pipe_en)\n",
    "\n",
    "template_en = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "\n",
    "prompt_template_en = PromptTemplate.from_template(\n",
    "    template_en\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e36c92a-db1a-48e2-9de5-67e54ae24564",
   "metadata": {},
   "source": [
    "## 中文 LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c1fdd86-603c-4972-b70e-ec75543d210e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8248c2e98ee1498595e7df28284f668e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_name = \"FlagAlpha/Llama2-Chinese-7b-Chat\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name,\n",
    "                                          local_files_only=True,\n",
    "                                          use_fast=False\n",
    "                                          # use_auth_token=True,\n",
    "                                         )\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                         local_files_only=True,\n",
    "                                         device_map='auto',\n",
    "                                         torch_dtype=torch.float16,\n",
    "                                         temperature=0.2, # must be strictly positive float\n",
    "                                         do_sample=True,\n",
    "                                         # use_auth_token=True,\n",
    "                                        #  load_in_8bit=True,\n",
    "                                        #  load_in_4bit=True\n",
    "                                         )\n",
    "pipe = pipeline(\"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer= tokenizer,\n",
    "            # return_full_text=True,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            max_new_tokens = 512,\n",
    "            do_sample=True,\n",
    "            top_k=30,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3165162c-ef0c-4791-89af-f263b079b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"使用以下上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答案。\n",
    "\n",
    "  {context}\n",
    "\n",
    "  问题: {question}\n",
    "  答案:\"\"\"\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "chain_type_kwargs = {\n",
    "    \"prompt\": prompt_template\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7adeea33-245a-489a-adfd-80e92202964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain\n",
    "qa = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 retriever=retriever,\n",
    "                                 return_source_documents=True,\n",
    "                                 chain_type_kwargs=chain_type_kwargs\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "033b08b3-a762-4362-8502-240831a7061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \"如何设定判定逻辑?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26ea1b5e-1dad-4edb-8c59-0bbb2031c351",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/generation/utils.py:1411: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.44 s, sys: 354 ms, total: 3.8 s\n",
      "Wall time: 3.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res1 = qa({\"query\": question1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7993a829-8dc7-4e76-8559-530a83e8c49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': '如何设定判定逻辑?',\n",
       " 'result': ' 设定PPID/SN的逻辑规则，用于解析保修起点日期等信息。可以在厂家保修管理后台->基础设定->判定逻辑中添加/编辑判定逻辑。\\n',\n",
       " 'source_documents': [Document(page_content='选择是否设定附加规则 , 限制 SN长度或厂家代码范围等 , 设定完毕后 , 点击【保存】 , \\n完成该产品序列号判定逻辑设定 .', metadata={'source': 'chinese_pdf/保修管理-廠家sop-repair(去圖標).pdf', 'page': 7}),\n",
       "  Document(page_content='2.2 检验逻辑设定   \\n逻辑判定设置完成后 , 厂家保修管理 后台->基础设定 ->判定逻辑 , 点击进入逻辑判定\\n页面, 最下方输入 PPID, 点击【解析】检验逻辑设定 ;如下图, 可依规则自动解析出厂\\n时间等信息 . \\n \\n3. 保修类别  \\n厂家保修管理 后台->基础设定 ->保修类别，点击【新增设定】 , 设定需要为产品配置\\n的保修类别 ,填写服务类型、时效值及单位 , 点击【保存】完成 ; \\n \\n \\n4. 保修约定  \\n当产品保修及保修卡无法满足保修需求时 , 可添加保修约定 .', metadata={'source': 'chinese_pdf/保修管理-廠家sop-repair(去圖標).pdf', 'page': 8}),\n",
       "  Document(page_content='基础设定  \\n1. 判定命名  \\n厂家保修管理 后台->基础设定 ->判定命名 ,依用户习惯 , 自主设定相关栏位名称 , 设定\\n后, 系统相关栏位均以设定名称显示 . \\n \\n2. 判定逻辑  \\n设定PPID/SN的逻辑规则 , 用于解析保修起点日期等信息 . \\n2.1 添加/编辑判定逻辑  \\n厂家保修管理 后台->基础设定 ->判定逻辑 , 点击【添加】按钮 , 添加判定逻辑 ;选定判\\n定逻辑, 点击对该判定逻辑进行编辑 ; \\n \\n填写解码规则名称、说明、 SN样例；点击【编辑】按钮 进行应用 ; \\n \\n填写 PPID/SN子串区段 , 依照子串区段 的代表意义 , 可自定义规则  \\n如下图中点击【月份 规则自定义】 ，  进入自定义模块后 , 点击【新增码值】 , 可自定义\\n月份解析规则 , 例如 , 可依 PPID/SN规则设定 A代表 10月, B代表 11月份 , C代表 12月\\n份; \\n【添加一项自定义规则】可添加 PPID/SN子串逻辑自定义规则 .', metadata={'source': 'chinese_pdf/保修管理-廠家sop-repair(去圖標).pdf', 'page': 6})]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "468becf0-4c0e-45cf-b13a-fb7abb5d8d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_en = RetrievalQA.from_chain_type(llm=llm_en,\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 retriever=retriever_en,\n",
    "                                 return_source_documents=True,\n",
    "                                 # chain_type_kwargs=chain_type_kwargs\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1794b9af-3682-439d-a7dc-d83eea0b7ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \"How can I navigate to the RMA request page in the Wareconn Customer Portal?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13cd0b4b-23e3-417a-8538-aa2887cfafc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.46 s, sys: 685 ms, total: 6.14 s\n",
      "Wall time: 6.15 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'How can I navigate to the RMA request page in the Wareconn Customer Portal?',\n",
       " 'result': ' You can navigate to the RMA request page in the Wareconn Customer Portal by following the steps outlined in the document. The steps are as follows:\\n\\n1. Log in to your account on the Wareconn Customer Portal by entering your email and password.\\n2. Click on the \"RMA Request\" button in the left menu.\\n3. Click on the \"Add\" button to create a new RMA request.\\n4. Fill in the required information, including the part number (PN) and serial number (SN), and click \"Submit\".\\nAlternatively, you can also search for the RMA request page by using the search function on the Wareconn Customer Portal.',\n",
       " 'source_documents': [Document(page_content='2 / 6 \\nThis document  belongs to Wareconn  Technology Services (Tianjin) Co., Ltd. It is only intended to be \\nused to introduce Wareconn functions and procedures. Please do not use it for other purposes.  2 wareconn standard operating procedure  \\nSOP  \\nI. This guide will walk you through the process of requesting  RMA  through the \\nWareconn Customer Portal.  \\nII. Details on how to fill in the Part Number (PN) and Serial Number (SN)  \\n1.1 RMA request  procedure (Parts return)  \\n1. System login  \\n⚫ Module: https://www.wareconn.com/  \\n⚫ Description:  Login  with account and password  \\n \\nTo log in, follow these steps:  \\nGo to https://www.wareconn.com/ and click Log in  \\n \\nFill in your email and password, then click Login\\n \\nClick Enter  in the Customer portal', metadata={'source': 'MSFT RMA request SOP v1.0.pdf', 'page': 1}),\n",
       "  Document(page_content='3 / 6 \\nThis document  belongs to Wareconn  Technology Services (Tianjin) Co., Ltd. It is only intended to be \\nused to introduce Wareconn functions and procedures. Please do not use it for other purposes.  3 wareconn standard operating procedure  \\n2. Go to  RMA Request  Page \\n⚫ Module: Customer portal -Warranty Claims -Warranty Claims  \\n⚫ Description: Go to RMA request page  \\n \\nTo go to the RMA request page, follow these steps:  \\nClick Warranty Claims  in the left menu, then click +Add  \\n \\nClick the icon shown in the image below  \\n(Select the corresponding SI )', metadata={'source': 'MSFT RMA request SOP v1.0.pdf', 'page': 2}),\n",
       "  Document(page_content='4 / 6 \\nThis document  belongs to Wareconn  Technology Services (Tianjin) Co., Ltd. It is only intended to be \\nused to introduce Wareconn functions and procedures. Please do not use it for other purposes.  4 wareconn standard operating procedure  \\n3. Create RMA Request  \\n⚫ Module: Customer portal -Warranty Claims -Warranty Claims -Create request  \\n⚫ Description: Create RMA request order  \\n \\nTo create a RMA request, follow these steps:  \\nClick Add Warranty Claims   \\n \\nThen, click Parts Return', metadata={'source': 'MSFT RMA request SOP v1.0.pdf', 'page': 3})]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "qa_en({\"query\": question1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a56067c-20ed-4855-91a3-a485ac28beeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep 12 14:30:30 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A40                     Off | 00000000:AF:00.0 Off |                    0 |\n",
      "|  0%   30C    P0              69W / 300W |  33683MiB / 46068MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     20878      C   /usr/local/bin/python3.10                 26194MiB |\n",
      "|    0   N/A  N/A    247554      C   /data/python/laby/llama.cpp/server         6700MiB |\n",
      "|    0   N/A  N/A    247563      C   ...ython/laby/text2vec/env/bin/python3      762MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570aec27-2c47-4727-a951-bbac34b93337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0154ee3d-5c67-4fe2-981d-70c168cf29b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List\n",
    "app = FastAPI()\n",
    "class Prompt(BaseModel):\n",
    "    # context: str\n",
    "    question: str\n",
    "    lang: str\n",
    "    model: str = Field(default=\"FlagAlpha/Llama2-Chinese-7b-Chat\")\n",
    "    temperature: float = Field(default=0.2)\n",
    "\n",
    "@app.post(\"/question_answering\")\n",
    "async def get_completion(input: Prompt):\n",
    "    input_dict = input.dict()\n",
    "\n",
    "    if input_dict[\"lang\"] == \"en_us\":\n",
    "        \n",
    "        ans = qa_en({\"query\": input_dict[\"question\"]})\n",
    "    else:\n",
    "      \n",
    "        ans = qa({\"query\": input_dict[\"question\"]})\n",
    "    return ans\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\n",
    "        \"message\": \"this is get\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1205536a-8e19-46ee-aff5-12c23114838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Documents(BaseModel):\n",
    "    input: List[str]\n",
    "    lang: str\n",
    "@app.post(\"/embeddings\")\n",
    "async def embed_docs(input: Documents):\n",
    "    input_dict = input.dict()\n",
    "    docs = input_dict[\"input\"]\n",
    "    \n",
    "    if input_dict[\"lang\"] == \"en_us\":\n",
    "        embed_docs = instructor_embeddings_en.embed_documents(docs)\n",
    "    else:\n",
    "        embed_docs = instructor_embeddings.embed_documents(docs)\n",
    "\n",
    "    data = [{\"embedding\": emb, \"index\": idx} for idx, emb in enumerate(embed_docs)]\n",
    "    return {\n",
    "        \"data\": data,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3a1dc10-b8a6-4c92-82e1-258a0b69c142",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question(BaseModel):\n",
    "    user_input:str\n",
    "    lang: str\n",
    "    k: Optional[int] = Field(default=3)\n",
    "\n",
    "\n",
    "@app.post(\"/get_relevant_docs\")\n",
    "async def retrieve_docs(input_q: Question):\n",
    "    input_dict = input_q.dict()\n",
    "\n",
    "    if input_dict[\"lang\"] == \"en_us\":\n",
    "        ans = retriever_en.get_relevant_documents(input_dict[\"user_input\"])\n",
    "    else:\n",
    "        ans = retriever.get_relevant_documents(input_dict[\"user_input\"])\n",
    "\n",
    "    docs = list(map(lambda a: {\n",
    "            'page_content': a.page_content,\n",
    "            'source': a.metadata[\"source\"],\n",
    "            'page': a.metadata[\"page\"]}, ans))\n",
    "    return {\n",
    "        # \"user_input\": question[\"user_input\"],\n",
    "        \"relavant_docs\": docs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e45e43d-a59f-429e-b32f-304d48c69ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc18f9f-0969-464c-b63d-338e3142a41f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "974cca8f-e323-48ec-bfcc-28d91e869c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "# Allow for asyncio to work within the Jupyter notebook cell\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4136a0f-7ed0-4364-8cf9-fe81187c495b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [31655]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8006 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:5169 - \"POST /get_relevant_docs HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:5171 - \"POST /get_relevant_docs HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:5183 - \"POST /embeddings HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:5185 - \"POST /embeddings HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:5187 - \"POST /embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error uploading: HTTPSConnectionPool(host='app.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fe079d3ea70>: Failed to establish a new connection: [Errno -2] Name or service not known'))\n",
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [31655]\n"
     ]
    }
   ],
   "source": [
    "import uvicorn\n",
    "uvicorn.run(app=app, host=\"127.0.0.1\", port=8006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06b47264-3f00-4896-a1ef-c9840753cedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: fastapi\n",
      "Version: 0.100.0\n",
      "Summary: FastAPI framework, high performance, easy to learn, fast to code, ready for production\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Sebastián Ramírez <tiangolo@gmail.com>\n",
      "License: \n",
      "Location: /usr/local/lib/python3.10/site-packages\n",
      "Requires: pydantic, starlette, typing-extensions\n",
      "Required-by: chromadb\n"
     ]
    }
   ],
   "source": [
    "!pip show fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f5d918-c25b-4116-a3a0-9c525065ccde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
