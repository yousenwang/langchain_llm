{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yousenwang/langchain_llm/blob/main/convert_and_quantize_chinese_llama2_and_alpaca2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# è½¬æ¢å¹¶é‡åŒ–ä¸­æ–‡LLaMA-2å’ŒAlpaca-2æ¨¡å‹\n",
        "\n",
        "é¡¹ç›®åœ°å€ï¼šhttps://github.com/ymcui/Chinese-LLaMA-Alpaca-2\n",
        "\n",
        "âš ï¸ å†…å­˜æ¶ˆè€—æç¤ºï¼ˆç¡®ä¿åˆ·å‡ºæ¥çš„æœºå™¨RAMå¤§äºä»¥ä¸‹è¦æ±‚ï¼‰ï¼š\n",
        "- 7Bæ¨¡å‹ï¼š15G+\n",
        "- 13Bæ¨¡å‹ï¼š18G+\n",
        "- 33Bæ¨¡å‹ï¼š22G+\n",
        "\n",
        "ğŸ’¡ æç¤ºå’Œå°çªé—¨ï¼š\n",
        "- å…è´¹ç”¨æˆ·é»˜è®¤çš„å†…å­˜åªæœ‰12Gå·¦å³ï¼Œä¸è¶³ä»¥è½¬æ¢æ¨¡å‹ã€‚**å®æµ‹é€‰æ‹©TPUçš„è¯æœ‰æœºä¼šéšæœºå‡º35Gå†…å­˜**ï¼Œå»ºè®®å¤šè¯•å‡ æ¬¡\n",
        "- Pro(+)ç”¨æˆ·è¯·é€‰æ‹© â€œä»£ç æ‰§è¡Œç¨‹åºâ€ -> â€œæ›´æ”¹è¿è¡Œæ—¶ç±»å‹â€ -> â€œé«˜RAMâ€\n",
        "- ç¨‹åºè«åå´©æ‰æˆ–æ–­å¼€è¿æ¥å°±è¯´æ˜å†…å­˜çˆ†äº†\n",
        "- å¦‚æœé€‰äº†â€œé«˜RAMâ€ä¹‹åå†…å­˜è¿˜æ˜¯ä¸å¤Ÿå¤§çš„è¯ï¼Œé€‰æ‹©ä»¥ä¸‹æ“ä½œï¼Œæœ‰çš„æ—¶å€™ä¼šåˆ†é…å‡ºå¾ˆé«˜å†…å­˜çš„æœºå™¨ï¼Œç¥ä½ å¥½è¿ğŸ˜„ï¼\n",
        "    - å¯ä»¥æŠŠGPUæˆ–è€…TPUä¹Ÿé€‰ä¸Šï¼ˆè™½ç„¶ä¸ä¼šç”¨åˆ°ï¼‰\n",
        "    - é€‰GPUæ—¶ï¼ŒPro(+)ç”¨æˆ·å¯é€‰â€œA100â€ç±»å‹GPU\n",
        "\n",
        "*æ¸©é¦¨æç¤ºï¼šç”¨å®Œä¹‹åæ³¨æ„æ–­å¼€è¿è¡Œæ—¶ï¼Œé€‰æ‹©æ»¡è¶³è¦æ±‚çš„æœ€ä½é…ç½®å³å¯ï¼Œé¿å…ä¸å¿…è¦çš„è®¡ç®—å•å…ƒæ¶ˆè€—ï¼ˆProåªç»™100ä¸ªè®¡ç®—å•å…ƒï¼‰ã€‚*"
      ],
      "metadata": {
        "id": "B1c96_k3MahN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## å®‰è£…ç›¸å…³ä¾èµ–"
      ],
      "metadata": {
        "id": "vScqHD_jMFOV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5WKFJXIL6ZU",
        "outputId": "4b667116-83dd-4f5b-8b48-a86ae7fc249f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/peft.git@13e53fc\n",
            "  Cloning https://github.com/huggingface/peft.git (to revision 13e53fc) to /tmp/pip-req-build-rth5yg2a\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-rth5yg2a\n",
            "\u001b[33m  WARNING: Did not find branch or tag '13e53fc', assuming revision or ref.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Running command git checkout -q 13e53fc\n",
            "  Resolved https://github.com/huggingface/peft.git to commit 13e53fc\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.3.0.dev0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.3.0.dev0) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.3.0.dev0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.3.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.3.0.dev0) (2.0.1+cu118)\n",
            "Collecting transformers (from peft==0.3.0.dev0)\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate (from peft==0.3.0.dev0)\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0.dev0) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0.dev0) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0.dev0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0.dev0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0.dev0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0.dev0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft==0.3.0.dev0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft==0.3.0.dev0) (16.0.6)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers->peft==0.3.0.dev0)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.3.0.dev0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.3.0.dev0) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->peft==0.3.0.dev0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers->peft==0.3.0.dev0)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.3.0.dev0) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft==0.3.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.3.0.dev0) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.3.0.dev0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.3.0.dev0) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.3.0.dev0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.3.0.dev0) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft==0.3.0.dev0) (1.3.0)\n",
            "Building wheels for collected packages: peft\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for peft: filename=peft-0.3.0.dev0-py3-none-any.whl size=40651 sha256=a877759f5558de91ed69f0a01aae40d8c3d3098855b2bea7b2e35369dcaf4c8f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-d1fi3rot/wheels/d9/13/c6/404d5f8a81c5620f65f7fd75b6a66619f013cd79c2875b981c\n",
            "Successfully built peft\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers, accelerate, peft\n",
            "Successfully installed accelerate-0.21.0 huggingface-hub-0.16.4 peft-0.3.0.dev0 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n",
            "Requirement already satisfied: transformers==4.31.0 in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.4)\n",
            "Collecting sentencepiece==0.1.97\n",
            "  Downloading sentencepiece-0.1.97-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Collecting bitsandbytes==0.39.1\n",
            "  Downloading bitsandbytes-0.39.1-py3-none-any.whl (97.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m97.1/97.1 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.39.1\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/huggingface/peft.git@13e53fc\n",
        "!pip install transformers==4.31.0\n",
        "!pip install sentencepiece==0.1.97\n",
        "!pip install bitsandbytes==0.39.1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## å…‹éš†ç›®å½•å’Œä»£ç "
      ],
      "metadata": {
        "id": "ygb1xFIMNQKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ymcui/Chinese-LLaMA-Alpaca\n",
        "!git clone https://github.com/ggerganov/llama.cpp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCEJh7NJNXz9",
        "outputId": "53ab994f-6506-4440-b97f-2111d7ddf32b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Chinese-LLaMA-Alpaca'...\n",
            "remote: Enumerating objects: 2127, done.\u001b[K\n",
            "remote: Counting objects: 100% (816/816), done.\u001b[K\n",
            "remote: Compressing objects: 100% (323/323), done.\u001b[K\n",
            "remote: Total 2127 (delta 553), reused 650 (delta 493), pack-reused 1311\u001b[K\n",
            "Receiving objects: 100% (2127/2127), 22.03 MiB | 14.36 MiB/s, done.\n",
            "Resolving deltas: 100% (1330/1330), done.\n",
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 5703, done.\u001b[K\n",
            "remote: Counting objects: 100% (2361/2361), done.\u001b[K\n",
            "remote: Compressing objects: 100% (338/338), done.\u001b[K\n",
            "remote: Total 5703 (delta 2235), reused 2051 (delta 2021), pack-reused 3342\u001b[K\n",
            "Receiving objects: 100% (5703/5703), 4.43 MiB | 20.99 MiB/s, done.\n",
            "Resolving deltas: 100% (3938/3938), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## åˆå¹¶æ¨¡å‹ï¼ˆä»¥LLaMA-2-7Bä¸ºä¾‹ï¼‰\n",
        "\n",
        "åˆå¹¶LoRAï¼Œç”Ÿæˆå…¨é‡æ¨¡å‹æƒé‡ã€‚å¯ä»¥ç›´æ¥æŒ‡å®šğŸ¤—æ¨¡å‹åº“çš„åœ°å€ï¼Œä¹Ÿå¯ä»¥æ˜¯æœ¬åœ°å­˜æ”¾åœ°å€ã€‚\n",
        "- åŸºæ¨¡å‹ï¼š`meta-llama/Llama-2-7b-hf`ï¼ˆæ³¨æ„éœ€è¦å®˜æ–¹æˆæƒï¼‰\n",
        "    - è¿™é‡Œä½¿ç”¨ä¸€ä¸ªå¹³æ›¿ï¼ˆSHA256ä¸€è‡´ï¼‰åšæ¼”ç¤ºï¼š`daryl149/llama-2-7b-hf`\n",
        "- LoRAæ¨¡å‹ï¼š`ziqingyang/chinese-llama-2-lora-7b`\n",
        "- è¾“å‡ºæ ¼å¼ï¼šå¯é€‰pthæˆ–è€…huggingfaceï¼Œè¿™é‡Œé€‰æ‹©huggingface\n",
        "\n",
        "è½¬æ¢å¥½çš„æ¨¡å‹å­˜æ”¾åœ¨`llama-2-7b-combined`ç›®å½•ã€‚\n",
        "å¦‚æœä½ ä¸éœ€è¦é‡åŒ–æ¨¡å‹ï¼Œé‚£ä¹ˆåˆ°è¿™ä¸€æ­¥å°±ç»“æŸäº†ï¼Œå¯è‡ªè¡Œä¸‹è½½æˆ–è€…è½¬å­˜åˆ°Google Driveã€‚"
      ],
      "metadata": {
        "id": "nIyxX0DSNsgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python ./Chinese-LLaMA-Alpaca-2/scripts/merge_llama2_with_chinese_lora_low_mem.py \\\n",
        "    --base_model daryl149/llama-2-7b-hf \\\n",
        "    --lora_model ziqingyang/chinese-llama-2-lora-7b \\\n",
        "    --output_type huggingface \\\n",
        "    --output_dir llama-2-7b-combined"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AV4EW5hNhVV",
        "outputId": "c9d5ca7a-aa61-49fc-b8bd-ce9d42ef3074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
            "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cpu.so...\n",
            "2023-07-27 07:44:07.692012: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "================================================================================\n",
            "Base model: daryl149/llama-2-7b-hf\n",
            "LoRA model: llama2-7b-lora\n",
            "Loading llama2-7b-lora\n",
            "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
            "Cannot find lora model on the disk. Downloading lora model from hub...\n",
            "Fetching 11 files:   0% 0/11 [00:00<?, ?it/s]\n",
            "Downloading (â€¦)model.bin.index.json: 100% 26.8k/26.8k [00:00<00:00, 34.4MB/s]\n",
            "\n",
            "Downloading (â€¦)307970dd/config.json: 100% 578/578 [00:00<00:00, 2.69MB/s]\n",
            "\n",
            "Downloading (â€¦)neration_config.json: 100% 132/132 [00:00<00:00, 929kB/s]\n",
            "\n",
            "Downloading (â€¦)cial_tokens_map.json: 100% 411/411 [00:00<00:00, 1.96MB/s]\n",
            "\n",
            "Downloading (â€¦)970dd/.gitattributes: 100% 1.52k/1.52k [00:00<00:00, 6.35MB/s]\n",
            "Fetching 11 files:   9% 1/11 [00:01<00:10,  1.06s/it]\n",
            "Downloading (â€¦)970dd/tokenizer.json:   0% 0.00/1.84M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)ed307970dd/README.md: 100% 7.23k/7.23k [00:00<00:00, 25.6MB/s]\n",
            "\n",
            "\n",
            "Downloading (â€¦)okenizer_config.json: 100% 745/745 [00:00<00:00, 4.74MB/s]\n",
            "\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   0% 0.00/9.98G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)970dd/tokenizer.json: 100% 1.84M/1.84M [00:00<00:00, 17.3MB/s]\n",
            "\n",
            "Downloading (â€¦)l-00002-of-00002.bin:   0% 0.00/3.50G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   0% 10.5M/9.98G [00:00<01:36, 104MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading tokenizer.model: 100% 500k/500k [00:00<00:00, 11.1MB/s]\n",
            "\n",
            "Downloading (â€¦)l-00002-of-00002.bin:   0% 10.5M/3.50G [00:00<00:34, 101MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   0% 31.5M/9.98G [00:00<01:01, 162MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:   1% 31.5M/3.50G [00:00<00:21, 162MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   1% 52.4M/9.98G [00:00<00:54, 182MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   1% 73.4M/9.98G [00:00<00:52, 190MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:   2% 62.9M/3.50G [00:00<00:18, 187MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   1% 94.4M/9.98G [00:00<00:51, 193MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:   2% 83.9M/3.50G [00:00<00:17, 194MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   1% 115M/9.98G [00:00<00:50, 194MB/s] \u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:   3% 105M/3.50G [00:00<00:17, 195MB/s] \u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   1% 136M/9.98G [00:00<00:50, 194MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:   4% 126M/3.50G [00:00<00:17, 194MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   2% 157M/9.98G [00:00<00:51, 192MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:   4% 147M/3.50G [00:00<00:17, 190MB/s]\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:   5% 168M/3.50G [00:00<00:17, 192MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   2% 178M/9.98G [00:00<00:52, 187MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:   5% 189M/3.50G [00:01<00:17, 189MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   2% 199M/9.98G [00:01<00:51, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   2% 220M/9.98G [00:01<00:51, 191MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:   6% 210M/3.50G [00:01<00:17, 188MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   2% 241M/9.98G [00:01<00:50, 193MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:   7% 231M/3.50G [00:01<00:17, 189MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   3% 262M/9.98G [00:01<00:50, 194MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:   7% 252M/3.50G [00:01<00:17, 190MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   3% 283M/9.98G [00:01<00:49, 196MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:   8% 273M/3.50G [00:01<00:16, 190MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   3% 304M/9.98G [00:01<00:49, 197MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:   8% 294M/3.50G [00:01<00:16, 194MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   3% 325M/9.98G [00:01<00:49, 195MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:   9% 315M/3.50G [00:01<00:16, 195MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   3% 346M/9.98G [00:01<00:49, 195MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  10% 336M/3.50G [00:01<00:16, 195MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   4% 367M/9.98G [00:01<00:49, 194MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  10% 357M/3.50G [00:01<00:16, 196MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   4% 388M/9.98G [00:02<00:49, 194MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  11% 377M/3.50G [00:01<00:16, 192MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   4% 409M/9.98G [00:02<00:49, 195MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  11% 398M/3.50G [00:02<00:16, 191MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   4% 430M/9.98G [00:02<00:52, 182MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  12% 419M/3.50G [00:02<00:17, 171MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   5% 451M/9.98G [00:02<00:58, 162MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  13% 440M/3.50G [00:02<00:19, 156MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   5% 472M/9.98G [00:02<01:02, 151MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  13% 461M/3.50G [00:02<00:20, 150MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   5% 493M/9.98G [00:02<01:10, 134MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  14% 482M/3.50G [00:02<00:22, 136MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   5% 514M/9.98G [00:02<01:09, 137MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  14% 503M/3.50G [00:02<00:22, 135MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   5% 535M/9.98G [00:03<01:11, 132MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  15% 524M/3.50G [00:03<00:22, 132MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   6% 556M/9.98G [00:03<01:09, 135MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  16% 545M/3.50G [00:03<00:22, 132MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   6% 577M/9.98G [00:03<01:13, 128MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  16% 566M/3.50G [00:03<00:23, 127MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   6% 598M/9.98G [00:03<01:13, 127MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  17% 587M/3.50G [00:03<00:22, 128MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   6% 619M/9.98G [00:03<01:22, 114MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  17% 608M/3.50G [00:03<00:25, 111MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   6% 640M/9.98G [00:04<01:26, 107MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  18% 629M/3.50G [00:04<00:26, 109MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   7% 661M/9.98G [00:04<01:24, 110MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  19% 650M/3.50G [00:04<00:26, 108MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   7% 682M/9.98G [00:04<01:23, 111MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  19% 671M/3.50G [00:04<00:26, 108MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   7% 703M/9.98G [00:04<01:19, 116MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  20% 692M/3.50G [00:04<00:24, 116MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   7% 724M/9.98G [00:04<01:18, 118MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  20% 713M/3.50G [00:04<00:24, 115MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   7% 744M/9.98G [00:04<01:18, 117MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  21% 734M/3.50G [00:04<00:23, 117MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   8% 765M/9.98G [00:05<01:18, 117MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  22% 755M/3.50G [00:05<00:22, 120MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   8% 786M/9.98G [00:05<01:19, 116MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  22% 776M/3.50G [00:05<00:23, 116MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   8% 807M/9.98G [00:05<01:22, 112MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  23% 797M/3.50G [00:05<00:24, 112MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   8% 828M/9.98G [00:05<01:26, 106MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  23% 818M/3.50G [00:05<00:26, 103MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   9% 849M/9.98G [00:07<05:25, 28.1MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  24% 839M/3.50G [00:08<02:05, 21.2MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   9% 860M/9.98G [00:08<06:43, 22.6MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  25% 860M/3.50G [00:08<01:33, 28.3MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   9% 881M/9.98G [00:08<04:57, 30.6MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  25% 881M/3.50G [00:08<01:11, 36.4MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   9% 902M/9.98G [00:09<03:50, 39.3MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  26% 902M/3.50G [00:09<00:55, 46.7MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   9% 912M/9.98G [00:09<03:25, 44.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   9% 923M/9.98G [00:09<03:01, 49.9MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  26% 923M/3.50G [00:09<00:45, 56.1MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:   9% 933M/9.98G [00:09<02:39, 56.5MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  27% 944M/3.50G [00:09<00:39, 64.9MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  10% 954M/9.98G [00:09<02:07, 71.0MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  28% 965M/3.50G [00:09<00:33, 74.8MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  10% 975M/9.98G [00:09<01:50, 81.4MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  28% 986M/3.50G [00:09<00:30, 82.7MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  10% 996M/9.98G [00:09<01:39, 90.1MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  29% 1.01G/3.50G [00:10<00:28, 87.5MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  10% 1.02G/9.98G [00:10<01:37, 91.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  10% 1.03G/9.98G [00:10<01:36, 92.5MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  29% 1.03G/3.50G [00:10<00:28, 86.2MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  10% 1.04G/9.98G [00:10<01:44, 85.6MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  30% 1.04G/3.50G [00:10<00:29, 83.4MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  11% 1.05G/9.98G [00:10<01:49, 81.2MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  30% 1.05G/3.50G [00:10<00:30, 80.3MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  11% 1.06G/9.98G [00:10<01:48, 82.0MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  31% 1.07G/3.50G [00:10<00:27, 89.8MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  11% 1.08G/9.98G [00:10<01:33, 95.4MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  31% 1.09G/3.50G [00:10<00:23, 101MB/s] \u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  11% 1.10G/9.98G [00:11<03:04, 48.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  11% 1.11G/9.98G [00:13<08:15, 17.9MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  32% 1.11G/3.50G [00:13<01:55, 20.6MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  11% 1.13G/9.98G [00:13<05:43, 25.8MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  32% 1.13G/3.50G [00:13<01:25, 27.8MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  11% 1.14G/9.98G [00:13<04:48, 30.6MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  33% 1.14G/3.50G [00:13<01:13, 32.1MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  12% 1.15G/9.98G [00:14<04:03, 36.2MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  33% 1.15G/3.50G [00:14<01:03, 37.0MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  12% 1.16G/9.98G [00:14<03:27, 42.5MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  33% 1.16G/3.50G [00:14<00:54, 42.8MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  12% 1.17G/9.98G [00:14<03:00, 48.8MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  34% 1.17G/3.50G [00:14<00:48, 48.3MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  12% 1.18G/9.98G [00:14<02:42, 54.1MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  34% 1.18G/3.50G [00:14<00:43, 53.5MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  12% 1.20G/9.98G [00:14<02:27, 59.7MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  34% 1.20G/3.50G [00:14<00:39, 58.7MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  12% 1.21G/9.98G [00:14<02:21, 62.1MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  34% 1.21G/3.50G [00:14<00:37, 61.2MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  12% 1.22G/9.98G [00:14<02:16, 64.2MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  35% 1.22G/3.50G [00:14<00:35, 64.9MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  12% 1.23G/9.98G [00:15<02:07, 68.4MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  35% 1.23G/3.50G [00:14<00:34, 66.7MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  12% 1.24G/9.98G [00:15<02:03, 71.0MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  35% 1.24G/3.50G [00:15<00:31, 71.5MB/s]\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  36% 1.25G/3.50G [00:15<00:28, 78.6MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  13% 1.26G/9.98G [00:15<01:41, 85.5MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  36% 1.26G/3.50G [00:15<00:26, 83.8MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  13% 1.28G/9.98G [00:15<01:24, 103MB/s] \u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  37% 1.28G/3.50G [00:15<00:21, 103MB/s] \u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  13% 1.30G/9.98G [00:15<01:22, 105MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  37% 1.30G/3.50G [00:15<00:20, 106MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  13% 1.32G/9.98G [00:15<01:17, 112MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  38% 1.32G/3.50G [00:15<00:19, 113MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  13% 1.34G/9.98G [00:15<01:10, 122MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  38% 1.34G/3.50G [00:15<00:17, 126MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  14% 1.36G/9.98G [00:16<01:03, 136MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  39% 1.36G/3.50G [00:16<00:15, 142MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  14% 1.38G/9.98G [00:16<00:57, 150MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  40% 1.38G/3.50G [00:16<00:13, 155MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  14% 1.41G/9.98G [00:16<00:53, 159MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  40% 1.41G/3.50G [00:16<00:12, 165MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  14% 1.43G/9.98G [00:16<00:50, 168MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  41% 1.43G/3.50G [00:16<00:12, 169MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  15% 1.45G/9.98G [00:16<00:49, 173MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  41% 1.45G/3.50G [00:16<00:11, 174MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  15% 1.47G/9.98G [00:16<00:48, 175MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  42% 1.47G/3.50G [00:16<00:11, 176MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  15% 1.49G/9.98G [00:16<00:47, 179MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  43% 1.49G/3.50G [00:16<00:11, 182MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  15% 1.51G/9.98G [00:16<00:46, 181MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  43% 1.51G/3.50G [00:16<00:10, 185MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  15% 1.53G/9.98G [00:17<00:49, 172MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  44% 1.53G/3.50G [00:16<00:11, 176MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  16% 1.55G/9.98G [00:17<00:52, 160MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  44% 1.55G/3.50G [00:17<00:12, 156MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  16% 1.57G/9.98G [00:17<00:54, 155MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  45% 1.57G/3.50G [00:17<00:13, 148MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  16% 1.59G/9.98G [00:17<01:01, 137MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  46% 1.59G/3.50G [00:17<00:13, 138MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  16% 1.61G/9.98G [00:17<01:00, 138MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  46% 1.61G/3.50G [00:17<00:14, 135MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  16% 1.64G/9.98G [00:17<01:02, 134MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  47% 1.64G/3.50G [00:17<00:14, 128MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  17% 1.66G/9.98G [00:17<01:03, 131MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  47% 1.66G/3.50G [00:17<00:14, 129MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  17% 1.68G/9.98G [00:18<01:03, 131MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  48% 1.68G/3.50G [00:18<00:13, 131MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  17% 1.70G/9.98G [00:18<01:20, 103MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  49% 1.70G/3.50G [00:18<00:17, 103MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  17% 1.72G/9.98G [00:18<01:21, 101MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  49% 1.72G/3.50G [00:18<00:16, 105MB/s]\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  50% 1.74G/3.50G [00:18<00:14, 123MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  18% 1.75G/9.98G [00:18<01:05, 126MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  50% 1.76G/3.50G [00:18<00:13, 128MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  18% 1.77G/9.98G [00:19<01:08, 121MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  51% 1.78G/3.50G [00:19<00:15, 110MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  18% 1.79G/9.98G [00:19<01:16, 106MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  52% 1.80G/3.50G [00:19<00:17, 99.4MB/s]\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  52% 1.82G/3.50G [00:19<00:18, 91.6MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  18% 1.81G/9.98G [00:19<01:46, 76.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  18% 1.84G/9.98G [00:19<01:26, 93.9MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  53% 1.85G/3.50G [00:19<00:18, 91.5MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  19% 1.86G/9.98G [00:20<01:21, 99.3MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  53% 1.86G/3.50G [00:19<00:17, 92.4MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  19% 1.88G/9.98G [00:20<01:21, 99.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  19% 1.90G/9.98G [00:20<01:09, 116MB/s] \u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  53% 1.87G/3.50G [00:20<00:24, 66.9MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  19% 1.92G/9.98G [00:20<01:06, 121MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  54% 1.89G/3.50G [00:20<00:18, 85.4MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  19% 1.94G/9.98G [00:20<01:01, 132MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  55% 1.91G/3.50G [00:20<00:16, 98.5MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  20% 1.96G/9.98G [00:20<00:58, 138MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  55% 1.93G/3.50G [00:20<00:13, 114MB/s] \u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  20% 1.98G/9.98G [00:20<00:52, 151MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  56% 1.95G/3.50G [00:20<00:11, 130MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  20% 2.00G/9.98G [00:20<00:50, 159MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  56% 1.97G/3.50G [00:20<00:10, 141MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  20% 2.02G/9.98G [00:21<00:48, 165MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  57% 1.99G/3.50G [00:21<00:09, 151MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  20% 2.04G/9.98G [00:21<00:46, 172MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  58% 2.01G/3.50G [00:21<00:09, 156MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  21% 2.07G/9.98G [00:21<00:46, 171MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  58% 2.03G/3.50G [00:21<00:09, 162MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  21% 2.09G/9.98G [00:21<00:44, 175MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  59% 2.06G/3.50G [00:21<00:08, 163MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  21% 2.11G/9.98G [00:21<00:44, 175MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  59% 2.08G/3.50G [00:21<00:08, 167MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  21% 2.13G/9.98G [00:21<00:44, 178MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  60% 2.10G/3.50G [00:21<00:08, 169MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  22% 2.15G/9.98G [00:21<00:45, 173MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  61% 2.12G/3.50G [00:21<00:08, 170MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  22% 2.17G/9.98G [00:21<00:44, 175MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  61% 2.14G/3.50G [00:21<00:08, 159MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  22% 2.19G/9.98G [00:22<00:47, 163MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  62% 2.16G/3.50G [00:22<00:09, 148MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  22% 2.21G/9.98G [00:22<00:51, 150MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  62% 2.18G/3.50G [00:22<00:10, 121MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  22% 2.23G/9.98G [00:22<00:58, 131MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  63% 2.20G/3.50G [00:22<00:09, 138MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  23% 2.25G/9.98G [00:22<00:59, 130MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  64% 2.22G/3.50G [00:22<00:10, 124MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  23% 2.28G/9.98G [00:22<01:05, 117MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  64% 2.24G/3.50G [00:23<00:13, 90.7MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  23% 2.30G/9.98G [00:23<01:28, 86.6MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  65% 2.26G/3.50G [00:23<00:12, 98.7MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  23% 2.31G/9.98G [00:23<01:26, 89.0MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  65% 2.29G/3.50G [00:23<00:11, 102MB/s] \u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  23% 2.33G/9.98G [00:23<01:19, 96.8MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  66% 2.31G/3.50G [00:23<00:11, 103MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  24% 2.35G/9.98G [00:23<01:17, 98.3MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  67% 2.33G/3.50G [00:23<00:11, 105MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  24% 2.37G/9.98G [00:23<01:15, 100MB/s] \u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  67% 2.35G/3.50G [00:24<00:11, 104MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  24% 2.39G/9.98G [00:24<01:14, 102MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  68% 2.37G/3.50G [00:24<00:10, 103MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  24% 2.41G/9.98G [00:24<01:13, 103MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  68% 2.39G/3.50G [00:24<00:10, 107MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  24% 2.43G/9.98G [00:24<01:09, 108MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  69% 2.41G/3.50G [00:24<00:09, 115MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  25% 2.45G/9.98G [00:24<01:06, 114MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  69% 2.43G/3.50G [00:24<00:08, 122MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  25% 2.47G/9.98G [00:24<01:04, 116MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  70% 2.45G/3.50G [00:24<00:08, 128MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  25% 2.50G/9.98G [00:24<00:56, 131MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  71% 2.47G/3.50G [00:24<00:07, 141MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  25% 2.52G/9.98G [00:25<00:52, 143MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  71% 2.50G/3.50G [00:25<00:06, 153MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  25% 2.54G/9.98G [00:25<00:47, 155MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  72% 2.52G/3.50G [00:25<00:06, 162MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  26% 2.56G/9.98G [00:25<00:44, 165MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  72% 2.54G/3.50G [00:25<00:05, 169MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  26% 2.58G/9.98G [00:25<00:43, 172MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  73% 2.56G/3.50G [00:25<00:05, 175MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  26% 2.60G/9.98G [00:25<00:41, 179MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  74% 2.58G/3.50G [00:25<00:05, 176MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  26% 2.62G/9.98G [00:25<00:40, 180MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  74% 2.60G/3.50G [00:25<00:05, 171MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  26% 2.64G/9.98G [00:25<00:43, 168MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  75% 2.62G/3.50G [00:25<00:05, 158MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  27% 2.66G/9.98G [00:25<00:47, 155MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  75% 2.64G/3.50G [00:25<00:05, 147MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  27% 2.68G/9.98G [00:26<00:50, 145MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  76% 2.66G/3.50G [00:26<00:05, 140MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  27% 2.71G/9.98G [00:26<00:52, 138MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  77% 2.68G/3.50G [00:26<00:05, 138MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  27% 2.73G/9.98G [00:26<00:56, 129MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  77% 2.71G/3.50G [00:26<00:06, 131MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  28% 2.75G/9.98G [00:26<00:54, 132MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  78% 2.73G/3.50G [00:26<00:05, 132MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  28% 2.77G/9.98G [00:26<00:55, 130MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  78% 2.75G/3.50G [00:26<00:05, 133MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  28% 2.79G/9.98G [00:26<00:56, 126MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  79% 2.77G/3.50G [00:26<00:05, 127MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  28% 2.81G/9.98G [00:27<00:55, 130MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  80% 2.79G/3.50G [00:27<00:05, 126MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  28% 2.83G/9.98G [00:27<00:56, 127MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  80% 2.81G/3.50G [00:27<00:05, 130MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  29% 2.85G/9.98G [00:27<00:56, 127MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  81% 2.83G/3.50G [00:27<00:05, 124MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  29% 2.87G/9.98G [00:27<00:56, 126MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  81% 2.85G/3.50G [00:27<00:06, 102MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  29% 2.89G/9.98G [00:27<01:06, 107MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  82% 2.87G/3.50G [00:27<00:05, 113MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  29% 2.92G/9.98G [00:27<01:00, 118MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  83% 2.89G/3.50G [00:28<00:06, 91.3MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  29% 2.94G/9.98G [00:28<01:16, 92.4MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  83% 2.90G/3.50G [00:28<00:07, 80.2MB/s]\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  83% 2.92G/3.50G [00:29<00:21, 27.4MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  30% 2.96G/9.98G [00:29<03:31, 33.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  30% 2.97G/9.98G [00:29<03:06, 37.5MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  84% 2.94G/3.50G [00:30<00:15, 37.4MB/s]\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  84% 2.95G/3.50G [00:30<00:13, 42.1MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  30% 2.99G/9.98G [00:30<02:28, 47.2MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  84% 2.96G/3.50G [00:30<00:11, 47.0MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  30% 3.00G/9.98G [00:30<02:17, 50.6MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  85% 2.98G/3.50G [00:30<00:08, 61.6MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  30% 3.02G/9.98G [00:30<01:49, 63.7MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  85% 2.99G/3.50G [00:30<00:07, 65.5MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  30% 3.03G/9.98G [00:30<01:43, 67.1MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  86% 3.01G/3.50G [00:30<00:05, 84.7MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  31% 3.05G/9.98G [00:30<01:22, 83.5MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  87% 3.03G/3.50G [00:30<00:04, 103MB/s] \u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  31% 3.07G/9.98G [00:30<01:15, 91.8MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  87% 3.05G/3.50G [00:32<00:12, 37.2MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  31% 3.09G/9.98G [00:32<03:05, 37.0MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  88% 3.07G/3.50G [00:32<00:08, 50.5MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  31% 3.11G/9.98G [00:32<02:18, 49.6MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  88% 3.09G/3.50G [00:32<00:06, 66.0MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  31% 3.14G/9.98G [00:32<01:45, 64.7MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  89% 3.11G/3.50G [00:32<00:04, 83.2MB/s]\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  90% 3.14G/3.50G [00:32<00:03, 102MB/s] \u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  90% 3.16G/3.50G [00:32<00:02, 120MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  32% 3.16G/9.98G [00:32<01:46, 64.0MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  91% 3.18G/3.50G [00:32<00:02, 135MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  32% 3.18G/9.98G [00:32<01:24, 80.3MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  91% 3.20G/3.50G [00:32<00:02, 146MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  32% 3.20G/9.98G [00:33<01:10, 96.4MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  92% 3.22G/3.50G [00:32<00:01, 158MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  32% 3.22G/9.98G [00:33<00:59, 113MB/s] \u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  93% 3.24G/3.50G [00:33<00:01, 168MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  32% 3.24G/9.98G [00:33<00:52, 130MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  93% 3.26G/3.50G [00:33<00:01, 170MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  33% 3.26G/9.98G [00:33<00:46, 143MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  94% 3.28G/3.50G [00:33<00:01, 179MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  33% 3.28G/9.98G [00:33<00:43, 153MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  33% 3.30G/9.98G [00:33<00:40, 165MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  33% 3.32G/9.98G [00:33<00:38, 175MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  94% 3.30G/3.50G [00:33<00:01, 117MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  34% 3.34G/9.98G [00:33<00:36, 183MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  95% 3.32G/3.50G [00:33<00:01, 135MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  34% 3.38G/9.98G [00:33<00:34, 189MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  96% 3.36G/3.50G [00:33<00:00, 156MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  34% 3.40G/9.98G [00:34<00:34, 191MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  96% 3.38G/3.50G [00:34<00:01, 116MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  34% 3.43G/9.98G [00:34<00:40, 160MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  97% 3.40G/3.50G [00:34<00:00, 119MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  35% 3.45G/9.98G [00:34<00:44, 146MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  98% 3.42G/3.50G [00:34<00:00, 119MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  35% 3.47G/9.98G [00:34<00:46, 139MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  98% 3.44G/3.50G [00:34<00:00, 117MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  35% 3.49G/9.98G [00:34<00:48, 132MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  99% 3.46G/3.50G [00:34<00:00, 114MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  35% 3.51G/9.98G [00:35<00:51, 126MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin:  99% 3.48G/3.50G [00:35<00:00, 112MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  35% 3.53G/9.98G [00:35<00:52, 122MB/s]\u001b[A\u001b[A\n",
            "Downloading (â€¦)l-00002-of-00002.bin: 100% 3.50G/3.50G [00:35<00:00, 107MB/s]\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00002-of-00002.bin: 100% 3.50G/3.50G [00:35<00:00, 99.0MB/s]\n",
            "\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  36% 3.58G/9.98G [00:36<02:12, 48.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  36% 3.59G/9.98G [00:38<05:46, 18.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  36% 3.62G/9.98G [00:38<03:29, 30.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  37% 3.65G/9.98G [00:38<02:20, 45.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  37% 3.68G/9.98G [00:39<01:40, 62.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  37% 3.71G/9.98G [00:39<01:16, 82.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  38% 3.74G/9.98G [00:39<01:00, 103MB/s] \u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  38% 3.77G/9.98G [00:39<00:49, 124MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  38% 3.80G/9.98G [00:39<00:46, 133MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  38% 3.82G/9.98G [00:39<00:45, 135MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  38% 3.84G/9.98G [00:39<00:45, 136MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  39% 3.86G/9.98G [00:40<00:43, 142MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  39% 3.88G/9.98G [00:40<00:40, 150MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  39% 3.90G/9.98G [00:40<00:38, 157MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  39% 3.93G/9.98G [00:40<00:35, 172MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  40% 3.95G/9.98G [00:40<00:35, 172MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  40% 3.97G/9.98G [00:40<00:33, 179MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  40% 4.00G/9.98G [00:40<00:32, 184MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  40% 4.02G/9.98G [00:40<00:32, 185MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  40% 4.04G/9.98G [00:41<00:32, 185MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  41% 4.06G/9.98G [00:41<00:32, 184MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  41% 4.08G/9.98G [00:41<00:31, 186MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  41% 4.10G/9.98G [00:41<00:31, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  41% 4.12G/9.98G [00:41<00:30, 192MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  42% 4.14G/9.98G [00:41<00:29, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  42% 4.16G/9.98G [00:41<00:29, 194MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  42% 4.18G/9.98G [00:41<00:29, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  42% 4.22G/9.98G [00:41<00:28, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  43% 4.25G/9.98G [00:42<00:28, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  43% 4.28G/9.98G [00:42<00:27, 208MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  43% 4.31G/9.98G [00:42<00:26, 212MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  44% 4.34G/9.98G [00:42<00:26, 215MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  44% 4.37G/9.98G [00:42<00:26, 215MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  44% 4.40G/9.98G [00:42<00:25, 216MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  44% 4.44G/9.98G [00:42<00:25, 217MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  45% 4.47G/9.98G [00:43<00:25, 218MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  45% 4.50G/9.98G [00:43<00:35, 152MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  45% 4.53G/9.98G [00:43<00:32, 167MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  46% 4.55G/9.98G [00:43<00:31, 174MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  46% 4.57G/9.98G [00:43<00:29, 181MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  46% 4.60G/9.98G [00:43<00:28, 192MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  46% 4.63G/9.98G [00:44<00:26, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  47% 4.67G/9.98G [00:44<00:25, 205MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  47% 4.70G/9.98G [00:44<00:25, 210MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  47% 4.73G/9.98G [00:44<00:25, 209MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  48% 4.76G/9.98G [00:44<00:25, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  48% 4.78G/9.98G [00:44<00:25, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  48% 4.80G/9.98G [00:44<00:26, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  48% 4.82G/9.98G [00:45<00:26, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  49% 4.84G/9.98G [00:45<00:26, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  49% 4.87G/9.98G [00:45<00:26, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  49% 4.89G/9.98G [00:45<00:26, 192MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  49% 4.91G/9.98G [00:45<00:26, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  49% 4.93G/9.98G [00:45<00:25, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  50% 4.95G/9.98G [00:45<00:26, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  50% 4.97G/9.98G [00:45<00:25, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  50% 4.99G/9.98G [00:45<00:26, 187MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  50% 5.01G/9.98G [00:46<00:26, 185MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  50% 5.03G/9.98G [00:46<00:26, 187MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  51% 5.05G/9.98G [00:46<00:25, 190MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  51% 5.08G/9.98G [00:46<00:25, 192MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  51% 5.10G/9.98G [00:46<00:25, 192MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  51% 5.12G/9.98G [00:46<00:24, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  52% 5.15G/9.98G [00:46<00:24, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  52% 5.18G/9.98G [00:46<00:23, 205MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  52% 5.21G/9.98G [00:47<00:22, 208MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  52% 5.23G/9.98G [00:47<00:22, 207MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  53% 5.25G/9.98G [00:47<00:22, 207MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  53% 5.28G/9.98G [00:47<00:22, 210MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  53% 5.31G/9.98G [00:47<00:22, 210MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  53% 5.34G/9.98G [00:47<00:21, 212MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  54% 5.37G/9.98G [00:47<00:21, 212MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  54% 5.40G/9.98G [00:47<00:22, 206MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  54% 5.42G/9.98G [00:48<00:22, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  55% 5.44G/9.98G [00:48<00:22, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  55% 5.46G/9.98G [00:48<00:22, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  55% 5.48G/9.98G [00:48<00:23, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  55% 5.51G/9.98G [00:48<00:23, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  55% 5.53G/9.98G [00:48<00:23, 192MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  56% 5.55G/9.98G [00:48<00:23, 191MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  56% 5.57G/9.98G [00:48<00:22, 192MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  56% 5.60G/9.98G [00:48<00:21, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  56% 5.62G/9.98G [00:49<00:21, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  57% 5.64G/9.98G [00:49<00:21, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  57% 5.67G/9.98G [00:49<00:20, 206MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  57% 5.70G/9.98G [00:49<00:20, 209MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  57% 5.73G/9.98G [00:49<00:20, 209MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  58% 5.75G/9.98G [00:49<00:20, 208MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  58% 5.78G/9.98G [00:49<00:20, 210MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  58% 5.80G/9.98G [00:49<00:20, 208MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  58% 5.83G/9.98G [00:50<00:19, 208MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  59% 5.86G/9.98G [00:50<00:19, 209MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  59% 5.88G/9.98G [00:50<00:19, 208MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  59% 5.90G/9.98G [00:50<00:20, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  59% 5.92G/9.98G [00:50<00:20, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  60% 5.95G/9.98G [00:50<00:20, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  60% 5.97G/9.98G [00:50<00:20, 194MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  60% 5.99G/9.98G [00:50<00:21, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  60% 6.01G/9.98G [00:50<00:21, 187MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  60% 6.03G/9.98G [00:51<00:21, 185MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  61% 6.05G/9.98G [00:51<00:20, 187MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  61% 6.07G/9.98G [00:51<00:21, 186MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  61% 6.09G/9.98G [00:51<00:20, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  61% 6.11G/9.98G [00:51<00:19, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  61% 6.13G/9.98G [00:51<00:19, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  62% 6.16G/9.98G [00:51<00:19, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  62% 6.19G/9.98G [00:51<00:18, 206MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  62% 6.22G/9.98G [00:52<00:18, 206MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  63% 6.25G/9.98G [00:52<00:17, 210MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  63% 6.27G/9.98G [00:52<00:17, 210MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  63% 6.30G/9.98G [00:52<00:17, 211MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  63% 6.33G/9.98G [00:52<00:17, 212MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  64% 6.36G/9.98G [00:52<00:16, 213MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  64% 6.40G/9.98G [00:52<00:16, 214MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  64% 6.43G/9.98G [00:52<00:16, 212MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  65% 6.46G/9.98G [00:53<00:17, 206MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  65% 6.48G/9.98G [00:53<00:17, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  65% 6.50G/9.98G [00:53<00:17, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  65% 6.52G/9.98G [00:53<00:17, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  66% 6.54G/9.98G [00:53<00:17, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  66% 6.56G/9.98G [00:53<00:17, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  66% 6.60G/9.98G [00:53<00:16, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  66% 6.62G/9.98G [00:53<00:16, 205MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  67% 6.65G/9.98G [00:54<00:15, 210MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  67% 6.68G/9.98G [00:54<00:15, 210MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  67% 6.70G/9.98G [00:54<00:15, 209MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  67% 6.72G/9.98G [00:54<00:15, 206MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  68% 6.74G/9.98G [00:54<00:15, 205MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  68% 6.77G/9.98G [00:54<00:15, 209MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  68% 6.81G/9.98G [00:54<00:14, 212MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  69% 6.84G/9.98G [00:54<00:14, 214MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  69% 6.87G/9.98G [00:55<00:14, 212MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  69% 6.90G/9.98G [00:55<00:14, 209MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  69% 6.92G/9.98G [00:55<00:14, 207MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  70% 6.94G/9.98G [00:55<00:14, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  70% 6.96G/9.98G [00:55<00:15, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  70% 6.98G/9.98G [00:55<00:15, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  70% 7.00G/9.98G [00:55<00:15, 194MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  70% 7.03G/9.98G [00:55<00:15, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  71% 7.05G/9.98G [00:56<00:16, 183MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  71% 7.07G/9.98G [00:56<00:15, 185MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  71% 7.09G/9.98G [00:56<00:15, 183MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  71% 7.11G/9.98G [00:56<00:15, 184MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  71% 7.13G/9.98G [00:56<00:15, 188MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  72% 7.15G/9.98G [00:56<00:14, 191MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  72% 7.17G/9.98G [00:56<00:14, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  72% 7.20G/9.98G [00:56<00:13, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  73% 7.24G/9.98G [00:57<00:13, 210MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  73% 7.27G/9.98G [00:57<00:12, 211MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  73% 7.30G/9.98G [00:57<00:12, 214MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  73% 7.33G/9.98G [00:57<00:12, 210MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  74% 7.35G/9.98G [00:57<00:13, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  74% 7.37G/9.98G [00:57<00:13, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  74% 7.39G/9.98G [00:57<00:13, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  74% 7.41G/9.98G [00:57<00:13, 185MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  75% 7.43G/9.98G [00:58<00:13, 190MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  75% 7.46G/9.98G [00:58<00:12, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  75% 7.49G/9.98G [00:58<00:12, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  75% 7.52G/9.98G [00:58<00:11, 207MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  76% 7.55G/9.98G [00:58<00:11, 212MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  76% 7.58G/9.98G [00:59<00:34, 69.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  76% 7.60G/9.98G [00:59<00:29, 81.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  77% 7.63G/9.98G [00:59<00:22, 103MB/s] \u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  77% 7.67G/9.98G [01:00<00:18, 125MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  77% 7.70G/9.98G [01:00<00:15, 144MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  77% 7.73G/9.98G [01:00<00:14, 160MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  78% 7.76G/9.98G [01:00<00:12, 175MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  78% 7.78G/9.98G [01:00<00:12, 178MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  78% 7.80G/9.98G [01:00<00:11, 182MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  78% 7.82G/9.98G [01:00<00:11, 182MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  79% 7.84G/9.98G [01:00<00:11, 183MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  79% 7.86G/9.98G [01:01<00:12, 175MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  79% 7.89G/9.98G [01:01<00:11, 178MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  79% 7.91G/9.98G [01:01<00:11, 183MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  79% 7.93G/9.98G [01:01<00:10, 186MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  80% 7.95G/9.98G [01:01<00:11, 184MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  80% 7.97G/9.98G [01:01<00:10, 186MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  80% 7.99G/9.98G [01:01<00:10, 188MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  80% 8.01G/9.98G [01:01<00:10, 190MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  81% 8.03G/9.98G [01:01<00:10, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  81% 8.05G/9.98G [01:02<00:10, 192MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  81% 8.07G/9.98G [01:02<00:09, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  81% 8.10G/9.98G [01:02<00:09, 194MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  81% 8.12G/9.98G [01:02<00:09, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  82% 8.14G/9.98G [01:02<00:09, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  82% 8.16G/9.98G [01:02<00:09, 194MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  82% 8.18G/9.98G [01:02<00:09, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  82% 8.21G/9.98G [01:02<00:08, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  83% 8.24G/9.98G [01:02<00:08, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  83% 8.27G/9.98G [01:03<00:08, 207MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  83% 8.29G/9.98G [01:03<00:08, 207MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  83% 8.32G/9.98G [01:03<00:08, 207MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  84% 8.35G/9.98G [01:03<00:07, 210MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  84% 8.38G/9.98G [01:03<00:07, 212MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  84% 8.41G/9.98G [01:03<00:07, 212MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  85% 8.44G/9.98G [01:03<00:07, 205MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  85% 8.46G/9.98G [01:04<00:07, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  85% 8.48G/9.98G [01:04<00:08, 183MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  85% 8.50G/9.98G [01:04<00:07, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  86% 8.54G/9.98G [01:04<00:07, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  86% 8.56G/9.98G [01:04<00:07, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  86% 8.59G/9.98G [01:04<00:06, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  86% 8.61G/9.98G [01:04<00:06, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  86% 8.63G/9.98G [01:04<00:06, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  87% 8.65G/9.98G [01:05<00:06, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  87% 8.67G/9.98G [01:05<00:06, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  87% 8.70G/9.98G [01:05<00:06, 205MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  88% 8.73G/9.98G [01:05<00:05, 208MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  88% 8.76G/9.98G [01:05<00:05, 205MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  88% 8.78G/9.98G [01:05<00:05, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  88% 8.80G/9.98G [01:05<00:05, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  88% 8.82G/9.98G [01:05<00:05, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  89% 8.84G/9.98G [01:05<00:05, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  89% 8.86G/9.98G [01:06<00:05, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  89% 8.88G/9.98G [01:06<00:05, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  89% 8.90G/9.98G [01:06<00:05, 194MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  89% 8.92G/9.98G [01:06<00:05, 194MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  90% 8.94G/9.98G [01:06<00:05, 192MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  90% 8.97G/9.98G [01:06<00:05, 191MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  90% 8.99G/9.98G [01:06<00:05, 191MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  90% 9.01G/9.98G [01:06<00:05, 190MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  90% 9.03G/9.98G [01:06<00:05, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  91% 9.05G/9.98G [01:07<00:05, 185MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  91% 9.07G/9.98G [01:07<00:05, 175MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  91% 9.09G/9.98G [01:07<00:04, 183MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  91% 9.11G/9.98G [01:07<00:04, 190MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  92% 9.14G/9.98G [01:07<00:04, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  92% 9.18G/9.98G [01:07<00:03, 207MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  92% 9.21G/9.98G [01:07<00:03, 211MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  93% 9.24G/9.98G [01:07<00:03, 213MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  93% 9.27G/9.98G [01:08<00:03, 216MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  93% 9.30G/9.98G [01:08<00:03, 216MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  94% 9.33G/9.98G [01:08<00:02, 217MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  94% 9.36G/9.98G [01:08<00:02, 218MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  94% 9.40G/9.98G [01:08<00:02, 212MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  94% 9.43G/9.98G [01:08<00:02, 212MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  95% 9.46G/9.98G [01:09<00:02, 213MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  95% 9.49G/9.98G [01:09<00:02, 206MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  95% 9.52G/9.98G [01:09<00:02, 210MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  96% 9.55G/9.98G [01:09<00:01, 215MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  96% 9.58G/9.98G [01:09<00:01, 216MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  96% 9.62G/9.98G [01:09<00:01, 217MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  97% 9.65G/9.98G [01:09<00:01, 215MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  97% 9.68G/9.98G [01:10<00:01, 209MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  97% 9.70G/9.98G [01:10<00:01, 205MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  97% 9.72G/9.98G [01:10<00:01, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  98% 9.74G/9.98G [01:10<00:01, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  98% 9.76G/9.98G [01:10<00:01, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  98% 9.78G/9.98G [01:10<00:00, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  98% 9.80G/9.98G [01:10<00:00, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  98% 9.83G/9.98G [01:10<00:00, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  99% 9.85G/9.98G [01:10<00:00, 187MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  99% 9.87G/9.98G [01:11<00:00, 187MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  99% 9.89G/9.98G [01:11<00:00, 186MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin:  99% 9.91G/9.98G [01:11<00:00, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin: 100% 9.93G/9.98G [01:11<00:00, 188MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin: 100% 9.95G/9.98G [01:11<00:00, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (â€¦)l-00001-of-00002.bin: 100% 9.98G/9.98G [01:11<00:00, 139MB/s]\n",
            "Fetching 11 files: 100% 11/11 [01:12<00:00,  6.62s/it]\n",
            "Loading ckpt pytorch_model-00001-of-00002.bin\n",
            "Merging...\n",
            "Saving ckpt pytorch_model-00001-of-00002.bin to llama-2-7b-combined in HF format...\n",
            "Loading ckpt pytorch_model-00002-of-00002.bin\n",
            "Merging...\n",
            "Saving ckpt pytorch_model-00002-of-00002.bin to llama-2-7b-combined in HF format...\n",
            "Saving tokenizer\n",
            "Saving config.json\n",
            "Saving generation_config.json\n",
            "Saving pytorch_model.bin.index.json\n",
            "Done.\n",
            "Check output dir: llama-2-7b-combined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## æ£€æŸ¥ä¿®æ”¹config.jsonï¼ˆå¦‚æœä½¿ç”¨metaåŸç‰ˆåˆ™å¯è·³è¿‡ï¼‰\n",
        "\n",
        "- Llama-2çš„configä¸­é€”æœ‰ä¸€æ¬¡æ›´æ–°ã€‚å› ä¸ºæœ¬æ•™ç¨‹é‡Œä½¿ç”¨çš„æ˜¯ç¬¬ä¸‰æ–¹çš„æƒé‡ï¼Œå¹¶æ²¡æœ‰åŠæ—¶æ›´æ–°å¯¹åº”çš„`config.json`æ–‡ä»¶ã€‚\n",
        "\n",
        "- è¯·æ‰‹åŠ¨æ‰“å¼€`llama-2-7b-combined`æ–‡ä»¶å¤¹ä¸‹çš„config.jsonï¼ˆå¯ç›´æ¥åŒå‡»æ‰“å¼€ï¼‰ï¼Œå°†`max_position_embeddings`å­—æ®µç”±`2048`æ”¹ä¸º`4096`ã€‚cmd/ctrl+sä¿å­˜å³å¯ã€‚"
      ],
      "metadata": {
        "id": "4lymK5fUTKCu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## é‡åŒ–æ¨¡å‹\n",
        "æ¥ä¸‹æ¥æˆ‘ä»¬ä½¿ç”¨[llama.cpp](https://github.com/ggerganov/llama.cpp)å·¥å…·å¯¹ä¸Šä¸€æ­¥ç”Ÿæˆçš„å…¨é‡ç‰ˆæœ¬æƒé‡è¿›è¡Œè½¬æ¢ï¼Œç”Ÿæˆé‡åŒ–æ¨¡å‹ã€‚\n",
        "\n",
        "### ç¼–è¯‘å·¥å…·\n",
        "\n",
        "é¦–å…ˆå¯¹llama.cppå·¥å…·è¿›è¡Œç¼–è¯‘ã€‚"
      ],
      "metadata": {
        "id": "ueexcKo-Q_EW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && make"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GbjsT2wRRCR",
        "outputId": "49fd1d75-ba19-4cdb-e87f-ba1136588e5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I llama.cpp build info: \n",
            "I UNAME_S:  Linux\n",
            "I UNAME_P:  x86_64\n",
            "I UNAME_M:  x86_64\n",
            "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
            "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
            "I LDFLAGS:  \n",
            "I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
            "I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
            "\n",
            "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS   -c ggml.c -o ggml.o\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -c llama.cpp -o llama.o\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -c examples/common.cpp -o common.o\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -c examples/grammar-parser.cpp -o grammar-parser.o\n",
            "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS   -c -o k_quants.o k_quants.c\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/main/main.cpp ggml.o llama.o common.o grammar-parser.o k_quants.o -o main \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/quantize/quantize.cpp ggml.o llama.o k_quants.o -o quantize \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o -o quantize-stats \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o -o perplexity \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o -o embedding \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS pocs/vdot/vdot.cpp ggml.o k_quants.o -o vdot \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o -o train-text-from-scratch \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o -o simple \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -Iexamples/server examples/server/server.cpp ggml.o llama.o common.o k_quants.o -o server  \n",
            "g++ --shared -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o -o libembdinput.so \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o -o embd-input-test  -L. -lembdinput\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### æ¨¡å‹è½¬æ¢ä¸ºGGMLæ ¼å¼ï¼ˆFP16ï¼‰\n",
        "\n",
        "è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å°†æ¨¡å‹è½¬æ¢ä¸ºGGMLæ ¼å¼ï¼ˆFP16ï¼‰ã€‚"
      ],
      "metadata": {
        "id": "gw2xpYC0RcQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && python convert.py ../llama-2-7b-combined"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUHeoTMQS1AQ",
        "outputId": "87de42b7-9390-4672-a61f-33b3557b97b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model file ../llama-2-7b-combined/pytorch_model-00001-of-00002.bin\n",
            "Loading model file ../llama-2-7b-combined/pytorch_model-00001-of-00002.bin\n",
            "Loading model file ../llama-2-7b-combined/pytorch_model-00002-of-00002.bin\n",
            "vocabtype: spm\n",
            "Loading vocab file ../llama-2-7b-combined/tokenizer.model\n",
            "params: n_vocab:55296 n_embd:4096 n_mult:256 n_head:32 n_layer:32\n",
            "Writing vocab...\n",
            "[  1/291] Writing tensor tok_embeddings.weight                  | size  55296 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[  2/291] Writing tensor norm.weight                            | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[  3/291] Writing tensor output.weight                          | size  55296 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[  4/291] Writing tensor layers.0.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[  5/291] Writing tensor layers.0.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[  6/291] Writing tensor layers.0.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[  7/291] Writing tensor layers.0.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[  8/291] Writing tensor layers.0.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[  9/291] Writing tensor layers.0.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 10/291] Writing tensor layers.0.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[ 11/291] Writing tensor layers.0.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 12/291] Writing tensor layers.0.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 13/291] Writing tensor layers.1.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 14/291] Writing tensor layers.1.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 15/291] Writing tensor layers.1.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 16/291] Writing tensor layers.1.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 17/291] Writing tensor layers.1.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 18/291] Writing tensor layers.1.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 19/291] Writing tensor layers.1.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[ 20/291] Writing tensor layers.1.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 21/291] Writing tensor layers.1.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 22/291] Writing tensor layers.2.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 23/291] Writing tensor layers.2.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 24/291] Writing tensor layers.2.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 25/291] Writing tensor layers.2.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 26/291] Writing tensor layers.2.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 27/291] Writing tensor layers.2.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 28/291] Writing tensor layers.2.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[ 29/291] Writing tensor layers.2.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 30/291] Writing tensor layers.2.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 31/291] Writing tensor layers.3.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 32/291] Writing tensor layers.3.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 33/291] Writing tensor layers.3.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 34/291] Writing tensor layers.3.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 35/291] Writing tensor layers.3.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 36/291] Writing tensor layers.3.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 37/291] Writing tensor layers.3.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[ 38/291] Writing tensor layers.3.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 39/291] Writing tensor layers.3.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 40/291] Writing tensor layers.4.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 41/291] Writing tensor layers.4.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 42/291] Writing tensor layers.4.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 43/291] Writing tensor layers.4.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 44/291] Writing tensor layers.4.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 45/291] Writing tensor layers.4.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 46/291] Writing tensor layers.4.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[ 47/291] Writing tensor layers.4.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 48/291] Writing tensor layers.4.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 49/291] Writing tensor layers.5.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 50/291] Writing tensor layers.5.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 51/291] Writing tensor layers.5.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 52/291] Writing tensor layers.5.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 53/291] Writing tensor layers.5.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 54/291] Writing tensor layers.5.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 55/291] Writing tensor layers.5.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[ 56/291] Writing tensor layers.5.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 57/291] Writing tensor layers.5.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 58/291] Writing tensor layers.6.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 59/291] Writing tensor layers.6.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 60/291] Writing tensor layers.6.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 61/291] Writing tensor layers.6.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 62/291] Writing tensor layers.6.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 63/291] Writing tensor layers.6.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 64/291] Writing tensor layers.6.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[ 65/291] Writing tensor layers.6.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 66/291] Writing tensor layers.6.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 67/291] Writing tensor layers.7.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 68/291] Writing tensor layers.7.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 69/291] Writing tensor layers.7.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 70/291] Writing tensor layers.7.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 71/291] Writing tensor layers.7.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 72/291] Writing tensor layers.7.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 73/291] Writing tensor layers.7.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[ 74/291] Writing tensor layers.7.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 75/291] Writing tensor layers.7.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 76/291] Writing tensor layers.8.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 77/291] Writing tensor layers.8.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 78/291] Writing tensor layers.8.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 79/291] Writing tensor layers.8.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 80/291] Writing tensor layers.8.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 81/291] Writing tensor layers.8.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 82/291] Writing tensor layers.8.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[ 83/291] Writing tensor layers.8.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 84/291] Writing tensor layers.8.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 85/291] Writing tensor layers.9.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 86/291] Writing tensor layers.9.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 87/291] Writing tensor layers.9.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 88/291] Writing tensor layers.9.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 89/291] Writing tensor layers.9.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 90/291] Writing tensor layers.9.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 91/291] Writing tensor layers.9.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[ 92/291] Writing tensor layers.9.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 93/291] Writing tensor layers.9.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 94/291] Writing tensor layers.10.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 95/291] Writing tensor layers.10.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 96/291] Writing tensor layers.10.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 97/291] Writing tensor layers.10.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[ 98/291] Writing tensor layers.10.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 99/291] Writing tensor layers.10.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[100/291] Writing tensor layers.10.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[101/291] Writing tensor layers.10.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[102/291] Writing tensor layers.10.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[103/291] Writing tensor layers.11.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[104/291] Writing tensor layers.11.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[105/291] Writing tensor layers.11.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[106/291] Writing tensor layers.11.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[107/291] Writing tensor layers.11.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[108/291] Writing tensor layers.11.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[109/291] Writing tensor layers.11.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[110/291] Writing tensor layers.11.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[111/291] Writing tensor layers.11.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[112/291] Writing tensor layers.12.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[113/291] Writing tensor layers.12.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[114/291] Writing tensor layers.12.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[115/291] Writing tensor layers.12.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[116/291] Writing tensor layers.12.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[117/291] Writing tensor layers.12.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[118/291] Writing tensor layers.12.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[119/291] Writing tensor layers.12.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[120/291] Writing tensor layers.12.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[121/291] Writing tensor layers.13.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[122/291] Writing tensor layers.13.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[123/291] Writing tensor layers.13.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[124/291] Writing tensor layers.13.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[125/291] Writing tensor layers.13.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[126/291] Writing tensor layers.13.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[127/291] Writing tensor layers.13.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[128/291] Writing tensor layers.13.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[129/291] Writing tensor layers.13.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[130/291] Writing tensor layers.14.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[131/291] Writing tensor layers.14.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[132/291] Writing tensor layers.14.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[133/291] Writing tensor layers.14.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[134/291] Writing tensor layers.14.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[135/291] Writing tensor layers.14.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[136/291] Writing tensor layers.14.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[137/291] Writing tensor layers.14.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[138/291] Writing tensor layers.14.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[139/291] Writing tensor layers.15.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[140/291] Writing tensor layers.15.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[141/291] Writing tensor layers.15.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[142/291] Writing tensor layers.15.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[143/291] Writing tensor layers.15.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[144/291] Writing tensor layers.15.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[145/291] Writing tensor layers.15.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[146/291] Writing tensor layers.15.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[147/291] Writing tensor layers.15.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[148/291] Writing tensor layers.16.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[149/291] Writing tensor layers.16.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[150/291] Writing tensor layers.16.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[151/291] Writing tensor layers.16.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[152/291] Writing tensor layers.16.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[153/291] Writing tensor layers.16.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[154/291] Writing tensor layers.16.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[155/291] Writing tensor layers.16.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[156/291] Writing tensor layers.16.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[157/291] Writing tensor layers.17.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[158/291] Writing tensor layers.17.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[159/291] Writing tensor layers.17.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[160/291] Writing tensor layers.17.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[161/291] Writing tensor layers.17.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[162/291] Writing tensor layers.17.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[163/291] Writing tensor layers.17.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[164/291] Writing tensor layers.17.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[165/291] Writing tensor layers.17.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[166/291] Writing tensor layers.18.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[167/291] Writing tensor layers.18.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[168/291] Writing tensor layers.18.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[169/291] Writing tensor layers.18.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[170/291] Writing tensor layers.18.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[171/291] Writing tensor layers.18.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[172/291] Writing tensor layers.18.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[173/291] Writing tensor layers.18.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[174/291] Writing tensor layers.18.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[175/291] Writing tensor layers.19.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[176/291] Writing tensor layers.19.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[177/291] Writing tensor layers.19.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[178/291] Writing tensor layers.19.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[179/291] Writing tensor layers.19.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[180/291] Writing tensor layers.19.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[181/291] Writing tensor layers.19.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[182/291] Writing tensor layers.19.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[183/291] Writing tensor layers.19.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[184/291] Writing tensor layers.20.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[185/291] Writing tensor layers.20.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[186/291] Writing tensor layers.20.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[187/291] Writing tensor layers.20.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[188/291] Writing tensor layers.20.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[189/291] Writing tensor layers.20.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[190/291] Writing tensor layers.20.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[191/291] Writing tensor layers.20.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[192/291] Writing tensor layers.20.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[193/291] Writing tensor layers.21.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[194/291] Writing tensor layers.21.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[195/291] Writing tensor layers.21.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[196/291] Writing tensor layers.21.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[197/291] Writing tensor layers.21.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[198/291] Writing tensor layers.21.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[199/291] Writing tensor layers.21.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[200/291] Writing tensor layers.21.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[201/291] Writing tensor layers.21.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[202/291] Writing tensor layers.22.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[203/291] Writing tensor layers.22.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[204/291] Writing tensor layers.22.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[205/291] Writing tensor layers.22.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[206/291] Writing tensor layers.22.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[207/291] Writing tensor layers.22.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[208/291] Writing tensor layers.22.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[209/291] Writing tensor layers.22.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[210/291] Writing tensor layers.22.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[211/291] Writing tensor layers.23.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[212/291] Writing tensor layers.23.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[213/291] Writing tensor layers.23.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[214/291] Writing tensor layers.23.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[215/291] Writing tensor layers.23.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[216/291] Writing tensor layers.23.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[217/291] Writing tensor layers.23.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[218/291] Writing tensor layers.23.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[219/291] Writing tensor layers.23.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[220/291] Writing tensor layers.24.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[221/291] Writing tensor layers.24.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[222/291] Writing tensor layers.24.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[223/291] Writing tensor layers.24.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[224/291] Writing tensor layers.24.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[225/291] Writing tensor layers.24.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[226/291] Writing tensor layers.24.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[227/291] Writing tensor layers.24.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[228/291] Writing tensor layers.24.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[229/291] Writing tensor layers.25.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[230/291] Writing tensor layers.25.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[231/291] Writing tensor layers.25.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[232/291] Writing tensor layers.25.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[233/291] Writing tensor layers.25.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[234/291] Writing tensor layers.25.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[235/291] Writing tensor layers.25.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[236/291] Writing tensor layers.25.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[237/291] Writing tensor layers.25.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[238/291] Writing tensor layers.26.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[239/291] Writing tensor layers.26.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[240/291] Writing tensor layers.26.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[241/291] Writing tensor layers.26.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[242/291] Writing tensor layers.26.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[243/291] Writing tensor layers.26.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[244/291] Writing tensor layers.26.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[245/291] Writing tensor layers.26.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[246/291] Writing tensor layers.26.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[247/291] Writing tensor layers.27.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[248/291] Writing tensor layers.27.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[249/291] Writing tensor layers.27.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[250/291] Writing tensor layers.27.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[251/291] Writing tensor layers.27.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[252/291] Writing tensor layers.27.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[253/291] Writing tensor layers.27.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[254/291] Writing tensor layers.27.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[255/291] Writing tensor layers.27.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[256/291] Writing tensor layers.28.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[257/291] Writing tensor layers.28.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[258/291] Writing tensor layers.28.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[259/291] Writing tensor layers.28.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[260/291] Writing tensor layers.28.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[261/291] Writing tensor layers.28.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[262/291] Writing tensor layers.28.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[263/291] Writing tensor layers.28.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[264/291] Writing tensor layers.28.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[265/291] Writing tensor layers.29.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[266/291] Writing tensor layers.29.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[267/291] Writing tensor layers.29.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[268/291] Writing tensor layers.29.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[269/291] Writing tensor layers.29.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[270/291] Writing tensor layers.29.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[271/291] Writing tensor layers.29.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[272/291] Writing tensor layers.29.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[273/291] Writing tensor layers.29.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[274/291] Writing tensor layers.30.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[275/291] Writing tensor layers.30.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[276/291] Writing tensor layers.30.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[277/291] Writing tensor layers.30.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[278/291] Writing tensor layers.30.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[279/291] Writing tensor layers.30.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[280/291] Writing tensor layers.30.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[281/291] Writing tensor layers.30.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[282/291] Writing tensor layers.30.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[283/291] Writing tensor layers.31.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[284/291] Writing tensor layers.31.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[285/291] Writing tensor layers.31.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[286/291] Writing tensor layers.31.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[287/291] Writing tensor layers.31.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[288/291] Writing tensor layers.31.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[289/291] Writing tensor layers.31.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
            "[290/291] Writing tensor layers.31.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
            "[291/291] Writing tensor layers.31.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "Wrote ../llama-2-7b-combined/ggml-model-f16.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### å°†FP16æ¨¡å‹è¿›è¡Œé‡åŒ–\n",
        "\n",
        "æˆ‘ä»¬è¿›ä¸€æ­¥å°†FP16æ¨¡å‹è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹ï¼Œæ­¤å¤„é€‰æ‹©çš„æ˜¯æ–°ç‰ˆQ6_Kæ–¹æ³•ï¼Œå…¶æ•ˆæœéå¸¸æ¥è¿‘FP16ã€‚"
      ],
      "metadata": {
        "id": "hEZEJAVYCHkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && ./quantize ../llama-2-7b-combined/ggml-model-f16.bin ../llama-2-7b-combined/ggml-model-q6_K.bin q6_K"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xyais7OUVDI",
        "outputId": "17319e87-4986-47a5-80f3-c30dd2b51ff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 916 (b5472ea)\n",
            "main: quantizing '../llama-2-7b-combined/ggml-model-f16.bin' to '../llama-2-7b-combined/ggml-model-q6_K.bin' as Q6_K\n",
            "llama.cpp: loading model from ../llama-2-7b-combined/ggml-model-f16.bin\n",
            "llama.cpp: saving model to ../llama-2-7b-combined/ggml-model-q6_K.bin\n",
            "[   1/ 291]                tok_embeddings.weight -     4096 x 55296, type =    f16, quantizing to q6_K .. size =   432.00 MB ->   177.19 MB | hist: \n",
            "[   2/ 291]                          norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[   3/ 291]                        output.weight -     4096 x 55296, type =    f16, quantizing to q6_K .. size =   432.00 MB ->   177.19 MB | hist: \n",
            "[   4/ 291]         layers.0.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[   5/ 291]         layers.0.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[   6/ 291]         layers.0.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[   7/ 291]         layers.0.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[   8/ 291]       layers.0.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[   9/ 291]      layers.0.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  10/ 291]      layers.0.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  11/ 291]      layers.0.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  12/ 291]             layers.0.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[  13/ 291]         layers.1.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  14/ 291]         layers.1.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  15/ 291]         layers.1.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  16/ 291]         layers.1.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  17/ 291]       layers.1.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[  18/ 291]      layers.1.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  19/ 291]      layers.1.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  20/ 291]      layers.1.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  21/ 291]             layers.1.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[  22/ 291]         layers.2.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  23/ 291]         layers.2.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  24/ 291]         layers.2.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  25/ 291]         layers.2.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  26/ 291]       layers.2.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[  27/ 291]      layers.2.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  28/ 291]      layers.2.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  29/ 291]      layers.2.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  30/ 291]             layers.2.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[  31/ 291]         layers.3.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  32/ 291]         layers.3.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  33/ 291]         layers.3.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  34/ 291]         layers.3.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  35/ 291]       layers.3.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[  36/ 291]      layers.3.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  37/ 291]      layers.3.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  38/ 291]      layers.3.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  39/ 291]             layers.3.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[  40/ 291]         layers.4.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  41/ 291]         layers.4.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  42/ 291]         layers.4.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  43/ 291]         layers.4.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  44/ 291]       layers.4.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[  45/ 291]      layers.4.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  46/ 291]      layers.4.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  47/ 291]      layers.4.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  48/ 291]             layers.4.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[  49/ 291]         layers.5.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  50/ 291]         layers.5.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  51/ 291]         layers.5.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  52/ 291]         layers.5.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  53/ 291]       layers.5.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[  54/ 291]      layers.5.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  55/ 291]      layers.5.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  56/ 291]      layers.5.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  57/ 291]             layers.5.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[  58/ 291]         layers.6.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  59/ 291]         layers.6.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  60/ 291]         layers.6.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  61/ 291]         layers.6.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  62/ 291]       layers.6.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[  63/ 291]      layers.6.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  64/ 291]      layers.6.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  65/ 291]      layers.6.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  66/ 291]             layers.6.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[  67/ 291]         layers.7.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  68/ 291]         layers.7.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  69/ 291]         layers.7.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  70/ 291]         layers.7.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  71/ 291]       layers.7.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[  72/ 291]      layers.7.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  73/ 291]      layers.7.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  74/ 291]      layers.7.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  75/ 291]             layers.7.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[  76/ 291]         layers.8.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  77/ 291]         layers.8.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  78/ 291]         layers.8.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  79/ 291]         layers.8.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  80/ 291]       layers.8.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[  81/ 291]      layers.8.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  82/ 291]      layers.8.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  83/ 291]      layers.8.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  84/ 291]             layers.8.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[  85/ 291]         layers.9.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  86/ 291]         layers.9.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  87/ 291]         layers.9.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  88/ 291]         layers.9.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  89/ 291]       layers.9.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[  90/ 291]      layers.9.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  91/ 291]      layers.9.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  92/ 291]      layers.9.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[  93/ 291]             layers.9.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[  94/ 291]        layers.10.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  95/ 291]        layers.10.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  96/ 291]        layers.10.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  97/ 291]        layers.10.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[  98/ 291]      layers.10.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[  99/ 291]     layers.10.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 100/ 291]     layers.10.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 101/ 291]     layers.10.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 102/ 291]            layers.10.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 103/ 291]        layers.11.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 104/ 291]        layers.11.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 105/ 291]        layers.11.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 106/ 291]        layers.11.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 107/ 291]      layers.11.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 108/ 291]     layers.11.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 109/ 291]     layers.11.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 110/ 291]     layers.11.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 111/ 291]            layers.11.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 112/ 291]        layers.12.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 113/ 291]        layers.12.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 114/ 291]        layers.12.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 115/ 291]        layers.12.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 116/ 291]      layers.12.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 117/ 291]     layers.12.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 118/ 291]     layers.12.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 119/ 291]     layers.12.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 120/ 291]            layers.12.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 121/ 291]        layers.13.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 122/ 291]        layers.13.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 123/ 291]        layers.13.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 124/ 291]        layers.13.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 125/ 291]      layers.13.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 126/ 291]     layers.13.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 127/ 291]     layers.13.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 128/ 291]     layers.13.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 129/ 291]            layers.13.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 130/ 291]        layers.14.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 131/ 291]        layers.14.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 132/ 291]        layers.14.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 133/ 291]        layers.14.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 134/ 291]      layers.14.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 135/ 291]     layers.14.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 136/ 291]     layers.14.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 137/ 291]     layers.14.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 138/ 291]            layers.14.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 139/ 291]        layers.15.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 140/ 291]        layers.15.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 141/ 291]        layers.15.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 142/ 291]        layers.15.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 143/ 291]      layers.15.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 144/ 291]     layers.15.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 145/ 291]     layers.15.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 146/ 291]     layers.15.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 147/ 291]            layers.15.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 148/ 291]        layers.16.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 149/ 291]        layers.16.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 150/ 291]        layers.16.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 151/ 291]        layers.16.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 152/ 291]      layers.16.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 153/ 291]     layers.16.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 154/ 291]     layers.16.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 155/ 291]     layers.16.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 156/ 291]            layers.16.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 157/ 291]        layers.17.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 158/ 291]        layers.17.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 159/ 291]        layers.17.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 160/ 291]        layers.17.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 161/ 291]      layers.17.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 162/ 291]     layers.17.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 163/ 291]     layers.17.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 164/ 291]     layers.17.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 165/ 291]            layers.17.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 166/ 291]        layers.18.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 167/ 291]        layers.18.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 168/ 291]        layers.18.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 169/ 291]        layers.18.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 170/ 291]      layers.18.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 171/ 291]     layers.18.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 172/ 291]     layers.18.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 173/ 291]     layers.18.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 174/ 291]            layers.18.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 175/ 291]        layers.19.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 176/ 291]        layers.19.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 177/ 291]        layers.19.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 178/ 291]        layers.19.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 179/ 291]      layers.19.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 180/ 291]     layers.19.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 181/ 291]     layers.19.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 182/ 291]     layers.19.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 183/ 291]            layers.19.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 184/ 291]        layers.20.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 185/ 291]        layers.20.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 186/ 291]        layers.20.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 187/ 291]        layers.20.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 188/ 291]      layers.20.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 189/ 291]     layers.20.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 190/ 291]     layers.20.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 191/ 291]     layers.20.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 192/ 291]            layers.20.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 193/ 291]        layers.21.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 194/ 291]        layers.21.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 195/ 291]        layers.21.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 196/ 291]        layers.21.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 197/ 291]      layers.21.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 198/ 291]     layers.21.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 199/ 291]     layers.21.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 200/ 291]     layers.21.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 201/ 291]            layers.21.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 202/ 291]        layers.22.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 203/ 291]        layers.22.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 204/ 291]        layers.22.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 205/ 291]        layers.22.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 206/ 291]      layers.22.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 207/ 291]     layers.22.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 208/ 291]     layers.22.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 209/ 291]     layers.22.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 210/ 291]            layers.22.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 211/ 291]        layers.23.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 212/ 291]        layers.23.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 213/ 291]        layers.23.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 214/ 291]        layers.23.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 215/ 291]      layers.23.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 216/ 291]     layers.23.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 217/ 291]     layers.23.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 218/ 291]     layers.23.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 219/ 291]            layers.23.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 220/ 291]        layers.24.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 221/ 291]        layers.24.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 222/ 291]        layers.24.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 223/ 291]        layers.24.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 224/ 291]      layers.24.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 225/ 291]     layers.24.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 226/ 291]     layers.24.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 227/ 291]     layers.24.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 228/ 291]            layers.24.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 229/ 291]        layers.25.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 230/ 291]        layers.25.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 231/ 291]        layers.25.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 232/ 291]        layers.25.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 233/ 291]      layers.25.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 234/ 291]     layers.25.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 235/ 291]     layers.25.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 236/ 291]     layers.25.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 237/ 291]            layers.25.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 238/ 291]        layers.26.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 239/ 291]        layers.26.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 240/ 291]        layers.26.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 241/ 291]        layers.26.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 242/ 291]      layers.26.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 243/ 291]     layers.26.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 244/ 291]     layers.26.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 245/ 291]     layers.26.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 246/ 291]            layers.26.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 247/ 291]        layers.27.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 248/ 291]        layers.27.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 249/ 291]        layers.27.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 250/ 291]        layers.27.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 251/ 291]      layers.27.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 252/ 291]     layers.27.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 253/ 291]     layers.27.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 254/ 291]     layers.27.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 255/ 291]            layers.27.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 256/ 291]        layers.28.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 257/ 291]        layers.28.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 258/ 291]        layers.28.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 259/ 291]        layers.28.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 260/ 291]      layers.28.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 261/ 291]     layers.28.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 262/ 291]     layers.28.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 263/ 291]     layers.28.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 264/ 291]            layers.28.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 265/ 291]        layers.29.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 266/ 291]        layers.29.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 267/ 291]        layers.29.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 268/ 291]        layers.29.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 269/ 291]      layers.29.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 270/ 291]     layers.29.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 271/ 291]     layers.29.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 272/ 291]     layers.29.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 273/ 291]            layers.29.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 274/ 291]        layers.30.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 275/ 291]        layers.30.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 276/ 291]        layers.30.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 277/ 291]        layers.30.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 278/ 291]      layers.30.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 279/ 291]     layers.30.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 280/ 291]     layers.30.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 281/ 291]     layers.30.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 282/ 291]            layers.30.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 283/ 291]        layers.31.attention.wq.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 284/ 291]        layers.31.attention.wk.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 285/ 291]        layers.31.attention.wv.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 286/ 291]        layers.31.attention.wo.weight -     4096 x  4096, type =    f16, quantizing to q6_K .. size =    32.00 MB ->    13.12 MB | hist: \n",
            "[ 287/ 291]      layers.31.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "[ 288/ 291]     layers.31.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 289/ 291]     layers.31.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 290/ 291]     layers.31.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing to q6_K .. size =    86.00 MB ->    35.27 MB | hist: \n",
            "[ 291/ 291]            layers.31.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
            "llama_model_quantize_internal: model size  = 13217.02 MB\n",
            "llama_model_quantize_internal: quant size  =  5421.64 MB\n",
            "\n",
            "main: quantize time = 177932.62 ms\n",
            "main:    total time = 177932.62 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ï¼ˆå¯é€‰ï¼‰æµ‹è¯•é‡åŒ–æ¨¡å‹è§£ç \n",
        "è‡³æ­¤å·²å®Œæˆäº†æ‰€æœ‰è½¬æ¢æ­¥éª¤ã€‚\n",
        "æˆ‘ä»¬è¿è¡Œä¸€æ¡å‘½ä»¤æµ‹è¯•ä¸€ä¸‹æ˜¯å¦èƒ½å¤Ÿæ­£å¸¸åŠ è½½å¹¶è¿›è¡Œè¾“å‡ºã€‚"
      ],
      "metadata": {
        "id": "DLkuRAo9Vkb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && ./main -m ../llama-2-7b-combined/ggml-model-q6_K.bin --color -p \"ä»¥ä¸‹æ˜¯10æ¡æ–‡æ˜ä¹˜è½¦çš„å»ºè®®ï¼š\" -n 256"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tW-ep1BsVQtG",
        "outputId": "27d9fa0b-16e9-410d-ace3-1bc8167b55cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 916 (b5472ea)\n",
            "main: seed  = 1690444520\n",
            "llama.cpp: loading model from ../llama-2-7b-combined/ggml-model-q6_K.bin\n",
            "llama_model_load_internal: format     = ggjt v3 (latest)\n",
            "llama_model_load_internal: n_vocab    = 55296\n",
            "llama_model_load_internal: n_ctx      = 512\n",
            "llama_model_load_internal: n_embd     = 4096\n",
            "llama_model_load_internal: n_mult     = 256\n",
            "llama_model_load_internal: n_head     = 32\n",
            "llama_model_load_internal: n_head_kv  = 32\n",
            "llama_model_load_internal: n_layer    = 32\n",
            "llama_model_load_internal: n_rot      = 128\n",
            "llama_model_load_internal: n_gqa      = 1\n",
            "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
            "llama_model_load_internal: n_ff       = 11008\n",
            "llama_model_load_internal: freq_base  = 10000.0\n",
            "llama_model_load_internal: freq_scale = 1\n",
            "llama_model_load_internal: ftype      = 18 (mostly Q6_K)\n",
            "llama_model_load_internal: model size = 7B\n",
            "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
            "llama_model_load_internal: mem required  = 5723.73 MB (+  256.00 MB per state)\n",
            "llama_new_context_with_model: kv self size  =  256.00 MB\n",
            "\n",
            "system_info: n_threads = 4 / 4 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
            "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
            "generate: n_ctx = 512, n_batch = 512, n_predict = 256, n_keep = 0\n",
            "\n",
            "\n",
            "\u001b[33m ä»¥ä¸‹æ˜¯10æ¡æ–‡æ˜ä¹˜è½¦çš„å»ºè®®ï¼š\u001b[0m1ã€è‡ªè§‰æ’é˜Ÿï¼Œä¾æ¬¡ä¸Šä¸‹è½¦ã€‚2ã€ä¸Šä¸‹æ¥¼æ¢¯é å³è¡Œèµ°ï¼›ä¹˜æ‰¶æ¢¯æ—¶ä¸è¦æ‹¥æŒ¤ã€æ¨ªè¡Œã€äº’ç›¸æ¨æŒ¤ï¼Œä»¥å…è¸©ç©ºæ‘”å€’æˆ–è¢«å¤¹ä¼¤ã€‚3ã€è¯·ç»™è€å¼±ç—…æ®‹è®©åº§å’Œå„¿ç«¥ä¼˜å…ˆä¸Š(ä¸‹)è½¦ã€‚4ã€è½¦è¾†è¡Œé©¶é€”ä¸­ä¸è¦ä¸å¸æœºæ”€è°ˆèŠå¤©ï¼Œæ›´ä¸èƒ½å¦¨ç¢é©¾é©¶å‘˜å®‰å…¨è¡Œè½¦å¹¶å½±å“å…¶ä»–äººå‘˜çš„æ­£å¸¸å·¥ä½œç”Ÿæ´»ï¼›è¦éšæ—¶å…³æ³¨è½¦å¢å†…çš„ä¹˜è½¦ç§©åºæƒ…å†µï¼Œå‘ç°æœ‰ä¹˜å®¢å¯¹è¯¥è½¦è¾†æœ‰æ¶æ„ç ´åã€å·ç›—ç­‰è¡Œä¸ºæ—¶è¦ç«‹å³åˆ¶æ­¢æˆ–å‘ä¹˜åŠ¡äººå‘˜æŠ¥å‘Šã€‚5ã€é‡åˆ°æŠ¢åº§ã€æ‰“æ¶æ–—æ®´ç­‰çªå‘äº‹ä»¶ï¼Œè¦åŠæ—¶æŠ¥è­¦å¤„ç†ã€‚6ã€ä¸å¾—åœ¨è½¦è¾†è¡Œé©¶è¿‡ç¨‹ä¸­éšæ„å¼€é—¨ä¸Šè½¦ä¸‹è½¦ï¼›ä¹˜è½¦å‰è¦çœ‹æ¸…è½¦é—¨æ˜¯å¦å…³é—­ï¼Œä»¥å…è¢«å¤¹ä¼¤è‚¢ä½“å’Œå¤´å‘ã€‚7ã€ä¸ç§è‡ªæ‹†è£…åº§ä½åº§æ¤…ï¼Œä¹Ÿä¸è¦ä¹±æ‰”æ‚ç‰©ï¼Œä»¥ä¿æŒè½¦å¢æ¸…æ´å«ç”Ÿã€‚8ã€è‡ªè§‰ç»´æŠ¤å…¬å…±ç§©åºï¼Œç¦æ­¢å¸çƒŸã€åç—°åŠéšåœ°å¤§å°ä¾¿ç­‰ä¸æ–‡æ˜è¡Œä¸º9ã€ä¸å¾—åœ¨è½¦å†…é¥®é£Ÿå–§å“—åµé—¹10ã€ä¸è¦éšæ„è§¦æ‘¸è½¦ä¸Šå„ç§è®¾å¤‡è®¾æ–½ ç‚¹å‡»\"é˜…è¯»åŸæ–‡\"æŸ¥çœ‹ï¼š [end of text]\n",
            "\n",
            "llama_print_timings:        load time = 17879.48 ms\n",
            "llama_print_timings:      sample time =   516.85 ms /   238 runs   (    2.17 ms per token,   460.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =  4132.95 ms /    11 tokens (  375.72 ms per token,     2.66 tokens per second)\n",
            "llama_print_timings:        eval time = 145458.46 ms /   237 runs   (  613.75 ms per token,     1.63 tokens per second)\n",
            "llama_print_timings:       total time = 150250.52 ms\n"
          ]
        }
      ]
    }
  ]
}